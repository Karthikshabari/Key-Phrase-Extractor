{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T06:26:02.573695Z",
     "iopub.status.busy": "2025-04-12T06:26:02.573373Z",
     "iopub.status.idle": "2025-04-12T06:27:15.873627Z",
     "shell.execute_reply": "2025-04-12T06:27:15.872635Z",
     "shell.execute_reply.started": "2025-04-12T06:26:02.573667Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for pke (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "mlxtend 0.23.3 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\n",
      "preprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9b1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mRequired libraries installed/updated.\n",
      "Added /kaggle/working/nltk_data to NLTK data path.\n",
      "Downloading NLTK resources (if not already present)...\n",
      "   - Resource 'stopwords' found.\n",
      "   - Downloading 'punkt'...\n",
      "   - Downloading 'wordnet'...\n",
      "   - Downloading 'omw-1.4'...\n",
      "NLTK resource check complete.\n",
      "NLTK WordNet is accessible.\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m103.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n",
      "SpaCy model 'en_core_web_sm' download command executed (if not already present).\n",
      "All libraries imported.\n",
      "\n",
      "==================================================\n",
      "WORDNET INSTALLATION TEST\n",
      "==================================================\n",
      "NLTK data paths:\n",
      "- /root/nltk_data\n",
      "  ✗ Path does not exist\n",
      "- /usr/nltk_data\n",
      "  ✗ Path does not exist\n",
      "- /usr/share/nltk_data\n",
      "  ✓ Path exists\n",
      "  ✓ 'corpora' directory found\n",
      "  Corpora contents: abc, crubadan.zip, cess_esp.zip, europarl_raw, omw.zip...\n",
      "  ✗ 'wordnet' directory NOT found in corpora\n",
      "- /usr/lib/nltk_data\n",
      "  ✗ Path does not exist\n",
      "- /usr/share/nltk_data\n",
      "  ✓ Path exists\n",
      "  ✓ 'corpora' directory found\n",
      "  Corpora contents: abc, crubadan.zip, cess_esp.zip, europarl_raw, omw.zip...\n",
      "  ✗ 'wordnet' directory NOT found in corpora\n",
      "- /usr/local/share/nltk_data\n",
      "  ✗ Path does not exist\n",
      "- /usr/lib/nltk_data\n",
      "  ✗ Path does not exist\n",
      "- /usr/local/lib/nltk_data\n",
      "  ✗ Path does not exist\n",
      "- /kaggle/working/nltk_data\n",
      "  ✓ Path exists\n",
      "  ✓ 'corpora' directory found\n",
      "  Corpora contents: omw-1.4.zip, wordnet.zip...\n",
      "  ✗ 'wordnet' directory NOT found in corpora\n",
      "\n",
      "Attempting to download WordNet:\n",
      "\n",
      "Testing WordNet directly:\n",
      "✓ Successfully loaded WordNet\n",
      "Found 13 synsets for 'test'\n",
      "First synset: trial.n.02, definition: trying something to find out about it\n",
      "\n",
      "Testing lemmatization:\n",
      "  'running' (v) -> 'run' ✓\n",
      "  'better' (a) -> 'good' ✓\n",
      "  'cars' (n) -> 'car' ✓\n",
      "  'is' (v) -> 'be' ✓\n",
      "\n",
      "Creating a robust lemmatizer:\n",
      "✓ WordNetLemmatizer is working\n",
      "\n",
      "Testing robust lemmatizer:\n",
      "  'running' (v) -> 'run'\n",
      "  'better' (a) -> 'good'\n",
      "  'cars' (n) -> 'car'\n",
      "  'is' (v) -> 'be'\n",
      "\n",
      "==================================================\n",
      "TEST COMPLETE\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Install libraries that might not be pre-installed or need specific versions/sources\n",
    "# Using -q for quieter output during installation\n",
    "!pip install -q keybert>=0.7.0\n",
    "!pip install -q sentence-transformers>=2.2.2\n",
    "!pip install -q --no-cache-dir git+https://github.com/boudinfl/pke.git # PKE needs to be installed from GitHub\n",
    "!pip install -q spacy>=3.0.0\n",
    "\n",
    "# Uncomment the line below ONLY if you consistently face issues with NLTK WordNet later\n",
    "!pip install -q nltk==3.9b1 --force-reinstall\n",
    "\n",
    "print(\"Required libraries installed/updated.\")\n",
    "import nltk\n",
    "import os # Import os library for path operations\n",
    "\n",
    "# Define a Kaggle-friendly path for NLTK data\n",
    "kaggle_nltk_path = '/kaggle/working/nltk_data'\n",
    "os.makedirs(kaggle_nltk_path, exist_ok=True)\n",
    "\n",
    "# Check if the path is already added, if not, append it\n",
    "if kaggle_nltk_path not in nltk.data.path:\n",
    "    nltk.data.path.append(kaggle_nltk_path)\n",
    "    print(f\"Added {kaggle_nltk_path} to NLTK data path.\")\n",
    "else:\n",
    "    print(f\"{kaggle_nltk_path} already in NLTK data path.\")\n",
    "\n",
    "# Download required NLTK resources to the specified path\n",
    "resources_to_download = ['stopwords', 'punkt', 'wordnet', 'omw-1.4']\n",
    "print(\"Downloading NLTK resources (if not already present)...\")\n",
    "for resource in resources_to_download:\n",
    "    try:\n",
    "        nltk.data.find(f'corpora/{resource}') # More specific check\n",
    "        print(f\"   - Resource '{resource}' found.\")\n",
    "    except LookupError:\n",
    "        print(f\"   - Downloading '{resource}'...\")\n",
    "        nltk.download(resource, download_dir=kaggle_nltk_path, quiet=True) # Use quiet download\n",
    "print(\"NLTK resource check complete.\")\n",
    "\n",
    "# Verify WordNet specifically after download\n",
    "try:\n",
    "    from nltk.corpus import wordnet as wn\n",
    "    _ = wn.synsets('test')\n",
    "    print(\"NLTK WordNet is accessible.\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Potential issue accessing WordNet after download: {e}\")\n",
    "    print(\"The main code contains fallback mechanisms, but check installation if lemmatization fails.\")\n",
    "\n",
    "\n",
    "# Download the required spaCy English model\n",
    "# The code tries to load 'en_core_web_sm'. If it fails, it should download it.\n",
    "# Running this command explicitly ensures it's downloaded beforehand.\n",
    "!python -m spacy download en_core_web_sm -q\n",
    "print(\"SpaCy model 'en_core_web_sm' download command executed (if not already present).\")\n",
    "\n",
    "# Standard Libraries\n",
    "import re\n",
    "import string\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import gc\n",
    "import time\n",
    "import sys\n",
    "import subprocess\n",
    "import importlib\n",
    "import io\n",
    "import base64\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Tuple, Set, Optional, Union, Any\n",
    "\n",
    "# Third-party Libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.language import Language # Needed for AbstractiveExtractor's NER type hints\n",
    "import pke\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn # Import wordnet alias as used in the code\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import pos_tag # Imported specifically in AbstractiveExtractor\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support # For test/analysis code\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from keybert import KeyBERT\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, set_seed, pipeline\n",
    "import matplotlib.pyplot as plt # For test/analysis code\n",
    "import seaborn as sns # For test/analysis code\n",
    "\n",
    "# Optional: Set a specific device early if needed, although the classes handle it.\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# print(f\"Using device: {device}\")\n",
    "\n",
    "# Import your custom classes (assuming they are defined in subsequent cells or .py files uploaded to Kaggle)\n",
    "# Example: If your classes are defined below this cell, you don't need these lines here.\n",
    "# If they are in .py files uploaded as utility scripts:\n",
    "# from Extractive_code import HybridExtractiveKeyphraseExtractor\n",
    "# from Abstractive_code import AbstractiveKeyphraseExtractor\n",
    "# from Fusion_code import FusionKeyphraseExtractor # Assuming a Fusion_code.py file\n",
    "\n",
    "print(\"All libraries imported.\")\n",
    "\n",
    "# Optional: Include the NLTK WordNet setup function if needed outside the class init\n",
    "# setup_nltk_wordnet() function from the test code - requires modifications for Kaggle paths\n",
    "# ... (rest of your code, including class definitions and main execution logic) ...\n",
    "\n",
    "\n",
    "# ===== WORDNET INSTALLATION TEST =====\n",
    "# This script tests if WordNet is properly installed and accessible\n",
    "\n",
    "import os\n",
    "import nltk\n",
    "import sys\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"WORDNET INSTALLATION TEST\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 1. Check NLTK data paths\n",
    "print(\"NLTK data paths:\")\n",
    "for path in nltk.data.path:\n",
    "    print(f\"- {path}\")\n",
    "    if os.path.exists(path):\n",
    "        print(f\"  ✓ Path exists\")\n",
    "        # List contents if it exists\n",
    "        if os.path.isdir(path):\n",
    "            contents = os.listdir(path)\n",
    "            if 'corpora' in contents:\n",
    "                print(f\"  ✓ 'corpora' directory found\")\n",
    "                corpora_path = os.path.join(path, 'corpora')\n",
    "                corpora_contents = os.listdir(corpora_path)\n",
    "                print(f\"  Corpora contents: {', '.join(corpora_contents[:5])}...\")\n",
    "                if 'wordnet' in corpora_contents:\n",
    "                    print(f\"  ✓ 'wordnet' directory found in corpora\")\n",
    "                    wordnet_path = os.path.join(corpora_path, 'wordnet')\n",
    "                    if os.path.isdir(wordnet_path):\n",
    "                        wordnet_contents = os.listdir(wordnet_path)\n",
    "                        print(f\"  Wordnet contents: {', '.join(wordnet_contents[:5])}...\")\n",
    "                    else:\n",
    "                        print(f\"  ✗ 'wordnet' is not a directory\")\n",
    "                else:\n",
    "                    print(f\"  ✗ 'wordnet' directory NOT found in corpora\")\n",
    "            else:\n",
    "                print(f\"  ✗ 'corpora' directory NOT found\")\n",
    "    else:\n",
    "        print(f\"  ✗ Path does not exist\")\n",
    "\n",
    "# 2. Try to download WordNet explicitly\n",
    "print(\"\\nAttempting to download WordNet:\")\n",
    "nltk.download('wordnet', quiet=False)\n",
    "\n",
    "# 3. Test WordNet directly\n",
    "print(\"\\nTesting WordNet directly:\")\n",
    "try:\n",
    "    from nltk.corpus import wordnet as wn\n",
    "    synsets = wn.synsets('test')\n",
    "    print(f\"✓ Successfully loaded WordNet\")\n",
    "    print(f\"Found {len(synsets)} synsets for 'test'\")\n",
    "    if len(synsets) > 0:\n",
    "        print(f\"First synset: {synsets[0].name()}, definition: {synsets[0].definition()}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error loading WordNet: {e}\")\n",
    "\n",
    "# 4. Test lemmatization\n",
    "print(\"\\nTesting lemmatization:\")\n",
    "try:\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    test_words = [(\"running\", \"v\"), (\"better\", \"a\"), (\"cars\", \"n\"), (\"is\", \"v\")]\n",
    "    \n",
    "    for word, pos in test_words:\n",
    "        try:\n",
    "            lemmatized = lemmatizer.lemmatize(word, pos)\n",
    "            print(f\"  '{word}' ({pos}) -> '{lemmatized}' ✓\")\n",
    "        except Exception as word_error:\n",
    "            print(f\"  '{word}' ({pos}) -> Error: {word_error} ✗\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error initializing lemmatizer: {e}\")\n",
    "\n",
    "# 5. Create a custom WordNetLemmatizer that doesn't fail\n",
    "print(\"\\nCreating a robust lemmatizer:\")\n",
    "\n",
    "class RobustLemmatizer:\n",
    "    def __init__(self):\n",
    "        self.wordnet_available = False\n",
    "        try:\n",
    "            self.lemmatizer = WordNetLemmatizer()\n",
    "            # Test if it works\n",
    "            self.lemmatizer.lemmatize(\"test\")\n",
    "            self.wordnet_available = True\n",
    "            print(\"✓ WordNetLemmatizer is working\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ WordNetLemmatizer failed: {e}\")\n",
    "            print(\"Using fallback lemmatization\")\n",
    "    \n",
    "    def lemmatize(self, word, pos='n'):\n",
    "        if self.wordnet_available:\n",
    "            try:\n",
    "                return self.lemmatizer.lemmatize(word, pos)\n",
    "            except:\n",
    "                return word\n",
    "        else:\n",
    "            # Simple fallback rules\n",
    "            if pos == 'v' and word.endswith('ing'):\n",
    "                return word[:-3]  # Remove 'ing'\n",
    "            elif pos == 'n' and word.endswith('s'):\n",
    "                return word[:-1]  # Remove plural 's'\n",
    "            return word\n",
    "\n",
    "# Test the robust lemmatizer\n",
    "robust_lemmatizer = RobustLemmatizer()\n",
    "print(\"\\nTesting robust lemmatizer:\")\n",
    "for word, pos in test_words:\n",
    "    lemmatized = robust_lemmatizer.lemmatize(word, pos)\n",
    "    print(f\"  '{word}' ({pos}) -> '{lemmatized}'\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TEST COMPLETE\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ex code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T06:27:15.875331Z",
     "iopub.status.busy": "2025-04-12T06:27:15.875109Z",
     "iopub.status.idle": "2025-04-12T06:27:37.694563Z",
     "shell.execute_reply": "2025-04-12T06:27:37.693735Z",
     "shell.execute_reply.started": "2025-04-12T06:27:15.875313Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating comprehensive news-focused IDF corpus\n",
      "Created comprehensive IDF corpus with 206 terms\n",
      "Added 20 additional news-specific terms\n",
      "Final IDF corpus contains 225 terms\n",
      "Using device: cuda\n",
      "Loading SentenceTransformer model: all-mpnet-base-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f521aca0f2b3446da409063d1b87c714",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73c3b36c24ad4d37870c89b2eacc6ea4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "addae9747ecc4a18b934a01507c8cd0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a027e8964369426794bbe008ee8c0f54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "accb036e86e1403d9c3e3de5f1b1fb1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fd65ca22a8e41a5ab36866fdc2ec551",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15a222a604d24586b85a844bec3735bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c350a3d16a31428b86b85d4ac4082223",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c5013f14b9446af80aabff3c2769a3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82615837186744f6a6fd29f66e47281e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1d6687c0669442ca30c50de2af47676",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading KeyBERT\n",
      "Advanced Hybrid Extractive Keyphrase Extractor initialized with ensemble methods\n",
      "DEBUG: Extracting keyphrases with use_position_weight=True, use_tfidf_weight=True, use_ensemble=True\n",
      "DEBUG: KeyBERT extracted 34 keyphrases\n",
      "DEBUG: MultipartiteRank position weight for 'new model': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'research team': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'human': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'model': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'unprecedented capabilities': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'OpenAI': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'context': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'like responses': 2.00\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'new model': 1.35\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'research team': 1.14\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'human': 1.45\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'model': 2.00\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'unprecedented capabilities': 0.64\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'OpenAI': 0.50\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'context': 0.50\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'like responses': 0.64\n",
      "DEBUG: MultipartiteRank extracted 8 keyphrases\n",
      "DEBUG: YAKE position weight for 'natural language processing': 2.00\n",
      "DEBUG: YAKE position weight for 'jane smith lead': 2.00\n",
      "DEBUG: YAKE position weight for 'language processing researchers': 2.00\n",
      "DEBUG: YAKE position weight for 'language processing technology': 2.00\n",
      "DEBUG: YAKE position weight for 'artificial intelligence breakthrough': 2.00\n",
      "DEBUG: YAKE position weight for 'like responses according': 2.00\n",
      "DEBUG: YAKE position weight for 'smith lead researcher': 2.00\n",
      "DEBUG: YAKE position weight for 'educational tools industry': 2.00\n",
      "DEBUG: YAKE position weight for 'tools industry experts': 2.00\n",
      "DEBUG: YAKE position weight for 'industry experts predict': 2.00\n",
      "DEBUG: YAKE TF-IDF weight for 'natural language processing': 1.54\n",
      "DEBUG: YAKE TF-IDF weight for 'jane smith lead': 0.50\n",
      "DEBUG: YAKE TF-IDF weight for 'language processing researchers': 1.59\n",
      "DEBUG: YAKE TF-IDF weight for 'language processing technology': 1.48\n",
      "DEBUG: YAKE TF-IDF weight for 'artificial intelligence breakthrough': 2.00\n",
      "DEBUG: YAKE TF-IDF weight for 'like responses according': 0.50\n",
      "DEBUG: YAKE TF-IDF weight for 'smith lead researcher': 0.50\n",
      "DEBUG: YAKE TF-IDF weight for 'educational tools industry': 0.58\n",
      "DEBUG: YAKE TF-IDF weight for 'tools industry experts': 0.75\n",
      "DEBUG: YAKE TF-IDF weight for 'industry experts predict': 0.58\n",
      "DEBUG: YAKE extracted 10 keyphrases\n",
      "DEBUG: TextRank position weight for 'natural language processing researchers': 2.00\n",
      "DEBUG: TextRank position weight for 'natural language processing technology': 2.00\n",
      "DEBUG: TextRank position weight for 'language models': 2.00\n",
      "DEBUG: TextRank position weight for 'generate human language': 2.00\n",
      "DEBUG: TextRank position weight for 'ai artificial intelligence breakthrough': 2.00\n",
      "DEBUG: TextRank position weight for 'develop ai artificial intelligence': 2.00\n",
      "DEBUG: TextRank position weight for 'ai artificial intelligence': 2.00\n",
      "DEBUG: TextRank position weight for 'new model': 2.00\n",
      "DEBUG: TextRank position weight for 'language': 2.00\n",
      "DEBUG: TextRank position weight for 'service content creation': 2.00\n",
      "DEBUG: TextRank TF-IDF weight for 'natural language processing researchers': 1.74\n",
      "DEBUG: TextRank TF-IDF weight for 'natural language processing technology': 1.64\n",
      "DEBUG: TextRank TF-IDF weight for 'language models': 1.23\n",
      "DEBUG: TextRank TF-IDF weight for 'generate human language': 1.19\n",
      "DEBUG: TextRank TF-IDF weight for 'ai artificial intelligence breakthrough': 2.00\n",
      "DEBUG: TextRank TF-IDF weight for 'develop ai artificial intelligence': 1.89\n",
      "DEBUG: TextRank TF-IDF weight for 'ai artificial intelligence': 1.91\n",
      "DEBUG: TextRank TF-IDF weight for 'new model': 0.81\n",
      "DEBUG: TextRank TF-IDF weight for 'language': 1.54\n",
      "DEBUG: TextRank TF-IDF weight for 'service content creation': 0.50\n",
      "DEBUG: TextRank extracted 10 keyphrases\n",
      "DEBUG: Raw domain score for sports: 8.00\n",
      "DEBUG: Top domains detected: academic: 0.2629, technology: 0.2441, entertainment: 0.1315\n",
      "DEBUG: All domain scores: {'technology': 0.24413145539906103, 'business': 0.07981220657276995, 'health': 0.0, 'science': 0.09389671361502347, 'news': 0.11267605633802817, 'academic': 0.26291079812206575, 'politics': 0.0, 'environment': 0.0, 'entertainment': 0.13145539906103287, 'sports': 0.07511737089201878}\n",
      "DEBUG: Adaptive method weights:\n",
      "DEBUG:   - keybert: 0.3249\n",
      "DEBUG:   - multipartiterank: 0.3279\n",
      "DEBUG:   - yake: 0.0958\n",
      "DEBUG:   - textrank: 0.2515\n",
      "DEBUG: Length bonus for 'new model' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'natural language processing' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'significant advancement' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'natural language processing researcher' (4 words): 1.45x\n",
      "DEBUG: Length bonus for 'language processing researcher' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'natural language processing technology' (4 words): 1.45x\n",
      "DEBUG: Length bonus for 'ai artificial intelligence breakthrough' (4 words): 1.45x\n",
      "DEBUG: Length bonus for 'ai artificial intelligence' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'artificial intelligence breakthrough' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'language processing technology' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'research team' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'diverse dataset' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'internet book' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'scientific paper' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'extensive training' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'deep understanding' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'language pattern' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'major step' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'dr jane smith' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'human language' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'potential application' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'develop ai artificial intelligence' (4 words): 1.45x\n",
      "DEBUG: Length bonus for 'unprecedented capability' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'various sector' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'ethical consideration' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'jane smith lead' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'response accord' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'smith lead researcher' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'educational tool industry' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'tool industry expert' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'industry expert predict' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'language model' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'generate human language' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'service content creation' (3 words): 1.30x\n",
      "DEBUG: Ensemble combined into 52 keyphrases\n",
      "DEBUG: After redundancy removal: 34 keyphrases\n",
      "DEBUG: Before post-filtering: 34 keyphrases\n",
      "DEBUG: Multi-word phrases: 19\n",
      "DEBUG: After post-filtering: 34 keyphrases\n",
      "DEBUG: Final top 10 keyphrases: ['breakthrough', 'model', 'industry', 'however', 'project', 'privacy', 'researcher', 'quest', 'text', 'adoption']\n",
      "\n",
      "Extracted keyphrases:\n",
      "1. breakthrough\n",
      "2. model\n",
      "3. industry\n",
      "4. however\n",
      "5. project\n",
      "6. privacy\n",
      "7. researcher\n",
      "8. quest\n",
      "9. text\n",
      "10. adoption\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import spacy\n",
    "import pke\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Tuple, Set, Optional, Union, Any\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "try:\n",
    "    nltk.data.find('stopwords')\n",
    "    nltk.data.find('punkt')\n",
    "    nltk.data.find('wordnet')\n",
    "    nltk.data.find('omw-1.4')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('omw-1.4')\n",
    "\n",
    "try:\n",
    "    import nltk.corpus.reader.wordnet\n",
    "    from nltk.corpus import wordnet as wn\n",
    "\n",
    "    if not hasattr(wn, '_morphy') or not hasattr(wn, 'morphy'):\n",
    "        nltk.download('wordnet')\n",
    "        nltk.download('omw-1.4')\n",
    "\n",
    "        if not hasattr(wn, '_morphy') or not hasattr(wn, 'morphy'):\n",
    "            from nltk.stem.wordnet import WordNetLemmatizer\n",
    "            original_init = WordNetLemmatizer.__init__\n",
    "\n",
    "            def patched_init(self):\n",
    "                self.lemmatize = lambda word, pos='n': word\n",
    "\n",
    "            WordNetLemmatizer.__init__ = patched_init\n",
    "            print(\"Applied NLTK 3.9 WordNet lemmatizer patch\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: NLTK WordNet initialization error: {e}\")\n",
    "    print(\"Trying alternative approach...\")\n",
    "    try:\n",
    "        import sys\n",
    "        import subprocess\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"nltk==3.9b1\", \"--force-reinstall\"])\n",
    "        print(\"Installed NLTK 3.9b1 as a workaround\")\n",
    "\n",
    "        import importlib\n",
    "        importlib.reload(nltk)\n",
    "        nltk.download('wordnet')\n",
    "        nltk.download('omw-1.4')\n",
    "    except Exception as e2:\n",
    "        print(f\"Warning: Could not fix NLTK WordNet issue: {e2}\")\n",
    "        print(\"Lemmatization will be disabled\")\n",
    "\n",
    "class HybridExtractiveKeyphraseExtractor:\n",
    "    \n",
    "\n",
    "    DOMAIN_STOPWORDS = {\n",
    "        'general': ['etc', 'e.g', 'i.e'],\n",
    "        'tech': ['technology', 'technologies'],\n",
    "        'business': ['business', 'company'],\n",
    "        'science': ['study', 'studies'],\n",
    "        'news': ['report', 'reported', 'according']\n",
    "    }\n",
    "\n",
    "    BOUNDARY_WORDS = {\n",
    "        'the', 'a', 'an', 'this', 'that', 'these', 'those', 'their', 'our', 'your',\n",
    "        'its', 'his', 'her', 'they', 'them', 'which', 'what', 'who', 'whom', 'whose'\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "    self,\n",
    "    model_name: str = \"all-mpnet-base-v2\",\n",
    "    use_gpu: bool = True,\n",
    "    top_n: int = 20,\n",
    "    language: str = \"en\",\n",
    "    alpha: float = 1.1,\n",
    "    threshold: float = 0.74,\n",
    "    method: str = 'mmr',\n",
    "    min_df: int = 1,\n",
    "    redundancy_threshold: float = 0.80,\n",
    "    diversity_penalty: float = 0.5,\n",
    "    prioritize_named_entities: bool = False,\n",
    "    ngram_range: Tuple[int, int] = (1, 3),\n",
    "    clean_boundaries: bool = True,\n",
    "    use_noun_chunks: bool = True,\n",
    "    boost_exact_matches: bool = True,\n",
    "    use_position_weight: bool = True,\n",
    "    use_tfidf_weight: bool = True,\n",
    "    use_ensemble: bool = True,\n",
    "    use_lemmatization: bool = True,\n",
    "    use_partial_matching: bool = True,\n",
    "    use_semantic_matching: bool = True,\n",
    "    use_enhanced_pos_filtering: bool = True,\n",
    "    use_title_lead_boost: bool = True,\n",
    "    method_weights: Dict[str, float] = None,\n",
    "    idf_corpus_path: str = None\n",
    "    ):\n",
    "        \n",
    "        self.top_n = top_n\n",
    "        self.language = language\n",
    "        self.alpha = alpha\n",
    "        self.threshold = threshold\n",
    "        self.method = method\n",
    "        self.min_df = min_df\n",
    "        self.redundancy_threshold = redundancy_threshold\n",
    "        self.diversity_penalty = diversity_penalty\n",
    "        self.model_name = model_name\n",
    "        self.prioritize_named_entities = prioritize_named_entities\n",
    "        self.ngram_range = ngram_range\n",
    "        self.clean_boundaries = clean_boundaries\n",
    "        self.use_noun_chunks = use_noun_chunks\n",
    "        self.boost_exact_matches = boost_exact_matches\n",
    "        self.use_position_weight = use_position_weight\n",
    "        self.use_tfidf_weight = use_tfidf_weight\n",
    "        self.use_ensemble = use_ensemble\n",
    "        self.use_enhanced_pos_filtering = use_enhanced_pos_filtering\n",
    "        self.use_title_lead_boost = use_title_lead_boost\n",
    "        self.use_lemmatization = use_lemmatization\n",
    "        self.use_partial_matching = use_partial_matching\n",
    "        self.use_semantic_matching = use_semantic_matching\n",
    "\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "        if method_weights is None:\n",
    "            self.method_weights = {\n",
    "                'keybert': 0.50,\n",
    "                'multipartiterank': 0.25,\n",
    "                'textrank': 0.15,\n",
    "                'yake': 0.10\n",
    "            }\n",
    "        else:\n",
    "            self.method_weights = method_weights\n",
    "        self.tfidf_vectorizer = TfidfVectorizer(\n",
    "        ngram_range=ngram_range,\n",
    "        stop_words='english',\n",
    "        max_features=10000,\n",
    "        min_df=1,\n",
    "        norm='l2',\n",
    "        use_idf=True,\n",
    "        smooth_idf=True,\n",
    "        sublinear_tf=True\n",
    "        )\n",
    "        self.initialize_idf_corpus(idf_corpus_path)\n",
    "\n",
    "        self.word_tfidf_cache = {}\n",
    "\n",
    "        try:\n",
    "            try:\n",
    "                from nltk.corpus import stopwords\n",
    "                self.all_stopwords = set(stopwords.words('english'))\n",
    "            except (ImportError, ModuleNotFoundError):\n",
    "                nltk.download('stopwords')\n",
    "                self.all_stopwords = set([\n",
    "                    'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you',\n",
    "                    'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself',\n",
    "                    'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them',\n",
    "                    'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this',\n",
    "                    'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been',\n",
    "                    'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing',\n",
    "                    'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
    "                    'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between',\n",
    "                    'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to',\n",
    "                    'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again',\n",
    "                    'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how',\n",
    "                    'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such',\n",
    "                    'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very',\n",
    "                    's', 't', 'can', 'will', 'just', 'don', 'should', 'now'\n",
    "                ])\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error loading stopwords: {e}\")\n",
    "            self.all_stopwords = set(['a', 'an', 'the', 'and', 'or', 'but', 'if', 'because', 'as', 'what', 'when', 'where', 'how', 'why', 'who'])\n",
    "\n",
    "        for domain, words in self.DOMAIN_STOPWORDS.items():\n",
    "            self.all_stopwords.update(words)\n",
    "\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() and use_gpu else \"cpu\"\n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "        print(f\"Loading SentenceTransformer model: {model_name}\")\n",
    "        try:\n",
    "            from sentence_transformers import SentenceTransformer\n",
    "            self.sentence_model = SentenceTransformer(model_name)\n",
    "            self.sentence_model.to(self.device)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading SentenceTransformer: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "        print(\"Loading KeyBERT\")\n",
    "        try:\n",
    "            from keybert import KeyBERT\n",
    "            self.kw_model = KeyBERT(model=self.sentence_model)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading KeyBERT: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "        try:\n",
    "            import spacy\n",
    "            try:\n",
    "                self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "            except OSError:\n",
    "                print(\"Downloading spaCy model...\")\n",
    "                import subprocess\n",
    "                subprocess.run([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
    "                self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        except ImportError:\n",
    "            print(\"spaCy not available, using simplified document classification\")\n",
    "            self.nlp = None\n",
    "\n",
    "        print(\"Advanced Hybrid Extractive Keyphrase Extractor initialized with ensemble methods\")\n",
    "\n",
    "    def post_process_keyphrases(self, keyphrases: List[Tuple[str, float]], text: str) -> List[Tuple[str, float]]:\n",
    "        \n",
    "        processed_keyphrases = []\n",
    "\n",
    "        prefixes_to_remove = ['the ', 'a ', 'an ', 'this ', 'that ', 'these ', 'those ']\n",
    "\n",
    "        for kp, score in keyphrases:\n",
    "            if self.clean_boundaries:\n",
    "                kp = self.clean_phrase_boundaries(kp)\n",
    "\n",
    "            for prefix in prefixes_to_remove:\n",
    "                if kp.lower().startswith(prefix):\n",
    "                    kp = kp[len(prefix):]\n",
    "                    break\n",
    "\n",
    "            text_lower = text.lower()\n",
    "            kp_lower = kp.lower()\n",
    "\n",
    "            if kp_lower in text_lower:\n",
    "                start_idx = text_lower.find(kp_lower)\n",
    "                exact_form = text[start_idx:start_idx+len(kp_lower)]\n",
    "\n",
    "                if exact_form[0].isupper():\n",
    "                    kp = exact_form\n",
    "\n",
    "            if kp and len(kp.split()) <= self.ngram_range[1]:\n",
    "                processed_keyphrases.append((kp, score))\n",
    "\n",
    "        if self.use_enhanced_pos_filtering:\n",
    "            processed_keyphrases = self.filter_by_pos_patterns(processed_keyphrases)\n",
    "\n",
    "        return processed_keyphrases\n",
    "\n",
    "    def filter_by_pos_patterns(self, keyphrases: List[Tuple[str, float]]) -> List[Tuple[str, float]]:\n",
    "        \n",
    "        if not self.use_enhanced_pos_filtering or not self.nlp:\n",
    "            return keyphrases\n",
    "\n",
    "        filtered_keyphrases = []\n",
    "\n",
    "        acceptable_patterns = [\n",
    "            (['NOUN'], 0.95),\n",
    "            (['PROPN'], 1.0),\n",
    "\n",
    "            (['ADJ', 'NOUN'], 1.1),\n",
    "            (['NOUN', 'NOUN'], 1.05),\n",
    "            (['PROPN', 'PROPN'], 1.1),\n",
    "            (['PROPN', 'NOUN'], 1.05),\n",
    "            (['NOUN', 'PROPN'], 1.05),\n",
    "\n",
    "            (['ADJ', 'NOUN', 'NOUN'], 1.15),\n",
    "            (['ADJ', 'ADJ', 'NOUN'], 1.1),\n",
    "            (['NOUN', 'NOUN', 'NOUN'], 1.05),\n",
    "            (['PROPN', 'PROPN', 'PROPN'], 1.1),\n",
    "            (['NOUN', 'PROPN', 'PROPN'], 1.05),\n",
    "            (['PROPN', 'NOUN', 'NOUN'], 1.05),\n",
    "\n",
    "            (['ADJ', 'NOUN', 'NOUN', 'NOUN'], 1.1),\n",
    "            (['ADJ', 'ADJ', 'NOUN', 'NOUN'], 1.1),\n",
    "            (['NOUN', 'PROPN', 'PROPN', 'PROPN'], 1.05),\n",
    "\n",
    "            (['NOUN', 'ADP', 'NOUN'], 0.9),\n",
    "            (['PROPN', 'ADP', 'NOUN'], 0.9),\n",
    "            (['PROPN', 'ADP', 'PROPN'], 0.9),\n",
    "            (['NOUN', 'ADP', 'PROPN'], 0.9),\n",
    "            (['ADJ', 'NOUN', 'ADP', 'NOUN'], 0.95),\n",
    "\n",
    "            (['VERB', 'NOUN'], 0.85),\n",
    "            (['VERB', 'PROPN'], 0.85),\n",
    "            (['VERB', 'ADJ', 'NOUN'], 0.9),\n",
    "        ]\n",
    "\n",
    "        low_quality_patterns = [\n",
    "            (['DET'], 0.5),\n",
    "            (['ADP'], 0.5),\n",
    "            (['ADJ'], 0.7),\n",
    "            (['VERB'], 0.7),\n",
    "            (['ADV'], 0.6),\n",
    "            (['DET', 'NOUN'], 0.8),\n",
    "            (['ADP', 'NOUN'], 0.7),\n",
    "            (['ADV', 'ADJ'], 0.7),\n",
    "            (['VERB', 'DET'], 0.6),\n",
    "        ]\n",
    "\n",
    "        for kp, score in keyphrases:\n",
    "            if not kp.strip():\n",
    "                continue\n",
    "\n",
    "            doc = self.nlp(kp)\n",
    "\n",
    "            pos_tags = [token.pos_ for token in doc]\n",
    "\n",
    "            matched = False\n",
    "            score_multiplier = 1.0\n",
    "\n",
    "            if len(pos_tags) <= 5:\n",
    "                for pattern, multiplier in acceptable_patterns:\n",
    "                    if pos_tags == pattern:\n",
    "                        matched = True\n",
    "                        score_multiplier = multiplier\n",
    "                        break\n",
    "\n",
    "                if not matched:\n",
    "                    for pattern, multiplier in acceptable_patterns:\n",
    "                        if len(pattern) <= len(pos_tags) and pos_tags[:len(pattern)] == pattern:\n",
    "                            matched = True\n",
    "                            score_multiplier = multiplier * 0.95\n",
    "                            break\n",
    "\n",
    "                if not matched:\n",
    "                    for pattern, multiplier in low_quality_patterns:\n",
    "                        if pos_tags == pattern or (len(pattern) <= len(pos_tags) and pos_tags[:len(pattern)] == pattern):\n",
    "                            matched = True\n",
    "                            score_multiplier = multiplier\n",
    "                            break\n",
    "            else:\n",
    "                for pattern, multiplier in acceptable_patterns:\n",
    "                    if len(pattern) <= len(pos_tags) and pos_tags[:len(pattern)] == pattern:\n",
    "                        matched = True\n",
    "                        score_multiplier = multiplier * 0.9\n",
    "                        break\n",
    "\n",
    "                if not matched and pos_tags and pos_tags[0] in ['NOUN', 'PROPN']:\n",
    "                    matched = True\n",
    "                    score_multiplier = 0.9\n",
    "\n",
    "            if matched:\n",
    "                adjusted_score = min(score * score_multiplier, 1.0)\n",
    "                filtered_keyphrases.append((kp, adjusted_score))\n",
    "            elif len(pos_tags) > 0 and pos_tags[0] in ['NOUN', 'PROPN', 'ADJ']:\n",
    "                adjusted_score = min(score * 0.85, 1.0)\n",
    "                filtered_keyphrases.append((kp, adjusted_score))\n",
    "\n",
    "        if len(filtered_keyphrases) < len(keyphrases) * 0.3:\n",
    "            print(f\"Warning: POS filtering removed too many keyphrases ({len(keyphrases)} -> {len(filtered_keyphrases)}). Using original list.\")\n",
    "            return keyphrases\n",
    "\n",
    "        return filtered_keyphrases\n",
    "\n",
    "    def initialize_idf_corpus(self, corpus_path: str = None):\n",
    "        \n",
    "        if corpus_path and os.path.exists(corpus_path):\n",
    "            try:\n",
    "                with open(corpus_path, 'rb') as f:\n",
    "                    self.idf_values = pickle.load(f)\n",
    "                print(f\"Loaded IDF corpus with {len(self.idf_values)} terms\")\n",
    "                return\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading IDF corpus: {e}\")\n",
    "\n",
    "        print(\"Creating comprehensive news-focused IDF corpus\")\n",
    "\n",
    "        news_corpus = [\n",
    "            \"The President signed an executive order addressing climate change and environmental protection measures.\",\n",
    "            \"Congress passed a new bill aimed at infrastructure development with bipartisan support.\",\n",
    "            \"The Supreme Court ruled on a landmark case regarding privacy rights in the digital age.\",\n",
    "            \"Election polls show a tight race between the incumbent and challenger in key swing states.\",\n",
    "            \"Diplomatic tensions escalated after the ambassador was recalled following controversial statements.\",\n",
    "            \"The Senate committee held hearings on proposed legislation for healthcare reform.\",\n",
    "            \"Local government officials announced new initiatives to address homelessness in urban areas.\",\n",
    "            \"International leaders gathered at the summit to discuss global trade agreements.\",\n",
    "            \"The administration faced criticism over its handling of the border security situation.\",\n",
    "            \"Voters expressed concerns about economic policies ahead of the upcoming election.\",\n",
    "\n",
    "            \"The stock market reached record highs following positive economic indicators and strong earnings reports.\",\n",
    "            \"The Federal Reserve announced changes to interest rates, impacting mortgage and loan markets.\",\n",
    "            \"The tech giant unveiled plans for a major expansion, creating thousands of new jobs.\",\n",
    "            \"Inflation rates rose to their highest level in a decade, affecting consumer purchasing power.\",\n",
    "            \"The company reported quarterly earnings that exceeded analysts' expectations by 15 percent.\",\n",
    "            \"Supply chain disruptions continue to affect manufacturing sectors across multiple industries.\",\n",
    "            \"The startup secured $50 million in venture capital funding for its innovative platform.\",\n",
    "            \"Unemployment figures dropped to 4.5 percent, signaling continued economic recovery.\",\n",
    "            \"The retail sector saw significant growth in online sales while physical stores struggled.\",\n",
    "            \"The cryptocurrency market experienced volatility following regulatory announcements.\",\n",
    "\n",
    "            \"Artificial intelligence advancements are transforming industries from healthcare to finance.\",\n",
    "            \"The new smartphone features a revolutionary camera system and faster processor technology.\",\n",
    "            \"Cybersecurity experts warned of increasing ransomware threats targeting critical infrastructure.\",\n",
    "            \"Cloud computing services continue to grow as businesses move away from on-premises solutions.\",\n",
    "            \"The social media platform introduced new features to address privacy concerns and misinformation.\",\n",
    "            \"Quantum computing researchers achieved a breakthrough in error correction techniques.\",\n",
    "            \"The electric vehicle manufacturer announced a new battery technology with extended range.\",\n",
    "            \"Virtual reality applications are expanding beyond gaming into education and healthcare.\",\n",
    "            \"The semiconductor shortage has impacted production across automotive and electronics industries.\",\n",
    "            \"5G network deployment accelerated, promising faster connectivity and new applications.\",\n",
    "\n",
    "            \"Researchers published promising results from clinical trials of the new cancer treatment.\",\n",
    "            \"The vaccine showed 95 percent efficacy in preventing the disease in large-scale studies.\",\n",
    "            \"Scientists discovered a new species in the remote rainforest region during the expedition.\",\n",
    "            \"The space telescope captured unprecedented images of distant galaxies, revealing new data.\",\n",
    "            \"Medical experts issued updated guidelines for managing chronic conditions and preventive care.\",\n",
    "            \"The genomic study identified key genetic factors associated with longevity and disease resistance.\",\n",
    "            \"Climate scientists reported accelerating ice melt in polar regions exceeding previous models.\",\n",
    "            \"The public health department launched initiatives to address mental health awareness.\",\n",
    "            \"Researchers developed a new diagnostic tool using artificial intelligence algorithms.\",\n",
    "            \"The pharmaceutical company received approval for its breakthrough treatment for rare diseases.\",\n",
    "\n",
    "            \"Record-breaking temperatures were recorded across multiple regions during the summer months.\",\n",
    "            \"Renewable energy installations surpassed fossil fuel capacity additions for the first time.\",\n",
    "            \"Conservation efforts successfully increased the population of the endangered species.\",\n",
    "            \"The environmental impact assessment revealed concerns about the proposed development project.\",\n",
    "            \"Extreme weather events caused significant damage to coastal communities and infrastructure.\",\n",
    "            \"Sustainable agriculture practices gained traction as concerns about food security increased.\",\n",
    "            \"The international agreement established new targets for reducing carbon emissions by 2030.\",\n",
    "            \"Ocean plastic pollution reached alarming levels according to the latest marine research.\",\n",
    "            \"Urban planning initiatives focused on green infrastructure and reducing carbon footprints.\",\n",
    "            \"The drought conditions affected agricultural production across multiple growing regions.\",\n",
    "\n",
    "            \"The university announced a major expansion of its online degree programs and digital learning.\",\n",
    "            \"School districts implemented new curriculum standards focusing on STEM education.\",\n",
    "            \"Research findings highlighted the impact of early childhood education on long-term outcomes.\",\n",
    "            \"The education department allocated additional funding for underserved communities.\",\n",
    "            \"Students demonstrated improved performance following the implementation of the new teaching methods.\",\n",
    "            \"The college admissions process underwent significant changes to address equity concerns.\",\n",
    "            \"Educational technology startups developed innovative tools for personalized learning experiences.\",\n",
    "            \"Teachers adapted to hybrid learning models combining in-person and virtual instruction.\",\n",
    "            \"The study examined the effectiveness of different approaches to literacy development.\",\n",
    "            \"Higher education institutions faced challenges addressing student mental health needs.\",\n",
    "\n",
    "            \"The film won multiple awards at the festival, receiving critical acclaim for its direction.\",\n",
    "            \"The streaming service announced a slate of original content productions for the coming year.\",\n",
    "            \"The musician's latest album debuted at number one on the charts with record-breaking streams.\",\n",
    "            \"The art exhibition featured works exploring themes of identity and social justice.\",\n",
    "            \"The bestselling novel will be adapted into a television series by the acclaimed director.\",\n",
    "            \"Cultural institutions implemented digital initiatives to reach audiences during closures.\",\n",
    "            \"The celebrity announced their involvement in humanitarian efforts addressing global issues.\",\n",
    "            \"The video game release broke sales records with millions of copies sold in the first week.\",\n",
    "            \"The theater production received standing ovations for its innovative staging and performances.\",\n",
    "            \"Social media influencers partnered with brands on campaigns targeting younger demographics.\",\n",
    "\n",
    "            \"The team secured the championship title after a dramatic overtime victory in the final game.\",\n",
    "            \"The athlete broke the world record that had stood for over a decade in the competition.\",\n",
    "            \"The league announced new safety protocols following concerns about player injuries.\",\n",
    "            \"The international tournament drew record viewership across global broadcasting platforms.\",\n",
    "            \"The coach implemented strategic changes that transformed the team's performance this season.\",\n",
    "            \"The player signed a record-breaking contract extension with the franchise.\",\n",
    "            \"The Olympic committee finalized preparations for the upcoming summer games.\",\n",
    "            \"The sports technology company introduced advanced analytics tools for performance tracking.\",\n",
    "            \"The stadium renovation project will increase capacity and enhance the fan experience.\",\n",
    "            \"The investigation into alleged rule violations resulted in sanctions against the organization.\",\n",
    "\n",
    "            \"The investigation led to multiple arrests in connection with the organized crime operation.\",\n",
    "            \"The court ruling established a precedent for future cases involving digital privacy rights.\",\n",
    "            \"Law enforcement agencies implemented new training programs focused on community relations.\",\n",
    "            \"The jury reached a verdict after deliberating for three days in the high-profile case.\",\n",
    "            \"The legislation introduced stricter penalties for specific categories of financial crimes.\",\n",
    "            \"The forensic evidence played a crucial role in identifying the suspect in the investigation.\",\n",
    "            \"The prison reform initiative aimed to reduce recidivism through education and rehabilitation.\",\n",
    "            \"The legal challenge questioned the constitutionality of the recently passed legislation.\",\n",
    "            \"The settlement agreement included significant compensation for affected individuals.\",\n",
    "            \"The regulatory agency imposed fines following violations of consumer protection standards.\",\n",
    "\n",
    "            \"The housing initiative aimed to address affordability challenges in metropolitan areas.\",\n",
    "            \"Community organizations partnered to provide resources for vulnerable populations.\",\n",
    "            \"The study documented disparities in access to essential services across different demographics.\",\n",
    "            \"Activists organized peaceful demonstrations advocating for policy changes and social justice.\",\n",
    "            \"The report highlighted progress and ongoing challenges in workplace diversity and inclusion.\",\n",
    "            \"Urban development projects focused on revitalizing neighborhoods while preventing displacement.\",\n",
    "            \"The survey revealed changing attitudes toward social issues among younger generations.\",\n",
    "            \"Nonprofit organizations expanded programs addressing food insecurity in underserved communities.\",\n",
    "            \"The panel discussion explored intersections between technology and social equity concerns.\",\n",
    "            \"The initiative provided support services for veterans transitioning to civilian life.\"\n",
    "        ]\n",
    "\n",
    "        specialized_terminology = [\n",
    "            \"Legislation is being debated in parliament regarding constitutional amendments and electoral reform.\",\n",
    "            \"Bipartisan support is needed for the bill to pass through both chambers of congress.\",\n",
    "            \"The filibuster prevented the vote on the controversial legislation despite majority support.\",\n",
    "            \"Gerrymandering has affected district boundaries, potentially impacting election outcomes.\",\n",
    "            \"The caucus meeting determined the party's position on key policy initiatives.\",\n",
    "\n",
    "            \"Quantitative easing measures were implemented by the central bank to stimulate economic growth.\",\n",
    "            \"The yield curve inversion raised concerns about potential recession indicators.\",\n",
    "            \"Fiscal policy adjustments were recommended to address the growing budget deficit.\",\n",
    "            \"Market volatility increased following uncertainty about monetary policy directions.\",\n",
    "            \"The merger and acquisition activity accelerated in the technology sector this quarter.\",\n",
    "\n",
    "            \"The neural network architecture demonstrated superior performance in natural language processing tasks.\",\n",
    "            \"Blockchain technology applications expanded beyond cryptocurrencies into supply chain verification.\",\n",
    "            \"The API integration enabled seamless connectivity between multiple software platforms.\",\n",
    "            \"Machine learning algorithms identified patterns that human analysts had overlooked.\",\n",
    "            \"The Internet of Things devices created a comprehensive monitoring system for the facility.\",\n",
    "\n",
    "            \"The genome sequencing revealed previously unknown genetic markers associated with the condition.\",\n",
    "            \"Quantum entanglement experiments demonstrated non-local correlations between particles.\",\n",
    "            \"The clinical trial entered phase three testing after promising preliminary results.\",\n",
    "            \"Neuroplasticity research suggested new approaches to rehabilitation after brain injury.\",\n",
    "            \"The particle accelerator experiments provided data supporting theoretical predictions.\",\n",
    "\n",
    "            \"Biodiversity conservation efforts focused on protecting critical habitat corridors.\",\n",
    "            \"Carbon sequestration technologies were evaluated for large-scale implementation potential.\",\n",
    "            \"Renewable energy integration presented challenges for existing power grid infrastructure.\",\n",
    "            \"Ecosystem services valuation informed policy decisions regarding natural resource management.\",\n",
    "            \"Climate mitigation strategies included both technological solutions and behavioral changes.\"\n",
    "        ]\n",
    "\n",
    "        comprehensive_corpus = news_corpus + specialized_terminology\n",
    "\n",
    "        corpus_vectorizer = TfidfVectorizer(\n",
    "            ngram_range=(1, 3),\n",
    "            stop_words='english',\n",
    "            max_features=20000,\n",
    "            min_df=2,\n",
    "            max_df=0.9,\n",
    "            norm='l2',\n",
    "            use_idf=True,\n",
    "            smooth_idf=True,\n",
    "            sublinear_tf=True\n",
    "        )\n",
    "\n",
    "        corpus_vectorizer.fit(comprehensive_corpus)\n",
    "\n",
    "        try:\n",
    "            feature_names = corpus_vectorizer.get_feature_names_out()\n",
    "        except AttributeError:\n",
    "            feature_names = corpus_vectorizer.get_feature_names()\n",
    "\n",
    "        self.idf_values = {}\n",
    "        for term, idf_idx in corpus_vectorizer.vocabulary_.items():\n",
    "            self.idf_values[term] = corpus_vectorizer.idf_[idf_idx]\n",
    "\n",
    "        print(f\"Created comprehensive IDF corpus with {len(self.idf_values)} terms\")\n",
    "\n",
    "        additional_terms = {\n",
    "            \"breaking news\": 2.0,\n",
    "            \"exclusive report\": 3.0,\n",
    "            \"sources say\": 1.8,\n",
    "            \"according to officials\": 1.5,\n",
    "            \"press conference\": 2.2,\n",
    "            \"official statement\": 2.3,\n",
    "            \"developing story\": 2.1,\n",
    "            \"anonymous source\": 2.8,\n",
    "            \"public opinion\": 2.5,\n",
    "            \"latest update\": 1.9,\n",
    "\n",
    "            \"united nations\": 4.0,\n",
    "            \"world health organization\": 4.2,\n",
    "            \"european union\": 3.8,\n",
    "            \"federal reserve\": 4.1,\n",
    "            \"supreme court\": 3.9,\n",
    "            \"white house\": 3.7,\n",
    "            \"wall street\": 3.6,\n",
    "            \"silicon valley\": 4.3,\n",
    "            \"climate change\": 3.5,\n",
    "            \"artificial intelligence\": 4.5\n",
    "        }\n",
    "\n",
    "        for term, idf in additional_terms.items():\n",
    "            self.idf_values[term] = idf\n",
    "\n",
    "        print(f\"Added {len(additional_terms)} additional news-specific terms\")\n",
    "        print(f\"Final IDF corpus contains {len(self.idf_values)} terms\")\n",
    "\n",
    "        if corpus_path:\n",
    "            try:\n",
    "                os.makedirs(os.path.dirname(corpus_path), exist_ok=True)\n",
    "                with open(corpus_path, 'wb') as f:\n",
    "                    pickle.dump(self.idf_values, f)\n",
    "                print(f\"Saved IDF corpus to {corpus_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving IDF corpus: {e}\")\n",
    "\n",
    "    def normalize_text(self, text: str) -> str:\n",
    "        \n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "        text = re.sub(r'[^\\w\\s\\-\\']', ' ', text)\n",
    "\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "        return text\n",
    "\n",
    "    def clean_phrase_boundaries(self, phrase: str) -> str:\n",
    "        \n",
    "        if not self.clean_boundaries:\n",
    "            return phrase\n",
    "\n",
    "        words = phrase.split()\n",
    "\n",
    "        while words and words[0].lower() in self.BOUNDARY_WORDS:\n",
    "            words.pop(0)\n",
    "\n",
    "        while words and words[-1].lower() in self.BOUNDARY_WORDS:\n",
    "            words.pop()\n",
    "\n",
    "        cleaned_phrase = ' '.join(words)\n",
    "\n",
    "        return cleaned_phrase\n",
    "\n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        \n",
    "        text = self.normalize_text(text)\n",
    "\n",
    "        doc = self.nlp(text)\n",
    "\n",
    "        processed_tokens = []\n",
    "\n",
    "        entities_spans = [(ent.start, ent.end, ent.text) for ent in doc.ents]\n",
    "        acronyms = [token.text for token in doc if token.text.isupper() and len(token.text) > 1]\n",
    "\n",
    "        compound_terms = []\n",
    "        technical_prefixes = ['pre-', 'post-', 'multi-', 'inter-', 'intra-', 'micro-', 'macro-', 'nano-', 'cyber-']\n",
    "\n",
    "        for i, token in enumerate(doc):\n",
    "            if '-' in token.text and not token.is_punct:\n",
    "                compound_terms.append((token.i, token.i + 1, token.text))\n",
    "\n",
    "            if i < len(doc) - 1:\n",
    "                for prefix in technical_prefixes:\n",
    "                    if token.text.lower().endswith(prefix[:-1]) and not doc[i+1].is_punct:\n",
    "                        compound_terms.append((token.i, token.i + 2, f\"{token.text}{doc[i+1].text}\"))\n",
    "\n",
    "        i = 0\n",
    "        while i < len(doc):\n",
    "            token = doc[i]\n",
    "\n",
    "            if token.is_space:\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "            is_compound_start = any(start == token.i for start, _, _ in compound_terms)\n",
    "            is_compound_part = any(start < token.i < end for start, end, _ in compound_terms)\n",
    "\n",
    "            is_entity = any(start <= token.i < end for start, end, _ in entities_spans)\n",
    "\n",
    "            if is_compound_start:\n",
    "                for start, end, text in compound_terms:\n",
    "                    if start == token.i:\n",
    "                        processed_tokens.append(text)\n",
    "                        i = end\n",
    "                        break\n",
    "            elif is_compound_part:\n",
    "                i += 1\n",
    "            elif (is_entity or token.text in acronyms or token.pos_ == \"PROPN\" or\n",
    "                (token.text[0].isupper() and i > 0 and not doc[i-1].text.endswith('.'))):\n",
    "                processed_tokens.append(token.text)\n",
    "                i += 1\n",
    "            else:\n",
    "                processed_tokens.append(token.text.lower())\n",
    "                i += 1\n",
    "\n",
    "        processed_text = \" \".join(processed_tokens)\n",
    "\n",
    "        technical_patterns = [\n",
    "            (r'(\\w+)-of-(\\w+)', r'\\1_of_\\2'),\n",
    "            (r'(\\w+)-to-(\\w+)', r'\\1_to_\\2'),\n",
    "            (r'(\\w+)-(\\w+)-(\\w+)', r'\\1_\\2_\\3'),\n",
    "        ]\n",
    "\n",
    "        for pattern, replacement in technical_patterns:\n",
    "            processed_text = re.sub(pattern, replacement, processed_text)\n",
    "\n",
    "        return processed_text\n",
    "    def get_named_entities(self, text: str) -> List[Tuple[str, str]]:\n",
    "        \n",
    "        doc = self.nlp(text)\n",
    "        entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "        return entities\n",
    "\n",
    "    def get_noun_chunks(self, text: str) -> List[str]:\n",
    "        \n",
    "        doc = self.nlp(text)\n",
    "        chunks = [chunk.text.lower() for chunk in doc.noun_chunks]\n",
    "\n",
    "        if self.clean_boundaries:\n",
    "            chunks = [self.clean_phrase_boundaries(chunk) for chunk in chunks]\n",
    "\n",
    "        chunks = [chunk for chunk in chunks if chunk and chunk not in self.all_stopwords]\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    def get_custom_stopwords(self, text: str) -> Set[str]:\n",
    "        \n",
    "        return self.all_stopwords\n",
    "\n",
    "    def extract_title_and_lead(self, text: str) -> Tuple[str, str, str]:\n",
    "        \n",
    "        lines = text.split('\\n')\n",
    "\n",
    "        title = \"\"\n",
    "        for line in lines:\n",
    "            if line.strip():\n",
    "                title = line.strip()\n",
    "                break\n",
    "\n",
    "        paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
    "\n",
    "        lead_paragraph = \"\"\n",
    "        if len(paragraphs) > 1 and title in paragraphs[0]:\n",
    "            lead_paragraph = paragraphs[1]\n",
    "        elif paragraphs:\n",
    "            lead_paragraph = paragraphs[0]\n",
    "            if lead_paragraph == title and len(paragraphs) > 1:\n",
    "                lead_paragraph = paragraphs[1]\n",
    "\n",
    "        first_paragraphs = \"\"\n",
    "        if len(paragraphs) > 1:\n",
    "            if title in paragraphs[0] and paragraphs[0] == title:\n",
    "                first_paragraphs = \" \".join(paragraphs[1:min(4, len(paragraphs))])\n",
    "            else:\n",
    "                first_paragraphs = \" \".join(paragraphs[:min(3, len(paragraphs))])\n",
    "        else:\n",
    "            first_paragraphs = lead_paragraph\n",
    "\n",
    "        return title, lead_paragraph, first_paragraphs\n",
    "\n",
    "    def extract_with_keybert(self, text: str, nr_candidates: int = 20) -> List[Tuple[str, float]]:\n",
    "        \n",
    "        try:\n",
    "            custom_stopwords = self.get_custom_stopwords(text)\n",
    "\n",
    "            if self.method == 'mmr':\n",
    "                keyphrases = self.kw_model.extract_keywords(\n",
    "                    text,\n",
    "                    keyphrase_ngram_range=self.ngram_range,\n",
    "                    stop_words=custom_stopwords,\n",
    "                    use_mmr=True,\n",
    "                    diversity=self.diversity_penalty,\n",
    "                    top_n=nr_candidates\n",
    "                )\n",
    "            else:\n",
    "                keyphrases = self.kw_model.extract_keywords(\n",
    "                    text,\n",
    "                    keyphrase_ngram_range=self.ngram_range,\n",
    "                    stop_words=custom_stopwords,\n",
    "                    use_mmr=False,\n",
    "                    top_n=nr_candidates\n",
    "                )\n",
    "\n",
    "            formatted_keyphrases = []\n",
    "            for kp_item in keyphrases:\n",
    "                if isinstance(kp_item, tuple) and len(kp_item) == 2:\n",
    "                    if isinstance(kp_item[0], str):\n",
    "                        kp, score = kp_item\n",
    "                    else:\n",
    "                        score, kp = kp_item\n",
    "\n",
    "                    if self.clean_boundaries:\n",
    "                        kp = self.clean_phrase_boundaries(kp)\n",
    "\n",
    "                    if kp:\n",
    "                        formatted_keyphrases.append((kp, score))\n",
    "                else:\n",
    "                    print(f\"Skipping invalid keyphrase item: {kp_item}\")\n",
    "\n",
    "            if self.use_position_weight and formatted_keyphrases:\n",
    "                position_weights = {}\n",
    "                for kp, _ in formatted_keyphrases:\n",
    "                    weight = self.calculate_position_weight(text, kp)\n",
    "                    position_weights[kp] = weight\n",
    "                    print(f\"DEBUG: Position weight for '{kp}': {weight:.2f}\")\n",
    "\n",
    "                formatted_keyphrases = [(kp, score * position_weights[kp]) for kp, score in formatted_keyphrases]\n",
    "\n",
    "            if self.use_tfidf_weight and formatted_keyphrases:\n",
    "                tfidf_weights = self.calculate_tfidf_weights(text, [kp for kp, _ in formatted_keyphrases])\n",
    "\n",
    "                for kp, weight in tfidf_weights.items():\n",
    "                    print(f\"DEBUG: TF-IDF weight for '{kp}': {weight:.2f}\")\n",
    "\n",
    "                formatted_keyphrases = [(kp, score * tfidf_weights[kp]) for kp, score in formatted_keyphrases]\n",
    "\n",
    "            if self.prioritize_named_entities:\n",
    "                entities = self.get_named_entities(text)\n",
    "                for entity, entity_type in entities:\n",
    "                    if entity_type in ['ORG', 'PRODUCT', 'PERSON', 'GPE', 'WORK_OF_ART', 'EVENT']:\n",
    "                        entity_lower = entity.lower()\n",
    "                        exists = False\n",
    "                        for i, (kp, score) in enumerate(formatted_keyphrases):\n",
    "                            if entity_lower == kp.lower():\n",
    "                                formatted_keyphrases[i] = (kp, min(1.0, score * 1.1))\n",
    "                                exists = True\n",
    "                                break\n",
    "\n",
    "                        if not exists and len(entity.split()) <= self.ngram_range[1]:\n",
    "                            formatted_keyphrases.append((entity, 0.6))\n",
    "\n",
    "            if self.use_noun_chunks:\n",
    "                chunks = self.get_noun_chunks(text)\n",
    "                for chunk in chunks:\n",
    "                    if len(chunk.split()) <= self.ngram_range[1]:\n",
    "                        chunk_lower = chunk.lower()\n",
    "                        exists = False\n",
    "                        for kp, _ in formatted_keyphrases:\n",
    "                            if chunk_lower == kp.lower():\n",
    "                                exists = True\n",
    "                                break\n",
    "\n",
    "                        if not exists:\n",
    "                            formatted_keyphrases.append((chunk, 0.5))\n",
    "\n",
    "            if self.boost_exact_matches:\n",
    "                important_phrases = []\n",
    "\n",
    "                quoted_phrases = re.findall(r'\"([^\"]+)\"', text)\n",
    "                important_phrases.extend(quoted_phrases)\n",
    "\n",
    "                capitalized_phrases = re.findall(r'\\b([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)\\b', text)\n",
    "                important_phrases.extend(capitalized_phrases)\n",
    "\n",
    "                for phrase in important_phrases:\n",
    "                    phrase_lower = phrase.lower()\n",
    "                    exists = False\n",
    "                    for i, (kp, score) in enumerate(formatted_keyphrases):\n",
    "                        if phrase_lower == kp.lower():\n",
    "                            formatted_keyphrases[i] = (kp, min(1.0, score * 1.2))\n",
    "                            exists = True\n",
    "                            break\n",
    "\n",
    "                    if not exists and len(phrase.split()) <= self.ngram_range[1]:\n",
    "                        formatted_keyphrases.append((phrase, 0.7))\n",
    "\n",
    "            return formatted_keyphrases\n",
    "        except Exception as e:\n",
    "            print(f\"Error in KeyBERT extraction: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def lemmatize_text(self, text: str) -> str:\n",
    "        \n",
    "        if not self.use_lemmatization:\n",
    "            return text\n",
    "\n",
    "        try:\n",
    "            doc = self.nlp(text)\n",
    "\n",
    "            lemmatized_tokens = []\n",
    "            for token in doc:\n",
    "                if token.pos_ == 'NOUN':\n",
    "                    pos = 'n'\n",
    "                elif token.pos_ == 'VERB':\n",
    "                    pos = 'v'\n",
    "                elif token.pos_ == 'ADJ':\n",
    "                    pos = 'a'\n",
    "                elif token.pos_ == 'ADV':\n",
    "                    pos = 'r'\n",
    "                else:\n",
    "                    lemmatized_tokens.append(token.text)\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    lemmatized_tokens.append(self.lemmatizer.lemmatize(token.text.lower(), pos))\n",
    "                except Exception:\n",
    "                    lemmatized_tokens.append(token.text.lower())\n",
    "\n",
    "            lemmatized_text = ' '.join(lemmatized_tokens).strip()\n",
    "\n",
    "            return lemmatized_text\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Lemmatization failed: {e}\")\n",
    "            return text\n",
    "\n",
    "    def calculate_position_weight(self, text: str, keyphrase: str) -> float:\n",
    "        \n",
    "        if not self.use_position_weight:\n",
    "            return 1.0\n",
    "\n",
    "        keyphrase_lower = keyphrase.lower()\n",
    "        text_lower = text.lower()\n",
    "\n",
    "        position = text_lower.find(keyphrase_lower)\n",
    "\n",
    "        if position == -1:\n",
    "            return 0.5\n",
    "\n",
    "        if self.use_title_lead_boost:\n",
    "            title, lead_paragraph, first_paragraphs = self.extract_title_and_lead(text)\n",
    "            title_lower = title.lower()\n",
    "            lead_lower = lead_paragraph.lower()\n",
    "            first_paragraphs_lower = first_paragraphs.lower()\n",
    "\n",
    "            title_exact_match = keyphrase_lower in title_lower\n",
    "            lead_exact_match = keyphrase_lower in lead_lower\n",
    "            first_paragraphs_exact_match = keyphrase_lower in first_paragraphs_lower and not title_exact_match and not lead_exact_match\n",
    "\n",
    "            kp_words = set(keyphrase_lower.split())\n",
    "            title_words = set(title_lower.split())\n",
    "            lead_words = set(lead_lower.split())\n",
    "\n",
    "            title_word_overlap = len(kp_words.intersection(title_words)) / len(kp_words) if kp_words else 0\n",
    "            lead_word_overlap = len(kp_words.intersection(lead_words)) / len(kp_words) if kp_words else 0\n",
    "\n",
    "            if title_exact_match:\n",
    "                return 2.0\n",
    "            elif lead_exact_match:\n",
    "                return 1.5\n",
    "            elif first_paragraphs_exact_match:\n",
    "                return 1.3\n",
    "            elif title_word_overlap > 0.75:\n",
    "                return 1.4\n",
    "            elif lead_word_overlap > 0.75:\n",
    "                return 1.3\n",
    "            elif title_word_overlap > 0.5 or lead_word_overlap > 0.5:\n",
    "                return 1.2\n",
    "\n",
    "        relative_position = position / len(text)\n",
    "\n",
    "        if relative_position < 0.01:\n",
    "            weight = 2.0\n",
    "        else:\n",
    "            weight = 0.5 + 1.5 * (math.log(1 + 10 * (1 - relative_position)) / math.log(11))\n",
    "\n",
    "        normalized_weight = min(2.0, max(0.5, weight))\n",
    "\n",
    "        return normalized_weight\n",
    "\n",
    "    def calculate_tfidf_weights(self, text: str, keyphrases: List[str]) -> Dict[str, float]:\n",
    "        \n",
    "        if not keyphrases:\n",
    "            return {}\n",
    "\n",
    "        try:\n",
    "            preprocessed_text = self.preprocess_text(text)\n",
    "\n",
    "            tokens = preprocessed_text.lower().split()\n",
    "\n",
    "            term_freq = {}\n",
    "            for token in tokens:\n",
    "                if token not in term_freq:\n",
    "                    term_freq[token] = 0\n",
    "                term_freq[token] += 1\n",
    "\n",
    "            for token in term_freq:\n",
    "                term_freq[token] = 1 + math.log(term_freq[token])\n",
    "\n",
    "            word_tfidf = {}\n",
    "            for token, tf in term_freq.items():\n",
    "                if token in self.idf_values:\n",
    "                    idf = self.idf_values[token]\n",
    "                else:\n",
    "                    idf = math.log(30 + 1)\n",
    "\n",
    "                    self.idf_values[token] = idf\n",
    "\n",
    "                word_tfidf[token] = tf * idf\n",
    "\n",
    "            self.word_tfidf_cache = word_tfidf\n",
    "\n",
    "            keyphrase_weights = {}\n",
    "            for keyphrase in keyphrases:\n",
    "                words = keyphrase.lower().split()\n",
    "\n",
    "                word_scores = []\n",
    "                for word in words:\n",
    "                    if word in word_tfidf:\n",
    "                        word_scores.append(word_tfidf[word])\n",
    "                    elif word in self.idf_values:\n",
    "                        word_scores.append(0.5 * self.idf_values[word])\n",
    "                    else:\n",
    "                        word_scores.append(0.5)\n",
    "\n",
    "                if word_scores:\n",
    "                    total_weight = 0\n",
    "                    total_score = 0\n",
    "                    for i, score in enumerate(word_scores):\n",
    "                        if len(word_scores) > 1:\n",
    "                            if i == 0 or i == len(word_scores) - 1:\n",
    "                                pos_weight = 1.5\n",
    "                            else:\n",
    "                                pos_weight = 1.0\n",
    "                        else:\n",
    "                            pos_weight = 1.0\n",
    "\n",
    "                        total_weight += pos_weight\n",
    "                        total_score += score * pos_weight\n",
    "\n",
    "                    avg_score = total_score / total_weight\n",
    "\n",
    "                    if len(words) > 1:\n",
    "                        length_bonus = 1.0 + 0.1 * (len(words) - 1)\n",
    "                        keyphrase_weights[keyphrase] = avg_score * length_bonus\n",
    "                    else:\n",
    "                        keyphrase_weights[keyphrase] = avg_score\n",
    "                else:\n",
    "                    keyphrase_weights[keyphrase] = 0.5\n",
    "\n",
    "            if keyphrase_weights:\n",
    "                max_weight = max(keyphrase_weights.values())\n",
    "                min_weight = min(keyphrase_weights.values())\n",
    "\n",
    "                if max_weight > min_weight:\n",
    "                    for kp in keyphrase_weights:\n",
    "                        keyphrase_weights[kp] = 0.5 + ((keyphrase_weights[kp] - min_weight) /\n",
    "                                                    (max_weight - min_weight)) * 1.5\n",
    "                else:\n",
    "                    for kp in keyphrase_weights:\n",
    "                        keyphrase_weights[kp] = 1.0\n",
    "\n",
    "            return keyphrase_weights\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating TF-IDF weights: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return {kp: 1.0 for kp in keyphrases}\n",
    "\n",
    "    def update_idf_corpus(self, documents: List[str], save_path: str = None):\n",
    "        \n",
    "        if not documents:\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            update_vectorizer = TfidfVectorizer(\n",
    "                ngram_range=(1, 2),\n",
    "                stop_words='english',\n",
    "                max_features=10000,\n",
    "                min_df=1,\n",
    "                norm='l2',\n",
    "                use_idf=True,\n",
    "                smooth_idf=True,\n",
    "                sublinear_tf=True\n",
    "            )\n",
    "\n",
    "            update_vectorizer.fit(documents)\n",
    "\n",
    "            try:\n",
    "                feature_names = update_vectorizer.get_feature_names_out()\n",
    "            except AttributeError:\n",
    "                feature_names = update_vectorizer.get_feature_names()\n",
    "\n",
    "            for term, idf_idx in update_vectorizer.vocabulary_.items():\n",
    "                if term in self.idf_values:\n",
    "                    self.idf_values[term] = 0.7 * self.idf_values[term] + 0.3 * update_vectorizer.idf_[idf_idx]\n",
    "                else:\n",
    "                    self.idf_values[term] = update_vectorizer.idf_[idf_idx]\n",
    "\n",
    "            print(f\"Updated IDF corpus, now contains {len(self.idf_values)} terms\")\n",
    "\n",
    "            if save_path:\n",
    "                try:\n",
    "                    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "                    with open(save_path, 'wb') as f:\n",
    "                        pickle.dump(self.idf_values, f)\n",
    "                    print(f\"Saved updated IDF corpus to {save_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error saving updated IDF corpus: {e}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error updating IDF corpus: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "    def extract_with_multipartiterank(self, text: str, nr_candidates: int = 20) -> List[Tuple[str, float]]:\n",
    "        \n",
    "        extractor = pke.unsupervised.MultipartiteRank()\n",
    "\n",
    "        custom_stopwords = self.get_custom_stopwords(text)\n",
    "\n",
    "        try:\n",
    "            extractor.load_document(input=text, language=self.language)\n",
    "\n",
    "            extractor.stoplist = list(custom_stopwords)\n",
    "\n",
    "            extractor.candidate_selection(\n",
    "                pos={'NOUN', 'PROPN', 'ADJ'}\n",
    "            )\n",
    "\n",
    "            extractor.candidate_weighting(\n",
    "                alpha=self.alpha,\n",
    "                threshold=self.threshold,\n",
    "                method='average'\n",
    "            )\n",
    "\n",
    "            keyphrases = extractor.get_n_best(n=nr_candidates)\n",
    "\n",
    "            keyphrases = self.post_process_keyphrases(keyphrases, text)\n",
    "\n",
    "            if self.use_position_weight:\n",
    "                keyphrases = [(self.clean_phrase_boundaries(kp), score) for kp, score in keyphrases]\n",
    "\n",
    "            keyphrases = [(kp, score) for kp, score in keyphrases if kp]\n",
    "\n",
    "            if self.use_position_weight:\n",
    "                position_weights = {}\n",
    "                for kp, _ in keyphrases:\n",
    "                    weight = self.calculate_position_weight(text, kp)\n",
    "                    position_weights[kp] = weight\n",
    "                    print(f\"DEBUG: MultipartiteRank position weight for '{kp}': {weight:.2f}\")\n",
    "\n",
    "                keyphrases = [(kp, score * position_weights[kp]) for kp, score in keyphrases]\n",
    "\n",
    "            if self.use_tfidf_weight:\n",
    "                tfidf_weights = self.calculate_tfidf_weights(text, [kp for kp, _ in keyphrases])\n",
    "\n",
    "                for kp, weight in tfidf_weights.items():\n",
    "                    print(f\"DEBUG: MultipartiteRank TF-IDF weight for '{kp}': {weight:.2f}\")\n",
    "\n",
    "                keyphrases = [(kp, score * tfidf_weights[kp]) for kp, score in keyphrases]\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in MultipartiteRank: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            keyphrases = []\n",
    "\n",
    "        return keyphrases\n",
    "\n",
    "    def extract_with_yake(self, text: str, nr_candidates: int = 20) -> List[Tuple[str, float]]:\n",
    "        \n",
    "        extractor = pke.unsupervised.YAKE()\n",
    "\n",
    "        try:\n",
    "            extractor.load_document(input=text, language=self.language, normalization=None)\n",
    "\n",
    "            extractor.candidate_selection(n=self.ngram_range[1])\n",
    "\n",
    "            extractor.candidate_weighting(window=3, use_stems=False)\n",
    "\n",
    "            raw_keyphrases = extractor.get_n_best(n=nr_candidates)\n",
    "\n",
    "            if raw_keyphrases:\n",
    "                max_score = max([score for _, score in raw_keyphrases])\n",
    "                keyphrases = [(kp, 1.0 - (score / max_score)) for kp, score in raw_keyphrases]\n",
    "            else:\n",
    "                keyphrases = []\n",
    "\n",
    "            if self.clean_boundaries:\n",
    "                keyphrases = [(self.clean_phrase_boundaries(kp), score) for kp, score in keyphrases]\n",
    "\n",
    "            keyphrases = [(kp, score) for kp, score in keyphrases if kp]\n",
    "\n",
    "            if self.use_position_weight:\n",
    "                position_weights = {}\n",
    "                for kp, _ in keyphrases:\n",
    "                    weight = self.calculate_position_weight(text, kp)\n",
    "                    position_weights[kp] = weight\n",
    "                    print(f\"DEBUG: YAKE position weight for '{kp}': {weight:.2f}\")\n",
    "\n",
    "                keyphrases = [(kp, score * position_weights[kp]) for kp, score in keyphrases]\n",
    "\n",
    "            if self.use_tfidf_weight:\n",
    "                tfidf_weights = self.calculate_tfidf_weights(text, [kp for kp, _ in keyphrases])\n",
    "\n",
    "                for kp, weight in tfidf_weights.items():\n",
    "                    print(f\"DEBUG: YAKE TF-IDF weight for '{kp}': {weight:.2f}\")\n",
    "\n",
    "                keyphrases = [(kp, score * tfidf_weights[kp]) for kp, score in keyphrases]\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in YAKE: {str(e)}\")\n",
    "            keyphrases = []\n",
    "\n",
    "        return keyphrases\n",
    "\n",
    "    def extract_with_textrank(self, text: str, nr_candidates: int = 20) -> List[Tuple[str, float]]:\n",
    "        \n",
    "        extractor = pke.unsupervised.TextRank()\n",
    "\n",
    "        custom_stopwords = self.get_custom_stopwords(text)\n",
    "\n",
    "        try:\n",
    "            extractor.load_document(input=text, language=self.language)\n",
    "\n",
    "            extractor.stoplist = list(custom_stopwords)\n",
    "\n",
    "            extractor.candidate_selection(\n",
    "                pos={'NOUN', 'PROPN', 'ADJ', 'VERB'}\n",
    "            )\n",
    "\n",
    "            extractor.candidate_weighting(\n",
    "                window=5,\n",
    "                pos={'NOUN', 'PROPN', 'ADJ', 'VERB'},\n",
    "                top_percent=0.33\n",
    "            )\n",
    "\n",
    "            keyphrases = extractor.get_n_best(n=nr_candidates)\n",
    "\n",
    "            if self.clean_boundaries:\n",
    "                keyphrases = [(self.clean_phrase_boundaries(kp), score) for kp, score in keyphrases]\n",
    "\n",
    "            keyphrases = [(kp, score) for kp, score in keyphrases if kp]\n",
    "\n",
    "            if self.use_position_weight:\n",
    "                position_weights = {}\n",
    "                for kp, _ in keyphrases:\n",
    "                    weight = self.calculate_position_weight(text, kp)\n",
    "                    position_weights[kp] = weight\n",
    "                    print(f\"DEBUG: TextRank position weight for '{kp}': {weight:.2f}\")\n",
    "\n",
    "                keyphrases = [(kp, score * position_weights[kp]) for kp, score in keyphrases]\n",
    "\n",
    "            if self.use_tfidf_weight:\n",
    "                tfidf_weights = self.calculate_tfidf_weights(text, [kp for kp, _ in keyphrases])\n",
    "\n",
    "                for kp, weight in tfidf_weights.items():\n",
    "                    print(f\"DEBUG: TextRank TF-IDF weight for '{kp}': {weight:.2f}\")\n",
    "\n",
    "                keyphrases = [(kp, score * tfidf_weights[kp]) for kp, score in keyphrases]\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in TextRank: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            keyphrases = []\n",
    "\n",
    "        return keyphrases\n",
    "\n",
    "    def classify_document_type(self, text: str) -> Dict[str, float]:\n",
    "        \n",
    "        doc_characteristics = {\n",
    "            'length': len(text),\n",
    "            'avg_sentence_length': 0,\n",
    "            'technical_density': 0,\n",
    "            'domain_scores': {\n",
    "                'technology': 0.0,\n",
    "                'business': 0.0,\n",
    "                'health': 0.0,\n",
    "                'science': 0.0,\n",
    "                'news': 0.0,\n",
    "                'academic': 0.0,\n",
    "                'politics': 0.0,\n",
    "                'environment': 0.0,\n",
    "                'entertainment': 0.0,\n",
    "                'sports': 0.0\n",
    "            },\n",
    "            'structure_scores': {\n",
    "                'formal': 0.0,\n",
    "                'informal': 0.0,\n",
    "                'narrative': 0.0,\n",
    "                'descriptive': 0.0,\n",
    "                'technical': 0.0\n",
    "            }\n",
    "        }\n",
    "\n",
    "        doc = self.nlp(text[:10000])\n",
    "\n",
    "        sentences = list(doc.sents)\n",
    "        if sentences:\n",
    "            doc_characteristics['avg_sentence_length'] = sum(len(sent) for sent in sentences) / len(sentences)\n",
    "\n",
    "        technical_terms = 0\n",
    "        for token in doc:\n",
    "            if (token.pos_ in ['NOUN', 'PROPN'] and\n",
    "                token.is_alpha and\n",
    "                not token.is_stop and\n",
    "                len(token.text) > 3):\n",
    "                if token.text.lower() in self.idf_values and self.idf_values[token.text.lower()] > 5.0:\n",
    "                    technical_terms += 1\n",
    "\n",
    "        if len(doc) > 0:\n",
    "            doc_characteristics['technical_density'] = technical_terms / len(doc)\n",
    "\n",
    "        domain_keywords = {\n",
    "            'technology': [\n",
    "                'technology', 'digital', 'software', 'hardware', 'data', 'internet', 'online',\n",
    "                'app', 'application', 'computer', 'computing', 'network', 'system', 'platform',\n",
    "\n",
    "                'ai', 'artificial intelligence', 'machine learning', 'deep learning', 'algorithm',\n",
    "                'cloud', 'cybersecurity', 'security', 'privacy', 'encryption', 'blockchain',\n",
    "                'automation', 'robot', 'robotics', 'iot', 'internet of things', 'virtual reality', 'vr',\n",
    "                'augmented reality', 'ar', 'analytics', 'big data', 'database', 'programming',\n",
    "\n",
    "                'google', 'microsoft', 'apple', 'amazon', 'facebook', 'meta', 'tesla', 'ibm',\n",
    "                'intel', 'nvidia', 'samsung', 'oracle', 'cisco', 'twitter', 'linkedin', 'tiktok',\n",
    "\n",
    "                'smartphone', 'mobile', 'website', 'browser', 'search engine', 'social media',\n",
    "                'email', 'e-commerce', 'streaming', 'wifi', 'bluetooth', 'server', 'chip',\n",
    "                'processor', 'interface', 'api', 'code', 'coding', 'developer', 'startup'\n",
    "            ],\n",
    "\n",
    "            'business': [\n",
    "                'business', 'company', 'corporation', 'enterprise', 'industry', 'market', 'firm',\n",
    "                'commercial', 'corporate', 'trade', 'commerce', 'sales', 'retail', 'wholesale',\n",
    "\n",
    "                'finance', 'financial', 'investment', 'investor', 'stock', 'share', 'shareholder',\n",
    "                'profit', 'revenue', 'income', 'earnings', 'loss', 'budget', 'funding', 'venture capital',\n",
    "                'capital', 'asset', 'liability', 'equity', 'dividend', 'portfolio', 'merger', 'acquisition',\n",
    "\n",
    "                'management', 'executive', 'ceo', 'cfo', 'coo', 'board', 'director', 'leadership',\n",
    "                'strategy', 'strategic', 'operation', 'operational', 'performance', 'productivity',\n",
    "\n",
    "                'marketing', 'advertising', 'brand', 'consumer', 'customer', 'client', 'product',\n",
    "                'service', 'market share', 'competition', 'competitive', 'pricing', 'promotion',\n",
    "\n",
    "                'economy', 'economic', 'gdp', 'growth', 'recession', 'inflation', 'deflation',\n",
    "                'unemployment', 'employment', 'labor', 'workforce', 'supply', 'demand', 'sector'\n",
    "            ],\n",
    "\n",
    "            'health': [\n",
    "                'health', 'healthcare', 'medical', 'medicine', 'clinical', 'hospital', 'doctor',\n",
    "                'physician', 'nurse', 'patient', 'treatment', 'therapy', 'diagnosis', 'prognosis',\n",
    "\n",
    "                'disease', 'disorder', 'condition', 'syndrome', 'infection', 'virus', 'bacterial',\n",
    "                'chronic', 'acute', 'symptom', 'pain', 'inflammation', 'cancer', 'diabetes',\n",
    "                'heart disease', 'stroke', 'alzheimer', 'dementia', 'obesity', 'hypertension',\n",
    "\n",
    "                'public health', 'epidemic', 'pandemic', 'outbreak', 'vaccination', 'vaccine',\n",
    "                'immunization', 'prevention', 'screening', 'mortality', 'morbidity', 'life expectancy',\n",
    "\n",
    "                'insurance', 'medicare', 'medicaid', 'pharmacy', 'pharmaceutical', 'drug', 'medication',\n",
    "                'prescription', 'clinic', 'emergency', 'surgery', 'surgical', 'specialist', 'primary care',\n",
    "\n",
    "                'wellness', 'fitness', 'nutrition', 'diet', 'exercise', 'mental health', 'psychology',\n",
    "                'psychiatry', 'therapy', 'counseling', 'wellbeing', 'lifestyle', 'stress', 'anxiety',\n",
    "                'depression'\n",
    "            ],\n",
    "\n",
    "            'science': [\n",
    "                'science', 'scientific', 'research', 'researcher', 'scientist', 'laboratory', 'lab',\n",
    "                'experiment', 'experimental', 'theory', 'theoretical', 'hypothesis', 'evidence',\n",
    "                'discovery', 'innovation', 'breakthrough',\n",
    "\n",
    "                'physics', 'physical', 'particle', 'quantum', 'relativity', 'gravity', 'energy',\n",
    "                'matter', 'atom', 'nuclear', 'electron', 'proton', 'neutron', 'radiation',\n",
    "\n",
    "                'chemistry', 'chemical', 'molecule', 'molecular', 'compound', 'element', 'reaction',\n",
    "                'catalyst', 'acid', 'base', 'organic', 'inorganic', 'polymer',\n",
    "\n",
    "                'biology', 'biological', 'cell', 'cellular', 'gene', 'genetic', 'dna', 'rna',\n",
    "                'protein', 'enzyme', 'organism', 'species', 'evolution', 'evolutionary',\n",
    "                'ecology', 'ecosystem', 'biodiversity',\n",
    "\n",
    "                'astronomy', 'astronomical', 'space', 'planet', 'planetary', 'star', 'stellar',\n",
    "                'galaxy', 'cosmic', 'universe', 'solar system', 'nasa', 'telescope', 'satellite',\n",
    "                'rocket', 'spacecraft', 'mission', 'orbit', 'mars', 'moon', 'jupiter'\n",
    "            ],\n",
    "\n",
    "            'news': [\n",
    "                'news', 'report', 'reported', 'reporting', 'journalist', 'journalism', 'media',\n",
    "                'press', 'broadcast', 'coverage', 'correspondent', 'reporter', 'editor',\n",
    "\n",
    "                'today', 'yesterday', 'this week', 'this month', 'this year', 'breaking',\n",
    "                'latest', 'update', 'developing', 'recent', 'current', 'ongoing',\n",
    "\n",
    "                'according to', 'sources', 'officials', 'authorities', 'spokesperson',\n",
    "                'statement', 'announced', 'confirmed', 'denied', 'claimed', 'alleged',\n",
    "\n",
    "                'incident', 'event', 'situation', 'development', 'crisis', 'scandal',\n",
    "                'controversy', 'investigation', 'probe', 'inquiry', 'hearing', 'testimony',\n",
    "\n",
    "                'exclusive', 'special report', 'analysis', 'opinion', 'editorial', 'feature',\n",
    "                'interview', 'profile', 'survey', 'poll', 'briefing', 'recap', 'highlights'\n",
    "            ],\n",
    "\n",
    "            'academic': [\n",
    "                'research', 'study', 'analysis', 'investigation', 'experiment', 'observation',\n",
    "                'survey', 'review', 'meta-analysis', 'literature review', 'case study',\n",
    "\n",
    "                'paper', 'article', 'publication', 'journal', 'thesis', 'dissertation',\n",
    "                'monograph', 'proceedings', 'abstract', 'introduction', 'methodology',\n",
    "                'results', 'discussion', 'conclusion', 'references', 'bibliography',\n",
    "\n",
    "                'theory', 'framework', 'model', 'paradigm', 'concept', 'hypothesis',\n",
    "                'variable', 'correlation', 'causation', 'significance', 'validity',\n",
    "                'reliability', 'replication', 'peer review', 'citation', 'impact factor',\n",
    "\n",
    "                'university', 'college', 'institution', 'department', 'faculty', 'school',\n",
    "                'academy', 'institute', 'laboratory', 'center', 'professor', 'researcher',\n",
    "                'scholar', 'academic', 'student', 'undergraduate', 'graduate', 'postgraduate',\n",
    "                'doctoral', 'phd', 'postdoc'\n",
    "            ],\n",
    "\n",
    "            'politics': [\n",
    "                'government', 'administration', 'president', 'prime minister', 'congress',\n",
    "                'parliament', 'senate', 'house', 'cabinet', 'minister', 'secretary', 'governor',\n",
    "                'mayor', 'official', 'authority', 'agency', 'federal', 'state', 'local',\n",
    "\n",
    "                'election', 'campaign', 'vote', 'voter', 'ballot', 'poll', 'polling',\n",
    "                'candidate', 'incumbent', 'challenger', 'primary', 'caucus', 'debate',\n",
    "\n",
    "                'democrat', 'republican', 'liberal', 'conservative', 'progressive',\n",
    "                'left-wing', 'right-wing', 'centrist', 'moderate', 'radical', 'party',\n",
    "                'partisan', 'bipartisan', 'coalition',\n",
    "\n",
    "                'policy', 'legislation', 'law', 'bill', 'act', 'amendment', 'regulation',\n",
    "                'deregulation', 'reform', 'initiative', 'proposal', 'budget', 'tax',\n",
    "                'spending', 'deficit',\n",
    "\n",
    "                'foreign policy', 'international', 'diplomatic', 'diplomacy', 'treaty',\n",
    "                'agreement', 'alliance', 'summit', 'sanction', 'conflict', 'crisis',\n",
    "                'security', 'defense', 'military', 'war', 'peace', 'negotiation'\n",
    "            ],\n",
    "\n",
    "            'environment': [\n",
    "                'climate', 'climate change', 'global warming', 'greenhouse gas', 'carbon',\n",
    "                'emission', 'temperature', 'weather', 'extreme weather', 'drought', 'flood',\n",
    "                'hurricane', 'storm', 'wildfire', 'heatwave',\n",
    "\n",
    "                'environment', 'environmental', 'ecosystem', 'biodiversity', 'habitat',\n",
    "                'species', 'endangered', 'extinction', 'conservation', 'preservation',\n",
    "                'wildlife', 'forest', 'deforestation', 'reforestation', 'wetland',\n",
    "\n",
    "                'pollution', 'pollutant', 'contamination', 'waste', 'plastic', 'recycling',\n",
    "                'landfill', 'toxic', 'hazardous', 'oil spill', 'air quality', 'water quality',\n",
    "\n",
    "                'energy', 'renewable', 'solar', 'wind', 'hydroelectric', 'geothermal',\n",
    "                'fossil fuel', 'coal', 'oil', 'natural gas', 'nuclear', 'power plant',\n",
    "\n",
    "                'sustainable', 'sustainability', 'green', 'eco-friendly', 'carbon footprint',\n",
    "                'carbon neutral', 'net zero', 'clean energy', 'organic', 'natural resource'\n",
    "            ],\n",
    "\n",
    "            'entertainment': [\n",
    "                'movie', 'film', 'cinema', 'television', 'tv', 'show', 'series', 'episode',\n",
    "                'documentary', 'drama', 'comedy', 'thriller', 'horror', 'action', 'sci-fi',\n",
    "                'director', 'producer', 'actor', 'actress', 'cast', 'character', 'screenplay',\n",
    "                'box office', 'streaming', 'netflix', 'disney', 'hbo', 'amazon prime',\n",
    "\n",
    "                'music', 'song', 'album', 'single', 'artist', 'band', 'musician', 'singer',\n",
    "                'rapper', 'concert', 'tour', 'performance', 'grammy', 'billboard', 'chart',\n",
    "                'spotify', 'itunes', 'vinyl', 'genre', 'pop', 'rock', 'hip-hop', 'rap', 'jazz',\n",
    "\n",
    "                'celebrity', 'star', 'famous', 'award', 'red carpet', 'premiere', 'interview',\n",
    "                'paparazzi', 'gossip', 'scandal', 'viral', 'trending', 'social media',\n",
    "                'instagram', 'twitter', 'tiktok', 'youtube', 'influencer',\n",
    "\n",
    "                'game', 'gaming', 'video game', 'esports', 'book', 'novel', 'author',\n",
    "                'bestseller', 'theater', 'broadway', 'art', 'exhibition', 'festival',\n",
    "                'fashion', 'design', 'model', 'runway', 'collection'\n",
    "            ],\n",
    "\n",
    "            'sports': [\n",
    "                'sports', 'sport', 'football', 'soccer', 'basketball', 'baseball', 'hockey',\n",
    "                'tennis', 'golf', 'cricket', 'rugby', 'boxing', 'mma', 'wrestling', 'racing',\n",
    "                'formula 1', 'nascar', 'swimming', 'track', 'field', 'gymnastics',\n",
    "\n",
    "                'game', 'match', 'tournament', 'championship', 'league', 'season', 'playoff',\n",
    "                'final', 'semifinal', 'quarterfinal', 'competition', 'event', 'olympic',\n",
    "                'world cup', 'grand slam', 'title', 'trophy', 'medal', 'record',\n",
    "\n",
    "                'team', 'player', 'athlete', 'coach', 'manager', 'referee', 'umpire',\n",
    "                'captain', 'rookie', 'veteran', 'draft', 'trade', 'free agent', 'contract',\n",
    "                'roster', 'lineup', 'bench', 'starter',\n",
    "\n",
    "                'nfl', 'nba', 'mlb', 'nhl', 'fifa', 'uefa', 'ioc', 'ncaa', 'espn',\n",
    "\n",
    "                'score', 'win', 'loss', 'victory', 'defeat', 'tie', 'draw', 'overtime',\n",
    "                'penalty', 'foul', 'injury', 'performance', 'stat', 'statistic', 'ranking',\n",
    "                'standing', 'point', 'goal', 'touchdown', 'homerun', 'basket'\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        text_lower = text.lower()\n",
    "        domain_scores = {domain: 0.0 for domain in domain_keywords}\n",
    "\n",
    "        for domain, keywords in domain_keywords.items():\n",
    "            domain_score = 0.0\n",
    "\n",
    "            high_importance = keywords[:int(len(keywords) * 0.3)]\n",
    "            medium_importance = keywords[int(len(keywords) * 0.3):int(len(keywords) * 0.7)]\n",
    "            low_importance = keywords[int(len(keywords) * 0.7):]\n",
    "\n",
    "            for keyword in high_importance:\n",
    "                if keyword in text_lower:\n",
    "                    domain_score += 1.5\n",
    "                elif len(keyword.split()) > 1 and all(word in text_lower for word in keyword.split()):\n",
    "                    domain_score += 0.75\n",
    "\n",
    "            for keyword in medium_importance:\n",
    "                count = text_lower.count(' ' + keyword + ' ') + text_lower.count(keyword + ' ') + text_lower.count(' ' + keyword)\n",
    "                if count > 0:\n",
    "                    domain_score += count * 2.0\n",
    "\n",
    "            for keyword in low_importance:\n",
    "                count = text_lower.count(' ' + keyword + ' ') + text_lower.count(keyword + ' ') + text_lower.count(' ' + keyword)\n",
    "                if count > 0:\n",
    "                    domain_score += count * 1.0\n",
    "\n",
    "            domain_scores[domain] = domain_score\n",
    "        print(f\"DEBUG: Raw domain score for {domain}: {domain_score:.2f}\")\n",
    "\n",
    "        total_score = sum(domain_scores.values())\n",
    "        if total_score > 0:\n",
    "            domain_scores = {domain: score / total_score for domain, score in domain_scores.items()}\n",
    "        else:\n",
    "            domain_scores = {domain: 0.0 for domain in domain_scores}\n",
    "\n",
    "        doc_characteristics['domain_scores'] = domain_scores\n",
    "        structure_features = {\n",
    "            'formal': 0,\n",
    "            'informal': 0,\n",
    "            'narrative': 0,\n",
    "            'descriptive': 0,\n",
    "            'technical': 0\n",
    "        }\n",
    "\n",
    "        for token in doc:\n",
    "            if len(token.text) > 8 and token.is_alpha:\n",
    "                structure_features['technical'] += 1\n",
    "\n",
    "            if token.pos_ == 'ADJ':\n",
    "                structure_features['descriptive'] += 1\n",
    "\n",
    "            if token.pos_ == 'VERB' and token.tag_ in ['VBD', 'VBN']:\n",
    "                structure_features['narrative'] += 1\n",
    "\n",
    "            if token.text.lower() in ['i', 'we', 'you', 'they', 'he', 'she']:\n",
    "                structure_features['informal'] += 1\n",
    "                structure_features['narrative'] += 0.5\n",
    "\n",
    "            if len(token.text) > 6 and token.is_alpha and token.pos_ not in ['PRON']:\n",
    "                structure_features['formal'] += 1\n",
    "\n",
    "        total_features = sum(structure_features.values())\n",
    "        if total_features > 0:\n",
    "            for structure, count in structure_features.items():\n",
    "                doc_characteristics['structure_scores'][structure] = count / total_features\n",
    "\n",
    "        top_domains = sorted(doc_characteristics['domain_scores'].items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "        print(f\"DEBUG: Top domains detected: {', '.join([f'{d}: {s:.4f}' for d, s in top_domains])}\")\n",
    "        print(f\"DEBUG: All domain scores: {doc_characteristics['domain_scores']}\")\n",
    "        return doc_characteristics\n",
    "\n",
    "    def estimate_method_confidence(\n",
    "    self,\n",
    "    method_name: str,\n",
    "    keyphrases: List[Tuple[str, float]],\n",
    "    doc_characteristics: Dict[str, Any]\n",
    "    ) -> float:\n",
    "        \n",
    "        if not keyphrases:\n",
    "            return 0.0\n",
    "\n",
    "        base_confidence = self.method_weights.get(method_name, 0.25)\n",
    "\n",
    "        scores = [score for _, score in keyphrases]\n",
    "\n",
    "        avg_score = sum(scores) / len(scores) if scores else 0\n",
    "        score_variance = sum((s - avg_score) ** 2 for s in scores) / len(scores) if len(scores) > 1 else 0\n",
    "        score_range = max(scores) - min(scores) if scores else 0\n",
    "\n",
    "        score_confidence = 1.0 - min(1.0, (score_variance * 5 + score_range * 0.5))\n",
    "\n",
    "        coherence = 0.0\n",
    "        if len(keyphrases) > 1:\n",
    "            top_keyphrases = [kp for kp, _ in keyphrases[:5]]\n",
    "\n",
    "            try:\n",
    "                embeddings = self.get_embeddings(top_keyphrases, convert_to_tensor=False)\n",
    "\n",
    "                similarities = []\n",
    "                for i in range(len(embeddings)):\n",
    "                    for j in range(i + 1, len(embeddings)):\n",
    "                        sim = cosine_similarity(embeddings[i].reshape(1, -1), embeddings[j].reshape(1, -1))[0][0]\n",
    "                        similarities.append(sim)\n",
    "\n",
    "                coherence = sum(similarities) / len(similarities) if similarities else 0.0\n",
    "            except Exception as e:\n",
    "                print(f\"Error calculating coherence: {e}\")\n",
    "                coherence = 0.5\n",
    "\n",
    "        method_domain_fit = 0.5\n",
    "\n",
    "        domain_scores = doc_characteristics['domain_scores']\n",
    "        structure_scores = doc_characteristics['structure_scores']\n",
    "\n",
    "        if method_name == 'keybert':\n",
    "            method_domain_fit = (\n",
    "                domain_scores.get('technology', 0) * 0.8 +\n",
    "                domain_scores.get('science', 0) * 0.8 +\n",
    "                domain_scores.get('academic', 0) * 0.7 +\n",
    "                domain_scores.get('business', 0) * 0.6 +\n",
    "                domain_scores.get('health', 0) * 0.6 +\n",
    "                structure_scores.get('technical', 0) * 0.8 +\n",
    "                structure_scores.get('formal', 0) * 0.7\n",
    "            ) / 5.0\n",
    "\n",
    "        elif method_name == 'multipartiterank':\n",
    "            method_domain_fit = (\n",
    "                domain_scores.get('academic', 0) * 0.8 +\n",
    "                domain_scores.get('science', 0) * 0.7 +\n",
    "                domain_scores.get('health', 0) * 0.6 +\n",
    "                domain_scores.get('politics', 0) * 0.6 +\n",
    "                domain_scores.get('business', 0) * 0.5 +\n",
    "                structure_scores.get('formal', 0) * 0.8 +\n",
    "                structure_scores.get('technical', 0) * 0.7 +\n",
    "                (1.0 - structure_scores.get('narrative', 0)) * 0.5\n",
    "            ) / 5.0\n",
    "\n",
    "        elif method_name == 'yake':\n",
    "            method_domain_fit = (\n",
    "                domain_scores.get('news', 0) * 0.9 +\n",
    "                domain_scores.get('entertainment', 0) * 0.8 +\n",
    "                domain_scores.get('sports', 0) * 0.8 +\n",
    "                domain_scores.get('politics', 0) * 0.7 +\n",
    "                domain_scores.get('business', 0) * 0.6 +\n",
    "                structure_scores.get('informal', 0) * 0.7 +\n",
    "                (1.0 - structure_scores.get('technical', 0)) * 0.6\n",
    "            ) / 5.0\n",
    "\n",
    "        elif method_name == 'textrank':\n",
    "            method_domain_fit = (\n",
    "                domain_scores.get('news', 0) * 0.8 +\n",
    "                domain_scores.get('entertainment', 0) * 0.7 +\n",
    "                domain_scores.get('environment', 0) * 0.7 +\n",
    "                domain_scores.get('sports', 0) * 0.6 +\n",
    "                domain_scores.get('health', 0) * 0.6 +\n",
    "                structure_scores.get('narrative', 0) * 0.8 +\n",
    "                structure_scores.get('descriptive', 0) * 0.7\n",
    "            ) / 5.0\n",
    "\n",
    "        confidence = (\n",
    "            base_confidence * 0.4 +\n",
    "            score_confidence * 0.3 +\n",
    "            coherence * 0.1 +\n",
    "            method_domain_fit * 0.2\n",
    "        )\n",
    "\n",
    "        confidence = max(0.1, min(1.0, confidence))\n",
    "\n",
    "        return confidence\n",
    "\n",
    "    def normalize_keyphrase(self, keyphrase: str) -> str:\n",
    "        \n",
    "        doc = self.nlp(keyphrase)\n",
    "\n",
    "        tokens = [token.text for token in doc if token.pos_ != 'DET' and token.pos_ != 'ADP']\n",
    "\n",
    "        normalized = ' '.join(tokens).strip()\n",
    "\n",
    "        if self.clean_boundaries:\n",
    "            normalized = self.clean_phrase_boundaries(normalized)\n",
    "\n",
    "        if self.use_lemmatization:\n",
    "            normalized = self.lemmatize_text(normalized)\n",
    "\n",
    "        return normalized\n",
    "\n",
    "    def get_embeddings(self, phrases: List[str], convert_to_tensor: bool = True) -> np.ndarray:\n",
    "        try:\n",
    "            embeddings = self.sentence_model.encode(\n",
    "                phrases,\n",
    "                convert_to_tensor=convert_to_tensor,\n",
    "                show_progress_bar=False\n",
    "            )\n",
    "\n",
    "            if convert_to_tensor and isinstance(embeddings, torch.Tensor):\n",
    "                return embeddings.cpu().numpy()\n",
    "\n",
    "            return embeddings\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting embeddings: {str(e)}\")\n",
    "            return np.array([])\n",
    "\n",
    "    def is_subphrase(self, phrase1: str, phrase2: str) -> bool:\n",
    "        \n",
    "        phrase1_lower = phrase1.lower()\n",
    "        phrase2_lower = phrase2.lower()\n",
    "\n",
    "        return phrase1_lower in phrase2_lower or phrase2_lower in phrase1_lower\n",
    "\n",
    "    def remove_redundant_keyphrases(self, keyphrases: List[Tuple[str, float]]) -> List[Tuple[str, float]]:\n",
    "        \n",
    "        if not keyphrases:\n",
    "            return []\n",
    "\n",
    "        sorted_keyphrases = sorted(keyphrases, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        phrases = [kp[0] for kp in sorted_keyphrases]\n",
    "        scores = [kp[1] for kp in sorted_keyphrases]\n",
    "\n",
    "        embeddings = self.get_embeddings(phrases, convert_to_tensor=False)\n",
    "\n",
    "        if len(embeddings) == 0:\n",
    "            return sorted_keyphrases\n",
    "\n",
    "        similarity_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "        to_remove = set()\n",
    "\n",
    "        merged_phrases = {}\n",
    "\n",
    "        for i in range(len(phrases)):\n",
    "            if i in to_remove:\n",
    "                continue\n",
    "\n",
    "            phrase_i = phrases[i].lower()\n",
    "            words_i = set(phrase_i.split())\n",
    "\n",
    "            lemma_i = self.lemmatize_text(phrase_i) if self.use_lemmatization else phrase_i\n",
    "\n",
    "            for j in range(len(phrases)):\n",
    "                if i == j or j in to_remove:\n",
    "                    continue\n",
    "\n",
    "                phrase_j = phrases[j].lower()\n",
    "                words_j = set(phrase_j.split())\n",
    "\n",
    "                lemma_j = self.lemmatize_text(phrase_j) if self.use_lemmatization else phrase_j\n",
    "\n",
    "                is_subphrase = (phrase_i in phrase_j or phrase_j in phrase_i or\n",
    "                            lemma_i in lemma_j or lemma_j in lemma_i)\n",
    "\n",
    "                if not is_subphrase:\n",
    "                    hyphen_i = phrase_i.replace(' ', '-')\n",
    "                    hyphen_j = phrase_j.replace(' ', '-')\n",
    "                    dehyphen_i = phrase_i.replace('-', ' ')\n",
    "                    dehyphen_j = phrase_j.replace('-', ' ')\n",
    "\n",
    "                    is_subphrase = (hyphen_i == phrase_j or hyphen_j == phrase_i or\n",
    "                                dehyphen_i == phrase_j or dehyphen_j == phrase_i)\n",
    "\n",
    "                if is_subphrase:\n",
    "                    if len(words_i) > len(words_j):\n",
    "                        if scores[i] >= 0.8 * scores[j]:\n",
    "                            to_remove.add(j)\n",
    "                            merged_phrases[j] = i\n",
    "                    elif len(words_j) > len(words_i):\n",
    "                        if scores[j] >= 0.8 * scores[i]:\n",
    "                            to_remove.add(i)\n",
    "                            merged_phrases[i] = j\n",
    "                            break\n",
    "                    else:\n",
    "                        if scores[i] >= scores[j]:\n",
    "                            to_remove.add(j)\n",
    "                            merged_phrases[j] = i\n",
    "                        else:\n",
    "                            to_remove.add(i)\n",
    "                            merged_phrases[i] = j\n",
    "                            break\n",
    "\n",
    "        for i in range(len(phrases)):\n",
    "            if i in to_remove:\n",
    "                continue\n",
    "\n",
    "            phrase_i = phrases[i]\n",
    "            len_i = len(phrase_i.split())\n",
    "\n",
    "            words_i = set(phrase_i.lower().split())\n",
    "\n",
    "            for j in range(i + 1, len(phrases)):\n",
    "                if j in to_remove:\n",
    "                    continue\n",
    "\n",
    "                phrase_j = phrases[j]\n",
    "                len_j = len(phrase_j.split())\n",
    "\n",
    "                words_j = set(phrase_j.lower().split())\n",
    "\n",
    "                if len(words_i) > 0 and len(words_j) > 0:\n",
    "                    overlap = len(words_i.intersection(words_j))\n",
    "                    overlap_ratio_i = overlap / len(words_i)\n",
    "                    overlap_ratio_j = overlap / len(words_j)\n",
    "                    max_overlap_ratio = max(overlap_ratio_i, overlap_ratio_j)\n",
    "                else:\n",
    "                    max_overlap_ratio = 0.0\n",
    "\n",
    "                base_threshold = 0.82\n",
    "\n",
    "                max_len = max(len_i, len_j)\n",
    "                length_factor = max_len / 3\n",
    "                length_adjustment = length_factor * 0.03\n",
    "\n",
    "                overlap_adjustment = max_overlap_ratio * 0.05\n",
    "\n",
    "                score_ratio = scores[i] / scores[j] if scores[j] > 0 else float('inf')\n",
    "                score_adjustment = 0.0\n",
    "                if score_ratio > 2.0 or score_ratio < 0.5:\n",
    "                    score_adjustment = 0.03\n",
    "\n",
    "                threshold = max(0.68, base_threshold - length_adjustment + overlap_adjustment + score_adjustment)\n",
    "\n",
    "                is_similar = similarity_matrix[i, j] > threshold\n",
    "                has_high_overlap = max_overlap_ratio > 0.8\n",
    "\n",
    "                if is_similar or has_high_overlap:\n",
    "\n",
    "                    score_factor = 1.0 if score_ratio > 1.2 else (-1.0 if score_ratio < 0.8 else 0.0)\n",
    "\n",
    "                    length_factor = 0.5 if len_i > len_j else (-0.5 if len_j > len_i else 0.0)\n",
    "\n",
    "                    specificity_factor = 0.3 if len_i > len_j else (-0.3 if len_j > len_i else 0.0)\n",
    "\n",
    "                    decision_score = (score_factor * 0.6) + (length_factor * 0.3) + (specificity_factor * 0.1)\n",
    "\n",
    "                    if decision_score > 0 or (decision_score == 0 and scores[i] >= scores[j]):\n",
    "                        to_remove.add(j)\n",
    "                        merged_phrases[j] = i\n",
    "                    else:\n",
    "                        to_remove.add(i)\n",
    "                        merged_phrases[i] = j\n",
    "                        break\n",
    "\n",
    "        absorbed_counts = {}\n",
    "        for removed, kept in merged_phrases.items():\n",
    "            absorbed_counts[kept] = absorbed_counts.get(kept, 0) + 1\n",
    "\n",
    "        boosted_scores = scores.copy()\n",
    "        for i in range(len(phrases)):\n",
    "            if i in absorbed_counts:\n",
    "                boost_factor = 1.0 + (0.1 * math.log(1 + absorbed_counts[i]))\n",
    "                boosted_scores[i] *= boost_factor\n",
    "\n",
    "        filtered_keyphrases = [\n",
    "            (phrases[i], boosted_scores[i]) for i in range(len(sorted_keyphrases)) if i not in to_remove\n",
    "        ]\n",
    "\n",
    "        filtered_keyphrases.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        return filtered_keyphrases\n",
    "    def combine_keyphrases_ensemble(\n",
    "    self,\n",
    "    method_keyphrases: Dict[str, List[Tuple[str, float]]],\n",
    "    text: str = None\n",
    "    ) -> List[Tuple[str, float]]:\n",
    "        \n",
    "        combined_dict = {}\n",
    "\n",
    "        if text is None:\n",
    "            return self._simple_ensemble(method_keyphrases)\n",
    "\n",
    "        doc_characteristics = self.classify_document_type(text)\n",
    "\n",
    "        adaptive_weights = {}\n",
    "        confidence_sum = 0.0\n",
    "\n",
    "        for method_name, keyphrases in method_keyphrases.items():\n",
    "            if not keyphrases:\n",
    "                adaptive_weights[method_name] = 0.0\n",
    "                continue\n",
    "\n",
    "            confidence = self.estimate_method_confidence(method_name, keyphrases, doc_characteristics)\n",
    "            adaptive_weights[method_name] = confidence\n",
    "            confidence_sum += confidence\n",
    "\n",
    "        if confidence_sum > 0:\n",
    "            for method_name in adaptive_weights:\n",
    "                adaptive_weights[method_name] /= confidence_sum\n",
    "        else:\n",
    "            adaptive_weights = {name: weight for name, weight in self.method_weights.items()}\n",
    "\n",
    "        print(f\"DEBUG: Adaptive method weights:\")\n",
    "        for method_name, weight in adaptive_weights.items():\n",
    "            print(f\"DEBUG:   - {method_name}: {weight:.4f}\")\n",
    "\n",
    "        for method_name, keyphrases in method_keyphrases.items():\n",
    "            if not keyphrases:\n",
    "                continue\n",
    "\n",
    "            method_weight = adaptive_weights.get(method_name, 0.0)\n",
    "\n",
    "            if method_weight <= 0.0:\n",
    "                continue\n",
    "\n",
    "            normalized_keyphrases = [(self.normalize_keyphrase(kp), score) for kp, score in keyphrases]\n",
    "\n",
    "            normalized_keyphrases = [(kp, score) for kp, score in normalized_keyphrases if kp]\n",
    "\n",
    "            for kp, score in normalized_keyphrases:\n",
    "                kp_lower = kp.lower()\n",
    "                weighted_score = score * method_weight\n",
    "\n",
    "                if self.use_position_weight:\n",
    "                    position_weight = self.calculate_position_weight(text, kp)\n",
    "                    weighted_score *= position_weight\n",
    "\n",
    "                if self.use_tfidf_weight and kp in self.word_tfidf_cache:\n",
    "                    tfidf_weight = self.word_tfidf_cache.get(kp, 1.0)\n",
    "                    weighted_score *= tfidf_weight\n",
    "\n",
    "                if kp_lower in combined_dict:\n",
    "                    existing_kp, existing_score, existing_votes, methods = combined_dict[kp_lower]\n",
    "\n",
    "                    new_score = existing_score + weighted_score\n",
    "                    new_votes = existing_votes + 1\n",
    "                    methods.add(method_name)\n",
    "\n",
    "                    combined_dict[kp_lower] = (existing_kp, new_score, new_votes, methods)\n",
    "                else:\n",
    "                    combined_dict[kp_lower] = (kp, weighted_score, 1, {method_name})\n",
    "\n",
    "        combined_dict = self._enhance_diversity(combined_dict, text, doc_characteristics)\n",
    "\n",
    "        normalized_dict = {}\n",
    "        for kp_lower, (kp, score, votes, methods) in combined_dict.items():\n",
    "            method_diversity_boost = min(2.0, 1.0 + (len(methods) - 1) * 0.6)\n",
    "\n",
    "            num_words = len(kp.split())\n",
    "            length_bonus = 1.0 + 0.15 * (num_words - 1)\n",
    "            length_bonus = min(1.6, length_bonus)\n",
    "\n",
    "            normalized_score = (score / votes) * method_diversity_boost * length_bonus\n",
    "\n",
    "            if num_words > 1:\n",
    "                print(f\"DEBUG: Length bonus for '{kp}' ({num_words} words): {length_bonus:.2f}x\")\n",
    "\n",
    "            primary_domain = max(doc_characteristics['domain_scores'].items(), key=lambda x: x[1])[0]\n",
    "            primary_domain_score = doc_characteristics['domain_scores'][primary_domain]\n",
    "\n",
    "            domain_boost = 1.0\n",
    "            if primary_domain_score > 0.3:\n",
    "                domain_important_terms = {\n",
    "                    'technology': [\n",
    "                        'artificial intelligence', 'machine learning', 'deep learning', 'algorithm',\n",
    "                        'data science', 'neural network', 'cloud computing', 'cybersecurity',\n",
    "                        'blockchain', 'software development', 'programming', 'automation',\n",
    "                        'digital transformation', 'big data', 'internet of things', 'computer vision',\n",
    "                        'robotics', 'virtual reality', 'augmented reality', 'api', 'mobile app'\n",
    "                    ],\n",
    "                    'business': [\n",
    "                        'market analysis', 'business strategy', 'investment', 'financial',\n",
    "                        'economic', 'revenue', 'profit margin', 'supply chain', 'customer acquisition',\n",
    "                        'marketing strategy', 'competitive advantage', 'business model', 'startup',\n",
    "                        'venture capital', 'merger', 'acquisition', 'stock market', 'shareholders',\n",
    "                        'corporate', 'management', 'leadership', 'ceo', 'executive'\n",
    "                    ],\n",
    "                    'health': [\n",
    "                        'medical research', 'clinical trial', 'patient care', 'treatment',\n",
    "                        'healthcare system', 'diagnosis', 'therapy', 'pharmaceutical',\n",
    "                        'disease prevention', 'public health', 'mental health', 'wellness',\n",
    "                        'chronic condition', 'medical technology', 'healthcare policy',\n",
    "                        'hospital', 'doctor', 'nurse', 'vaccine', 'medication', 'surgery'\n",
    "                    ],\n",
    "                    'science': [\n",
    "                        'scientific research', 'experiment', 'laboratory', 'hypothesis',\n",
    "                        'theory', 'discovery', 'innovation', 'quantum', 'molecular',\n",
    "                        'genetic', 'evolutionary', 'physics', 'chemistry', 'biology',\n",
    "                        'astronomy', 'neuroscience', 'particle', 'atom', 'cell', 'dna',\n",
    "                        'ecosystem', 'species', 'climate science'\n",
    "                    ],\n",
    "                    'news': [\n",
    "                        'breaking news', 'latest development', 'report', 'announcement',\n",
    "                        'press release', 'statement', 'interview', 'investigation',\n",
    "                        'coverage', 'media', 'journalist', 'correspondent', 'source',\n",
    "                        'official', 'spokesperson', 'exclusive', 'update', 'developing story'\n",
    "                    ],\n",
    "                    'academic': [\n",
    "                        'research paper', 'academic study', 'scholarly article', 'publication',\n",
    "                        'peer review', 'methodology', 'literature review', 'theoretical framework',\n",
    "                        'empirical evidence', 'data analysis', 'findings', 'conclusion',\n",
    "                        'contribution', 'citation', 'academic journal', 'university',\n",
    "                        'professor', 'student', 'thesis', 'dissertation', 'faculty'\n",
    "                    ],\n",
    "                    'politics': [\n",
    "                        'government policy', 'legislation', 'election', 'political party',\n",
    "                        'campaign', 'voter', 'democracy', 'administration', 'congress',\n",
    "                        'parliament', 'diplomatic', 'international relations', 'national security',\n",
    "                        'public policy', 'constitutional', 'president', 'prime minister',\n",
    "                        'senator', 'representative', 'bill', 'law', 'regulation'\n",
    "                    ],\n",
    "                    'environment': [\n",
    "                        'climate change', 'global warming', 'sustainability', 'renewable energy',\n",
    "                        'carbon emissions', 'conservation', 'biodiversity', 'ecosystem',\n",
    "                        'environmental protection', 'pollution', 'green technology',\n",
    "                        'natural resources', 'wildlife', 'environmental policy',\n",
    "                        'recycling', 'solar power', 'wind energy', 'fossil fuels'\n",
    "                    ],\n",
    "                    'entertainment': [\n",
    "                        'movie premiere', 'television series', 'music album', 'celebrity',\n",
    "                        'box office', 'streaming service', 'award show', 'performance',\n",
    "                        'director', 'actor', 'actress', 'artist', 'concert', 'festival',\n",
    "                        'entertainment industry', 'film', 'tv show', 'song', 'album',\n",
    "                        'hollywood', 'broadway', 'bestseller', 'video game'\n",
    "                    ],\n",
    "                    'sports': [\n",
    "                        'championship', 'tournament', 'athlete', 'team', 'competition',\n",
    "                        'olympic', 'world cup', 'league', 'season', 'player', 'coach',\n",
    "                        'record', 'performance', 'sports event', 'victory', 'defeat',\n",
    "                        'football', 'soccer', 'basketball', 'baseball', 'tennis',\n",
    "                        'golf', 'racing', 'medal', 'stadium', 'fans'\n",
    "                    ]\n",
    "                }\n",
    "\n",
    "                important_terms = domain_important_terms.get(primary_domain, [])\n",
    "\n",
    "                if any(term.lower() in kp_lower or kp_lower in term.lower() for term in important_terms):\n",
    "                    domain_boost = 1.5\n",
    "\n",
    "            normalized_score *= domain_boost\n",
    "            normalized_dict[kp_lower] = (kp, normalized_score)\n",
    "\n",
    "        combined_list = [(kp, score) for kp_lower, (kp, score) in normalized_dict.items()]\n",
    "        combined_list.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        return combined_list\n",
    "    def _simple_ensemble(self, method_keyphrases: Dict[str, List[Tuple[str, float]]]) -> List[Tuple[str, float]]:\n",
    "        \n",
    "        combined_dict = {}\n",
    "\n",
    "        for method_name, keyphrases in method_keyphrases.items():\n",
    "            if not keyphrases:\n",
    "                continue\n",
    "\n",
    "            method_weight = self.method_weights.get(method_name, 0.25)\n",
    "\n",
    "            normalized_keyphrases = [(self.normalize_keyphrase(kp), score) for kp, score in keyphrases]\n",
    "\n",
    "            normalized_keyphrases = [(kp, score) for kp, score in normalized_keyphrases if kp]\n",
    "\n",
    "            for kp, score in normalized_keyphrases:\n",
    "                kp_lower = kp.lower()\n",
    "                weighted_score = score * method_weight\n",
    "\n",
    "                if kp_lower in combined_dict:\n",
    "                    existing_kp, existing_score, existing_votes = combined_dict[kp_lower]\n",
    "\n",
    "                    new_score = existing_score + weighted_score\n",
    "                    new_votes = existing_votes + 1\n",
    "\n",
    "                    combined_dict[kp_lower] = (existing_kp, new_score, new_votes)\n",
    "                else:\n",
    "                    combined_dict[kp_lower] = (kp, weighted_score, 1)\n",
    "\n",
    "        normalized_dict = {}\n",
    "        for kp_lower, (kp, score, votes) in combined_dict.items():\n",
    "            vote_boost = min(2.0, 1.0 + (votes - 1) * 0.3)\n",
    "            normalized_score = (score / votes) * vote_boost\n",
    "            normalized_dict[kp_lower] = (kp, normalized_score)\n",
    "\n",
    "        combined_list = [(kp, score) for kp_lower, (kp, score) in normalized_dict.items()]\n",
    "        combined_list.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        return combined_list\n",
    "\n",
    "    def _enhance_diversity(\n",
    "    self,\n",
    "    combined_dict: Dict[str, Tuple[str, float, int, Set[str]]],\n",
    "    text: str,\n",
    "    doc_characteristics: Dict[str, Any]\n",
    "    ) -> Dict[str, Tuple[str, float, int, Set[str]]]:\n",
    "        \n",
    "        if len(combined_dict) <= 5:\n",
    "            return combined_dict\n",
    "\n",
    "        position_data = []\n",
    "        for kp_lower, (kp, score, votes, methods) in combined_dict.items():\n",
    "            position = text.lower().find(kp_lower)\n",
    "            if position != -1:\n",
    "                relative_position = position / len(text)\n",
    "                position_data.append({\n",
    "                    'keyphrase': kp,\n",
    "                    'keyphrase_lower': kp_lower,\n",
    "                    'score': score,\n",
    "                    'votes': votes,\n",
    "                    'methods': methods,\n",
    "                    'position': position,\n",
    "                    'relative_position': relative_position\n",
    "                })\n",
    "\n",
    "        quartiles = {\n",
    "            'first_quarter': [],\n",
    "            'second_quarter': [],\n",
    "            'third_quarter': [],\n",
    "            'fourth_quarter': []\n",
    "        }\n",
    "\n",
    "        for item in position_data:\n",
    "            rel_pos = item['relative_position']\n",
    "            if rel_pos < 0.25:\n",
    "                quartiles['first_quarter'].append(item)\n",
    "            elif rel_pos < 0.5:\n",
    "                quartiles['second_quarter'].append(item)\n",
    "            elif rel_pos < 0.75:\n",
    "                quartiles['third_quarter'].append(item)\n",
    "            else:\n",
    "                quartiles['fourth_quarter'].append(item)\n",
    "\n",
    "        enhanced_dict = {}\n",
    "\n",
    "        total_keyphrases = len(combined_dict)\n",
    "        target_counts = {\n",
    "            'first_quarter': int(total_keyphrases * 0.5),\n",
    "            'second_quarter': int(total_keyphrases * 0.3),\n",
    "            'third_quarter': int(total_keyphrases * 0.15),\n",
    "            'fourth_quarter': int(total_keyphrases * 0.05)\n",
    "        }\n",
    "\n",
    "        for quartile, items in quartiles.items():\n",
    "            if items:\n",
    "                target_counts[quartile] = max(1, target_counts[quartile])\n",
    "\n",
    "        for quartile in quartiles:\n",
    "            quartiles[quartile].sort(key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "        for quartile, items in quartiles.items():\n",
    "            count = min(len(items), target_counts[quartile])\n",
    "            for i in range(count):\n",
    "                item = items[i]\n",
    "                kp_lower = item['keyphrase_lower']\n",
    "                kp = item['keyphrase']\n",
    "                score = item['score']\n",
    "                votes = item['votes']\n",
    "                methods = item['methods']\n",
    "\n",
    "                diversity_boost = 1.0\n",
    "                if quartile == 'second_quarter':\n",
    "                    diversity_boost = 1.05\n",
    "                elif quartile == 'third_quarter':\n",
    "                    diversity_boost = 1.1\n",
    "                elif quartile == 'fourth_quarter':\n",
    "                    diversity_boost = 1.15\n",
    "\n",
    "                enhanced_dict[kp_lower] = (kp, score * diversity_boost, votes, methods)\n",
    "\n",
    "        for kp_lower, (kp, score, votes, methods) in combined_dict.items():\n",
    "            if kp_lower not in enhanced_dict:\n",
    "                enhanced_dict[kp_lower] = (kp, score, votes, methods)\n",
    "\n",
    "        return enhanced_dict\n",
    "\n",
    "    def combine_keyphrases(\n",
    "        self,\n",
    "        keybert_keyphrases: List[Tuple[str, float]],\n",
    "        multipartite_keyphrases: List[Tuple[str, float]],\n",
    "        yake_keyphrases: List[Tuple[str, float]] = None,\n",
    "        textrank_keyphrases: List[Tuple[str, float]] = None\n",
    "    ) -> List[Tuple[str, float]]:\n",
    "        \n",
    "        if self.use_ensemble and (yake_keyphrases is not None or textrank_keyphrases is not None):\n",
    "            method_keyphrases = {\n",
    "                'keybert': keybert_keyphrases,\n",
    "                'multipartiterank': multipartite_keyphrases\n",
    "            }\n",
    "\n",
    "            if yake_keyphrases is not None:\n",
    "                method_keyphrases['yake'] = yake_keyphrases\n",
    "\n",
    "            if textrank_keyphrases is not None:\n",
    "                method_keyphrases['textrank'] = textrank_keyphrases\n",
    "\n",
    "            return self.combine_keyphrases_ensemble(method_keyphrases)\n",
    "\n",
    "        normalized_keybert = [(self.normalize_keyphrase(kp), score) for kp, score in keybert_keyphrases]\n",
    "        normalized_multipartite = [(self.normalize_keyphrase(kp), score) for kp, score in multipartite_keyphrases]\n",
    "\n",
    "        normalized_keybert = [(kp, score) for kp, score in normalized_keybert if kp]\n",
    "        normalized_multipartite = [(kp, score) for kp, score in normalized_multipartite if kp]\n",
    "\n",
    "        combined_dict = {}\n",
    "\n",
    "        for kp, score in normalized_keybert:\n",
    "            combined_dict[kp.lower()] = (kp, score)\n",
    "\n",
    "        for kp, score in normalized_multipartite:\n",
    "            kp_lower = kp.lower()\n",
    "            if kp_lower in combined_dict:\n",
    "                existing_kp, existing_score = combined_dict[kp_lower]\n",
    "                new_score = max(existing_score, score) * 1.1\n",
    "                combined_dict[kp_lower] = (existing_kp, min(1.0, new_score))\n",
    "            else:\n",
    "                combined_dict[kp_lower] = (kp, score)\n",
    "\n",
    "        keys = list(combined_dict.keys())\n",
    "        for i in range(len(keys)):\n",
    "            for j in range(i + 1, len(keys)):\n",
    "                key_i = keys[i]\n",
    "                key_j = keys[j]\n",
    "\n",
    "                if key_i not in combined_dict or key_j not in combined_dict:\n",
    "                    continue\n",
    "\n",
    "                if self.is_subphrase(key_i, key_j):\n",
    "                    score_i = combined_dict[key_i][1]\n",
    "                    score_j = combined_dict[key_j][1]\n",
    "\n",
    "                    if score_i >= score_j:\n",
    "                        del combined_dict[key_j]\n",
    "                    else:\n",
    "                        del combined_dict[key_i]\n",
    "                        break\n",
    "\n",
    "        combined_keyphrases = list(combined_dict.values())\n",
    "        combined_keyphrases.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        return combined_keyphrases\n",
    "\n",
    "    def extract_keyphrases(self, text: str) -> List[str]:\n",
    "        \n",
    "        keyphrases_with_scores = self.extract_keyphrases_with_scores(text)\n",
    "\n",
    "        return [kp for kp, _ in keyphrases_with_scores]\n",
    "\n",
    "    def extract_keyphrases_with_scores(self, text: str) -> List[Tuple[str, float]]:\n",
    "        \n",
    "        if not text or len(text.strip()) < 10:\n",
    "            return []\n",
    "\n",
    "        text_with_expansions = self.expand_technical_abbreviations(text)\n",
    "\n",
    "        preprocessed_text = self.preprocess_text(text_with_expansions)\n",
    "\n",
    "        print(f\"DEBUG: Extracting keyphrases with use_position_weight={self.use_position_weight}, use_tfidf_weight={self.use_tfidf_weight}, use_ensemble={self.use_ensemble}\")\n",
    "\n",
    "        keybert_keyphrases = self.extract_with_keybert(preprocessed_text, nr_candidates=self.top_n)\n",
    "        print(f\"DEBUG: KeyBERT extracted {len(keybert_keyphrases)} keyphrases\")\n",
    "\n",
    "        multipartite_keyphrases = self.extract_with_multipartiterank(preprocessed_text, nr_candidates=self.top_n)\n",
    "        print(f\"DEBUG: MultipartiteRank extracted {len(multipartite_keyphrases)} keyphrases\")\n",
    "\n",
    "        yake_keyphrases = None\n",
    "        textrank_keyphrases = None\n",
    "\n",
    "        if self.use_ensemble:\n",
    "            yake_keyphrases = self.extract_with_yake(preprocessed_text, nr_candidates=self.top_n)\n",
    "            print(f\"DEBUG: YAKE extracted {len(yake_keyphrases) if yake_keyphrases else 0} keyphrases\")\n",
    "\n",
    "            textrank_keyphrases = self.extract_with_textrank(preprocessed_text, nr_candidates=self.top_n)\n",
    "            print(f\"DEBUG: TextRank extracted {len(textrank_keyphrases) if textrank_keyphrases else 0} keyphrases\")\n",
    "\n",
    "            method_keyphrases = {\n",
    "                'keybert': keybert_keyphrases,\n",
    "                'multipartiterank': multipartite_keyphrases,\n",
    "                'yake': yake_keyphrases,\n",
    "                'textrank': textrank_keyphrases\n",
    "            }\n",
    "            combined_keyphrases = self.combine_keyphrases_ensemble(method_keyphrases, text=preprocessed_text)\n",
    "            print(f\"DEBUG: Ensemble combined into {len(combined_keyphrases)} keyphrases\")\n",
    "        else:\n",
    "            combined_keyphrases = self.combine_keyphrases(\n",
    "                keybert_keyphrases,\n",
    "                multipartite_keyphrases,\n",
    "                None,\n",
    "                None\n",
    "            )\n",
    "            print(f\"DEBUG: Simple combination produced {len(combined_keyphrases)} keyphrases\")\n",
    "\n",
    "        filtered_keyphrases = self.remove_redundant_keyphrases(combined_keyphrases)\n",
    "        print(f\"DEBUG: After redundancy removal: {len(filtered_keyphrases)} keyphrases\")\n",
    "\n",
    "        final_keyphrases = []\n",
    "        multi_word_phrases = [kp.lower() for kp, _ in filtered_keyphrases if ' ' in kp]\n",
    "\n",
    "        print(f\"DEBUG: Before post-filtering: {len(filtered_keyphrases)} keyphrases\")\n",
    "        print(f\"DEBUG: Multi-word phrases: {len(multi_word_phrases)}\")\n",
    "\n",
    "        for kp, score in filtered_keyphrases:\n",
    "            if ' ' in kp:\n",
    "                final_keyphrases.append((kp, score))\n",
    "                continue\n",
    "\n",
    "            kp_lower = kp.lower()\n",
    "            is_subpart = any(kp_lower in phrase for phrase in multi_word_phrases)\n",
    "\n",
    "            if not is_subpart:\n",
    "                final_keyphrases.append((kp, score))\n",
    "            else:\n",
    "                print(f\"DEBUG: Filtered out single word '{kp}' as it's part of multi-word phrase(s)\")\n",
    "\n",
    "        final_keyphrases.sort(key=lambda x: x[1], reverse=True)\n",
    "        print(f\"DEBUG: After post-filtering: {len(final_keyphrases)} keyphrases\")\n",
    "\n",
    "        top_keyphrases = final_keyphrases[:self.top_n]\n",
    "        print(f\"DEBUG: Final top {len(top_keyphrases)} keyphrases: {[kp for kp, _ in top_keyphrases]}\")\n",
    "\n",
    "        return top_keyphrases\n",
    "\n",
    "    def expand_technical_abbreviations(self, text: str) -> str:\n",
    "        \n",
    "        tech_abbr = {\n",
    "            'AI': 'artificial intelligence',\n",
    "            'ML': 'machine learning',\n",
    "            'NLP': 'natural language processing',\n",
    "            'IoT': 'internet of things',\n",
    "            'API': 'application programming interface',\n",
    "            'UI': 'user interface',\n",
    "            'UX': 'user experience',\n",
    "            'OS': 'operating system',\n",
    "            'CPU': 'central processing unit',\n",
    "            'GPU': 'graphics processing unit',\n",
    "            'RAM': 'random access memory',\n",
    "\n",
    "            'HTML': 'hypertext markup language',\n",
    "            'CSS': 'cascading style sheets',\n",
    "            'JS': 'javascript',\n",
    "            'SQL': 'structured query language',\n",
    "            'HTTP': 'hypertext transfer protocol',\n",
    "            'URL': 'uniform resource locator',\n",
    "            'JSON': 'javascript object notation',\n",
    "            'REST': 'representational state transfer',\n",
    "            'SDK': 'software development kit',\n",
    "            'IDE': 'integrated development environment',\n",
    "\n",
    "            'ROI': 'return on investment',\n",
    "            'CRM': 'customer relationship management',\n",
    "            'ERP': 'enterprise resource planning',\n",
    "            'SaaS': 'software as a service',\n",
    "            'B2B': 'business to business',\n",
    "            'B2C': 'business to consumer',\n",
    "            'KPI': 'key performance indicator',\n",
    "\n",
    "            'LLM': 'large language model',\n",
    "            'CNN': 'convolutional neural network',\n",
    "            'RNN': 'recurrent neural network',\n",
    "            'LSTM': 'long short-term memory',\n",
    "            'NER': 'named entity recognition',\n",
    "            'TF-IDF': 'term frequency-inverse document frequency',\n",
    "            'BERT': 'bidirectional encoder representations from transformers',\n",
    "            'GPT': 'generative pre-trained transformer',\n",
    "\n",
    "            'AWS': 'amazon web services',\n",
    "            'GCP': 'google cloud platform',\n",
    "            'CI/CD': 'continuous integration and continuous deployment',\n",
    "            'DevOps': 'development operations',\n",
    "            'VM': 'virtual machine',\n",
    "            'VPC': 'virtual private cloud',\n",
    "\n",
    "            'SSL': 'secure sockets layer',\n",
    "            'VPN': 'virtual private network',\n",
    "            'MFA': 'multi-factor authentication',\n",
    "            'SSO': 'single sign-on',\n",
    "\n",
    "            'AR': 'augmented reality',\n",
    "            'VR': 'virtual reality',\n",
    "            'IoMT': 'internet of medical things',\n",
    "            'UAV': 'unmanned aerial vehicle',\n",
    "            'EV': 'electric vehicle',\n",
    "\n",
    "            'FAQ': 'frequently asked questions',\n",
    "            'CEO': 'chief executive officer',\n",
    "            'CTO': 'chief technology officer',\n",
    "            'CFO': 'chief financial officer',\n",
    "            'COO': 'chief operating officer',\n",
    "            'CIO': 'chief information officer',\n",
    "            'HR': 'human resources',\n",
    "            'PR': 'public relations',\n",
    "            'R&D': 'research and development'\n",
    "        }\n",
    "\n",
    "        pattern = r'\\b(' + '|'.join(re.escape(abbr) for abbr in tech_abbr.keys()) + r')\\b'\n",
    "\n",
    "        def expand_match(match):\n",
    "            abbr = match.group(0)\n",
    "            return f\"{abbr} ({tech_abbr[abbr]})\"\n",
    "\n",
    "        expanded_text = re.sub(pattern, expand_match, text)\n",
    "\n",
    "        return expanded_text\n",
    "\n",
    "    def cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:\n",
    "        \n",
    "        dot_product = sum(a * b for a, b in zip(vec1, vec2))\n",
    "        magnitude1 = math.sqrt(sum(a * a for a in vec1))\n",
    "        magnitude2 = math.sqrt(sum(b * b for b in vec2))\n",
    "\n",
    "        if magnitude1 * magnitude2 == 0:\n",
    "            return 0.0\n",
    "\n",
    "        return dot_product / (magnitude1 * magnitude2)\n",
    "\n",
    "    def batch_extract_keyphrases(self, texts: List[str], batch_size: int = 8) -> List[List[str]]:\n",
    "        \n",
    "        all_keyphrases = []\n",
    "\n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Extracting keyphrases\"):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            batch_results = []\n",
    "\n",
    "            for text in batch_texts:\n",
    "                keyphrases = self.extract_keyphrases(text)\n",
    "                batch_results.append(keyphrases)\n",
    "\n",
    "            all_keyphrases.extend(batch_results)\n",
    "\n",
    "        return all_keyphrases\n",
    "\n",
    "    def evaluate_with_partial_matching(self, texts: List[str], ground_truth_keyphrases: List[List[str]]) -> Dict[str, float]:\n",
    "        \n",
    "        total_precision = 0\n",
    "        total_recall = 0\n",
    "        total_f1 = 0\n",
    "\n",
    "        for i, (text, true_keyphrases) in enumerate(zip(texts, ground_truth_keyphrases)):\n",
    "            predicted_keyphrases = self.extract_keyphrases(text)\n",
    "\n",
    "            if self.use_lemmatization:\n",
    "                predicted_lower = [self.lemmatize_text(kp.lower()) for kp in predicted_keyphrases]\n",
    "                true_lower = [self.lemmatize_text(kp.lower()) for kp in true_keyphrases]\n",
    "            else:\n",
    "                predicted_lower = [kp.lower() for kp in predicted_keyphrases]\n",
    "                true_lower = [kp.lower() for kp in true_keyphrases]\n",
    "\n",
    "            exact_matches = set(predicted_lower) & set(true_lower)\n",
    "\n",
    "            partial_matches = set()\n",
    "            for pred in predicted_lower:\n",
    "                if pred in exact_matches:\n",
    "                    continue\n",
    "                for true in true_lower:\n",
    "                    if true in exact_matches:\n",
    "                        continue\n",
    "                    if pred in true or true in pred:\n",
    "                        partial_matches.add((pred, true))\n",
    "\n",
    "            tp = len(exact_matches) + 0.7 * len(partial_matches)\n",
    "            fp = len(predicted_lower) - len(exact_matches) - len(set(p for p, _ in partial_matches))\n",
    "            fn = len(true_lower) - len(exact_matches) - len(set(t for _, t in partial_matches))\n",
    "\n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "            total_precision += precision\n",
    "            total_recall += recall\n",
    "            total_f1 += f1\n",
    "\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"Processed {i + 1}/{len(texts)} texts\")\n",
    "\n",
    "        avg_precision = total_precision / len(texts)\n",
    "        avg_recall = total_recall / len(texts)\n",
    "        avg_f1 = total_f1 / len(texts)\n",
    "\n",
    "        return {\n",
    "            \"precision\": avg_precision,\n",
    "            \"recall\": avg_recall,\n",
    "            \"f1\": avg_f1\n",
    "        }\n",
    "\n",
    "    def evaluate(self, texts: List[str], ground_truth_keyphrases: List[List[str]]) -> Dict[str, float]:\n",
    "        \n",
    "        print(f\"DEBUG: Evaluating with use_partial_matching={self.use_partial_matching}, use_semantic_matching={self.use_semantic_matching}, use_lemmatization={self.use_lemmatization}\")\n",
    "\n",
    "        total_precision = 0\n",
    "        total_recall = 0\n",
    "        total_f1 = 0\n",
    "\n",
    "        for i, (text, true_keyphrases) in enumerate(zip(texts, ground_truth_keyphrases)):\n",
    "            predicted_keyphrases = self.extract_keyphrases(text)\n",
    "\n",
    "            print(f\"DEBUG: Evaluating text {i+1}: {len(predicted_keyphrases)} predicted vs {len(true_keyphrases)} true keyphrases\")\n",
    "\n",
    "            if self.use_lemmatization:\n",
    "                predicted_lower = [self.lemmatize_text(kp.lower()) for kp in predicted_keyphrases]\n",
    "                true_lower = [self.lemmatize_text(kp.lower()) for kp in true_keyphrases]\n",
    "                print(f\"DEBUG: Applied lemmatization\")\n",
    "            else:\n",
    "                predicted_lower = [kp.lower() for kp in predicted_keyphrases]\n",
    "                true_lower = [kp.lower() for kp in true_keyphrases]\n",
    "\n",
    "            exact_matches = set(predicted_lower) & set(true_lower)\n",
    "            print(f\"DEBUG: Found {len(exact_matches)} exact matches\")\n",
    "\n",
    "            partial_matches = set()\n",
    "            semantic_matches = []\n",
    "\n",
    "            if self.use_partial_matching:\n",
    "                for pred in predicted_lower:\n",
    "                    if pred in exact_matches:\n",
    "                        continue\n",
    "                    for true in true_lower:\n",
    "                        if true in exact_matches:\n",
    "                            continue\n",
    "                        if pred in true or true in pred:\n",
    "                            partial_matches.add((pred, true))\n",
    "                print(f\"DEBUG: Found {len(partial_matches)} partial matches\")\n",
    "\n",
    "            if self.use_semantic_matching and (set(predicted_lower) - exact_matches) and (set(true_lower) - exact_matches):\n",
    "                remaining_pred = [p for p in predicted_lower if p not in exact_matches and not any(p in t or t in p for _, t in partial_matches)]\n",
    "                remaining_true = [t for t in true_lower if t not in exact_matches and not any(p in t or t in p for p, _ in partial_matches)]\n",
    "\n",
    "                if remaining_pred and remaining_true:\n",
    "                    pred_embeddings = self.get_embeddings(remaining_pred)\n",
    "                    true_embeddings = self.get_embeddings(remaining_true)\n",
    "\n",
    "                    similarity_matrix = cosine_similarity(pred_embeddings, true_embeddings)\n",
    "\n",
    "                    for p_idx, p in enumerate(remaining_pred):\n",
    "                        for t_idx, t in enumerate(remaining_true):\n",
    "                            similarity = similarity_matrix[p_idx, t_idx]\n",
    "                            if similarity > 0.6:\n",
    "                                semantic_matches.append((p, t, similarity))\n",
    "                print(f\"DEBUG: Found {len(semantic_matches)} semantic matches\")\n",
    "\n",
    "            if self.use_partial_matching or self.use_semantic_matching:\n",
    "                tp = len(exact_matches) + 0.7 * len(partial_matches) + 0.8 * len(semantic_matches)\n",
    "                fp = len(predicted_lower) - len(exact_matches) - len(set(p for p, _ in partial_matches)) - len(set(p for p, _, _ in semantic_matches))\n",
    "                fn = len(true_lower) - len(exact_matches) - len(set(t for _, t in partial_matches)) - len(set(t for _, t, _ in semantic_matches))\n",
    "            else:\n",
    "                tp = len(exact_matches)\n",
    "                fp = len(set(predicted_lower) - exact_matches)\n",
    "                fn = len(set(true_lower) - exact_matches)\n",
    "\n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "            print(f\"DEBUG: Metrics - P={precision:.4f}, R={recall:.4f}, F1={f1:.4f}\")\n",
    "\n",
    "            total_precision += precision\n",
    "            total_recall += recall\n",
    "            total_f1 += f1\n",
    "\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"Processed {i + 1}/{len(texts)} texts\")\n",
    "\n",
    "        avg_precision = total_precision / len(texts)\n",
    "        avg_recall = total_recall / len(texts)\n",
    "        avg_f1 = total_f1 / len(texts)\n",
    "\n",
    "        print(f\"DEBUG: Final metrics - P={avg_precision:.4f}, R={avg_recall:.4f}, F1={avg_f1:.4f}\")\n",
    "\n",
    "        return {\n",
    "            \"precision\": avg_precision,\n",
    "            \"recall\": avg_recall,\n",
    "            \"f1\": avg_f1\n",
    "        }\n",
    "\n",
    "    def evaluate_with_semantic_matching(self, texts: List[str], ground_truth_keyphrases: List[List[str]]) -> Dict[str, float]:\n",
    "        \n",
    "        total_precision = 0\n",
    "        total_recall = 0\n",
    "        total_f1 = 0\n",
    "\n",
    "        for i, (text, true_keyphrases) in enumerate(zip(texts, ground_truth_keyphrases)):\n",
    "            predicted_keyphrases = self.extract_keyphrases(text)\n",
    "\n",
    "            if self.use_lemmatization:\n",
    "                predicted_processed = [self.lemmatize_text(kp.lower()) for kp in predicted_keyphrases]\n",
    "                true_processed = [self.lemmatize_text(kp.lower()) for kp in true_keyphrases]\n",
    "            else:\n",
    "                predicted_processed = [kp.lower() for kp in predicted_keyphrases]\n",
    "                true_processed = [kp.lower() for kp in true_keyphrases]\n",
    "\n",
    "            exact_matches = set(predicted_processed) & set(true_processed)\n",
    "\n",
    "            remaining_pred = [p for p in predicted_processed if p not in exact_matches]\n",
    "            remaining_true = [t for t in true_processed if t not in exact_matches]\n",
    "\n",
    "            if not remaining_pred or not remaining_true:\n",
    "                semantic_matches = []\n",
    "            else:\n",
    "                pred_embeddings = self.get_embeddings(remaining_pred)\n",
    "                true_embeddings = self.get_embeddings(remaining_true)\n",
    "\n",
    "                similarity_matrix = cosine_similarity(pred_embeddings, true_embeddings)\n",
    "\n",
    "                semantic_matches = []\n",
    "                for p_idx, p in enumerate(remaining_pred):\n",
    "                    for t_idx, t in enumerate(remaining_true):\n",
    "                        similarity = similarity_matrix[p_idx, t_idx]\n",
    "                        if similarity > 0.6:\n",
    "                            semantic_matches.append((p, t, similarity))\n",
    "\n",
    "            tp = len(exact_matches) + len(semantic_matches)\n",
    "            fp = len(predicted_processed) - len(exact_matches) - len(set(p for p, _, _ in semantic_matches))\n",
    "            fn = len(true_processed) - len(exact_matches) - len(set(t for _, t, _ in semantic_matches))\n",
    "\n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "            total_precision += precision\n",
    "            total_recall += recall\n",
    "            total_f1 += f1\n",
    "\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"Processed {i + 1}/{len(texts)} texts\")\n",
    "\n",
    "        avg_precision = total_precision / len(texts)\n",
    "        avg_recall = total_recall / len(texts)\n",
    "        avg_f1 = total_f1 / len(texts)\n",
    "\n",
    "        return {\n",
    "            \"precision\": avg_precision,\n",
    "            \"recall\": avg_recall,\n",
    "            \"f1\": avg_f1\n",
    "        }\n",
    "\n",
    "    def evaluate_with_details(self, texts: List[str], ground_truth_keyphrases: List[List[str]]) -> Dict:\n",
    "        \n",
    "        total_precision = 0\n",
    "        total_recall = 0\n",
    "        total_f1 = 0\n",
    "        per_article_results = []\n",
    "\n",
    "        for i, (text, true_keyphrases) in enumerate(zip(texts, ground_truth_keyphrases)):\n",
    "            predicted_keyphrases = self.extract_keyphrases(text)\n",
    "\n",
    "            if self.use_lemmatization:\n",
    "                predicted_processed = [self.lemmatize_text(kp.lower()) for kp in predicted_keyphrases]\n",
    "                true_processed = [self.lemmatize_text(kp.lower()) for kp in true_keyphrases]\n",
    "            else:\n",
    "                predicted_processed = [kp.lower() for kp in predicted_keyphrases]\n",
    "                true_processed = [kp.lower() for kp in true_keyphrases]\n",
    "\n",
    "            exact_matches = set(predicted_processed) & set(true_processed)\n",
    "\n",
    "            partial_matches = set()\n",
    "            if self.use_partial_matching:\n",
    "                for pred in predicted_processed:\n",
    "                    if pred in exact_matches:\n",
    "                        continue\n",
    "                    for true in true_processed:\n",
    "                        if true in exact_matches:\n",
    "                            continue\n",
    "                        if pred in true or true in pred:\n",
    "                            partial_matches.add((pred, true))\n",
    "\n",
    "            semantic_matches = []\n",
    "            if self.use_semantic_matching and (set(predicted_processed) - exact_matches) and (set(true_processed) - exact_matches):\n",
    "                remaining_pred = [p for p in predicted_processed if p not in exact_matches and not any(p in t or t in p for _, t in partial_matches)]\n",
    "                remaining_true = [t for t in true_processed if t not in exact_matches and not any(p in t or t in p for p, _ in partial_matches)]\n",
    "\n",
    "                if remaining_pred and remaining_true:\n",
    "                    pred_embeddings = self.get_embeddings(remaining_pred)\n",
    "                    true_embeddings = self.get_embeddings(remaining_true)\n",
    "\n",
    "                    similarity_matrix = cosine_similarity(pred_embeddings, true_embeddings)\n",
    "\n",
    "                    for p_idx, p in enumerate(remaining_pred):\n",
    "                        for t_idx, t in enumerate(remaining_true):\n",
    "                            similarity = similarity_matrix[p_idx, t_idx]\n",
    "                            if similarity > 0.6:\n",
    "                                semantic_matches.append((p, t, similarity_matrix[p_idx, t_idx]))\n",
    "\n",
    "            if self.use_partial_matching or self.use_semantic_matching:\n",
    "                tp = len(exact_matches) + 0.7 * len(partial_matches) + 0.8 * len(semantic_matches)\n",
    "                fp = len(predicted_processed) - len(exact_matches) - len(set(p for p, _ in partial_matches)) - len(set(p for p, _, _ in semantic_matches))\n",
    "                fn = len(true_processed) - len(exact_matches) - len(set(t for _, t in partial_matches)) - len(set(t for _, t, _ in semantic_matches))\n",
    "\n",
    "            else:\n",
    "                tp = len(exact_matches)\n",
    "                fp = len(set(predicted_processed) - exact_matches)\n",
    "                fn = len(set(true_processed) - exact_matches)\n",
    "\n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "            total_precision += precision\n",
    "            total_recall += recall\n",
    "            total_f1 += f1\n",
    "\n",
    "            text_snippet = text[:100] + \"...\" if len(text) > 100 else text\n",
    "            per_article_results.append({\n",
    "                \"text_snippet\": text_snippet,\n",
    "                \"predicted_keyphrases\": predicted_keyphrases,\n",
    "                \"true_keyphrases\": true_keyphrases,\n",
    "                \"exact_matches\": list(exact_matches),\n",
    "                \"partial_matches\": [(p, t) for p, t in partial_matches] if self.use_partial_matching else [],\n",
    "                \"semantic_matches\": [(p, t) for p, t, _ in semantic_matches] if self.use_semantic_matching else [],\n",
    "                \"precision\": precision,\n",
    "                \"recall\": recall,\n",
    "                \"f1\": f1\n",
    "            })\n",
    "\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"Processed {i + 1}/{len(texts)} texts\")\n",
    "\n",
    "        avg_precision = total_precision / len(texts)\n",
    "        avg_recall = total_recall / len(texts)\n",
    "        avg_f1 = total_f1 / len(texts)\n",
    "\n",
    "        all_false_positives = []\n",
    "        all_false_negatives = []\n",
    "\n",
    "        for result in per_article_results:\n",
    "            all_matches_pred = set(result[\"exact_matches\"])\n",
    "            all_matches_pred.update([p for p, _ in result[\"partial_matches\"]])\n",
    "            all_matches_pred.update([p for p, _ in result[\"semantic_matches\"]])\n",
    "\n",
    "            all_matches_true = set(result[\"exact_matches\"])\n",
    "            all_matches_true.update([t for _, t in result[\"partial_matches\"]])\n",
    "            all_matches_true.update([t for _, t in result[\"semantic_matches\"]])\n",
    "\n",
    "            if self.use_lemmatization:\n",
    "                predicted_processed = [self.lemmatize_text(kp.lower()) for kp in result[\"predicted_keyphrases\"]]\n",
    "                true_processed = [self.lemmatize_text(kp.lower()) for kp in result[\"true_keyphrases\"]]\n",
    "            else:\n",
    "                predicted_processed = [kp.lower() for kp in result[\"predicted_keyphrases\"]]\n",
    "                true_processed = [kp.lower() for kp in result[\"true_keyphrases\"]]\n",
    "\n",
    "            false_positives = [p for p in predicted_processed if p not in all_matches_pred]\n",
    "            false_negatives = [t for t in true_processed if t not in all_matches_true]\n",
    "\n",
    "            all_false_positives.extend(false_positives)\n",
    "            all_false_negatives.extend(false_negatives)\n",
    "\n",
    "        fp_counts = {}\n",
    "        fn_counts = {}\n",
    "\n",
    "        for fp in all_false_positives:\n",
    "            fp_counts[fp] = fp_counts.get(fp, 0) + 1\n",
    "\n",
    "        for fn in all_false_negatives:\n",
    "            fn_counts[fn] = fn_counts.get(fn, 0) + 1\n",
    "\n",
    "        common_fps = sorted(fp_counts.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "        common_fns = sorted(fn_counts.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "\n",
    "        return {\n",
    "            \"average_precision\": avg_precision,\n",
    "            \"average_recall\": avg_recall,\n",
    "            \"average_f1\": avg_f1,\n",
    "            \"per_article_results\": per_article_results,\n",
    "            \"common_false_positives\": [fp for fp, _ in common_fps],\n",
    "            \"common_false_negatives\": [fn for fn, _ in common_fns]\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T06:27:37.697048Z",
     "iopub.status.busy": "2025-04-12T06:27:37.696810Z",
     "iopub.status.idle": "2025-04-12T06:27:37.797486Z",
     "shell.execute_reply": "2025-04-12T06:27:37.796606Z",
     "shell.execute_reply.started": "2025-04-12T06:27:37.697028Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined comprehensive domain keyword lists in global namespace\n"
     ]
    }
   ],
   "source": [
    "# TECHNOLOGY DOMAIN\n",
    "TECHNOLOGY_KEYWORDS = [\n",
    "    # Core technology terms\n",
    "    \"technology\", \"tech\", \"digital\", \"software\", \"hardware\", \"device\", \"gadget\", \"innovation\",\n",
    "    \"computer\", \"computing\", \"processor\", \"chip\", \"semiconductor\", \"electronics\", \"electronic\",\n",
    "    \"algorithm\", \"code\", \"programming\", \"developer\", \"development\", \"application\", \"app\",\n",
    "    \n",
    "    # Internet and web\n",
    "    \"internet\", \"web\", \"website\", \"online\", \"cloud\", \"server\", \"hosting\", \"domain\",\n",
    "    \"browser\", \"search engine\", \"social media\", \"platform\", \"e-commerce\", \"digital marketing\",\n",
    "    \"SEO\", \"SEM\", \"analytics\", \"data\", \"big data\", \"database\", \"SQL\", \"NoSQL\",\n",
    "    \n",
    "    # Mobile and devices\n",
    "    \"mobile\", \"smartphone\", \"tablet\", \"laptop\", \"desktop\", \"wearable\", \"IoT\", \"internet of things\",\n",
    "    \"smart home\", \"smart device\", \"wireless\", \"bluetooth\", \"GPS\", \"sensor\", \"camera\", \"display\",\n",
    "    \"touchscreen\", \"battery\", \"charger\", \"portable\", \"connectivity\", \"interface\",\n",
    "    \n",
    "    # Software and applications\n",
    "    \"operating system\", \"OS\", \"Windows\", \"macOS\", \"Linux\", \"Android\", \"iOS\", \"application\",\n",
    "    \"software\", \"program\", \"app\", \"mobile app\", \"web app\", \"SaaS\", \"API\", \"SDK\",\n",
    "    \"framework\", \"library\", \"plugin\", \"extension\", \"update\", \"version\", \"bug\", \"patch\",\n",
    "    \n",
    "    # Hardware and components\n",
    "    \"hardware\", \"CPU\", \"processor\", \"GPU\", \"graphics card\", \"memory\", \"RAM\", \"storage\",\n",
    "    \"SSD\", \"HDD\", \"motherboard\", \"circuit\", \"chip\", \"semiconductor\", \"transistor\",\n",
    "    \"peripheral\", \"input device\", \"output device\", \"monitor\", \"keyboard\", \"mouse\", \"printer\",\n",
    "    \n",
    "    # Networks and communications\n",
    "    \"network\", \"networking\", \"LAN\", \"WAN\", \"WiFi\", \"wireless\", \"router\", \"modem\",\n",
    "    \"broadband\", \"bandwidth\", \"protocol\", \"IP\", \"TCP/IP\", \"DNS\", \"HTTP\", \"HTTPS\",\n",
    "    \"encryption\", \"firewall\", \"VPN\", \"proxy\", \"server\", \"client\", \"peer-to-peer\", \"P2P\",\n",
    "    \n",
    "    # Emerging technologies\n",
    "    \"artificial intelligence\", \"AI\", \"machine learning\", \"ML\", \"deep learning\", \"neural network\",\n",
    "    \"blockchain\", \"cryptocurrency\", \"bitcoin\", \"ethereum\", \"NFT\", \"virtual reality\", \"VR\",\n",
    "    \"augmented reality\", \"AR\", \"mixed reality\", \"MR\", \"quantum computing\", \"robotics\",\n",
    "    \"automation\", \"drone\", \"autonomous vehicle\", \"self-driving\", \"3D printing\", \"additive manufacturing\",\n",
    "    \n",
    "    # Cybersecurity\n",
    "    \"cybersecurity\", \"security\", \"privacy\", \"encryption\", \"decryption\", \"authentication\",\n",
    "    \"authorization\", \"password\", \"biometric\", \"fingerprint\", \"facial recognition\", \"firewall\",\n",
    "    \"antivirus\", \"malware\", \"virus\", \"trojan\", \"ransomware\", \"phishing\", \"hacking\", \"hacker\",\n",
    "    \"vulnerability\", \"exploit\", \"patch\", \"security breach\", \"data breach\", \"identity theft\",\n",
    "    \n",
    "    # Tech industry\n",
    "    \"startup\", \"tech company\", \"Silicon Valley\", \"venture capital\", \"VC\", \"funding\", \"IPO\",\n",
    "    \"acquisition\", \"merger\", \"tech giant\", \"FAANG\", \"Apple\", \"Google\", \"Microsoft\", \"Amazon\",\n",
    "    \"Facebook\", \"Meta\", \"Netflix\", \"Tesla\", \"Uber\", \"Airbnb\", \"tech industry\", \"tech sector\",\n",
    "    \n",
    "    # Tech roles and skills\n",
    "    \"programmer\", \"developer\", \"software engineer\", \"web developer\", \"full-stack\", \"front-end\",\n",
    "    \"back-end\", \"DevOps\", \"SRE\", \"data scientist\", \"data analyst\", \"UX designer\", \"UI designer\",\n",
    "    \"product manager\", \"project manager\", \"CTO\", \"CIO\", \"CISO\", \"IT\", \"information technology\",\n",
    "    \n",
    "    # Tech events and trends\n",
    "    \"hackathon\", \"tech conference\", \"tech event\", \"tech trend\", \"digital transformation\",\n",
    "    \"disruption\", \"innovation\", \"tech adoption\", \"tech integration\", \"tech solution\",\n",
    "    \"tech implementation\", \"tech infrastructure\", \"tech ecosystem\", \"tech community\"\n",
    "]\n",
    "\n",
    "# BUSINESS DOMAIN\n",
    "BUSINESS_KEYWORDS = [\n",
    "    # Core business terms\n",
    "    \"business\", \"company\", \"corporation\", \"enterprise\", \"firm\", \"organization\", \"industry\",\n",
    "    \"sector\", \"market\", \"commercial\", \"corporate\", \"trade\", \"commerce\", \"venture\",\n",
    "    \n",
    "    # Business operations\n",
    "    \"operations\", \"management\", \"administration\", \"logistics\", \"supply chain\", \"procurement\",\n",
    "    \"inventory\", \"warehouse\", \"distribution\", \"production\", \"manufacturing\", \"quality control\",\n",
    "    \"process\", \"efficiency\", \"optimization\", \"automation\", \"outsourcing\", \"offshoring\",\n",
    "    \n",
    "    # Finance and accounting\n",
    "    \"finance\", \"financial\", \"accounting\", \"revenue\", \"profit\", \"loss\", \"income\", \"expense\",\n",
    "    \"budget\", \"forecast\", \"projection\", \"cash flow\", \"balance sheet\", \"income statement\",\n",
    "    \"statement of cash flows\", \"asset\", \"liability\", \"equity\", \"debt\", \"credit\", \"loan\",\n",
    "    \"investment\", \"investor\", \"shareholder\", \"stakeholder\", \"dividend\", \"capital\", \"funding\",\n",
    "    \n",
    "    # Markets and economics\n",
    "    \"market\", \"marketplace\", \"market share\", \"market segment\", \"market research\", \"market analysis\",\n",
    "    \"economy\", \"economic\", \"macroeconomic\", \"microeconomic\", \"supply\", \"demand\", \"price\",\n",
    "    \"inflation\", \"deflation\", \"recession\", \"growth\", \"GDP\", \"consumer\", \"consumption\",\n",
    "    \"producer\", \"production\", \"competition\", \"competitive\", \"monopoly\", \"oligopoly\",\n",
    "    \n",
    "    # Sales and marketing\n",
    "    \"sales\", \"selling\", \"marketing\", \"advertise\", \"advertising\", \"promotion\", \"campaign\",\n",
    "    \"brand\", \"branding\", \"customer\", \"client\", \"consumer\", \"B2B\", \"B2C\", \"lead\", \"prospect\",\n",
    "    \"conversion\", \"funnel\", \"retention\", \"loyalty\", \"market research\", \"focus group\",\n",
    "    \"survey\", \"segmentation\", \"targeting\", \"positioning\", \"value proposition\", \"USP\",\n",
    "    \n",
    "    # Management and leadership\n",
    "    \"management\", \"leadership\", \"executive\", \"C-suite\", \"CEO\", \"CFO\", \"COO\", \"CTO\", \"CMO\",\n",
    "    \"manager\", \"supervisor\", \"director\", \"board\", \"governance\", \"strategy\", \"strategic\",\n",
    "    \"vision\", \"mission\", \"goal\", \"objective\", \"KPI\", \"performance\", \"evaluation\", \"assessment\",\n",
    "    \"feedback\", \"coaching\", \"mentoring\", \"delegation\", \"authority\", \"responsibility\",\n",
    "    \n",
    "    # Human resources\n",
    "    \"human resources\", \"HR\", \"personnel\", \"staff\", \"employee\", \"employer\", \"workforce\",\n",
    "    \"talent\", \"recruitment\", \"hiring\", \"onboarding\", \"training\", \"development\", \"performance\",\n",
    "    \"review\", \"compensation\", \"salary\", \"wage\", \"benefit\", \"perk\", \"incentive\", \"bonus\",\n",
    "    \"commission\", \"retention\", \"turnover\", \"culture\", \"engagement\", \"satisfaction\",\n",
    "    \n",
    "    # Entrepreneurship and startups\n",
    "    \"entrepreneur\", \"entrepreneurship\", \"startup\", \"start-up\", \"founder\", \"co-founder\",\n",
    "    \"venture capital\", \"VC\", \"angel investor\", \"seed funding\", \"Series A\", \"Series B\",\n",
    "    \"incubator\", \"accelerator\", \"pitch\", \"business plan\", \"business model\", \"MVP\",\n",
    "    \"minimum viable product\", \"pivot\", \"scale\", \"scaling\", \"growth\", \"exit\", \"acquisition\",\n",
    "    \n",
    "    # Business strategy\n",
    "    \"strategy\", \"strategic\", \"planning\", \"business plan\", \"forecast\", \"projection\",\n",
    "    \"SWOT\", \"strength\", \"weakness\", \"opportunity\", \"threat\", \"competitive advantage\",\n",
    "    \"differentiation\", \"diversification\", \"integration\", \"expansion\", \"growth\", \"merger\",\n",
    "    \"acquisition\", \"joint venture\", \"partnership\", \"alliance\", \"collaboration\",\n",
    "    \n",
    "    # Business law and regulations\n",
    "    \"legal\", \"law\", \"regulation\", \"compliance\", \"regulatory\", \"contract\", \"agreement\",\n",
    "    \"terms\", \"conditions\", \"intellectual property\", \"IP\", \"patent\", \"trademark\", \"copyright\",\n",
    "    \"licensing\", \"liability\", \"lawsuit\", \"litigation\", \"arbitration\", \"mediation\",\n",
    "    \"corporate governance\", \"board of directors\", \"shareholder\", \"fiduciary duty\"\n",
    "]\n",
    "\n",
    "# HEALTH DOMAIN\n",
    "HEALTH_KEYWORDS = [\n",
    "    # Core health terms\n",
    "    \"health\", \"healthcare\", \"medical\", \"medicine\", \"clinical\", \"patient\", \"doctor\", \"physician\",\n",
    "    \"nurse\", \"hospital\", \"clinic\", \"care\", \"treatment\", \"therapy\", \"wellness\", \"wellbeing\",\n",
    "    \n",
    "    # Medical specialties\n",
    "    \"cardiology\", \"dermatology\", \"endocrinology\", \"gastroenterology\", \"gynecology\", \"neurology\",\n",
    "    \"oncology\", \"ophthalmology\", \"orthopedics\", \"pediatrics\", \"psychiatry\", \"radiology\",\n",
    "    \"surgery\", \"urology\", \"primary care\", \"family medicine\", \"internal medicine\", \"emergency medicine\",\n",
    "    \n",
    "    # Diseases and conditions\n",
    "    \"disease\", \"condition\", \"disorder\", \"syndrome\", \"illness\", \"infection\", \"inflammation\",\n",
    "    \"cancer\", \"diabetes\", \"hypertension\", \"heart disease\", \"stroke\", \"asthma\", \"arthritis\",\n",
    "    \"alzheimer's\", \"dementia\", \"obesity\", \"malnutrition\", \"allergy\", \"autoimmune\", \"chronic\",\n",
    "    \"acute\", \"congenital\", \"genetic\", \"hereditary\", \"viral\", \"bacterial\", \"fungal\", \"parasitic\",\n",
    "    \n",
    "    # Symptoms and diagnosis\n",
    "    \"symptom\", \"sign\", \"diagnosis\", \"diagnostic\", \"test\", \"screening\", \"examination\", \"assessment\",\n",
    "    \"evaluation\", \"scan\", \"x-ray\", \"MRI\", \"CT scan\", \"ultrasound\", \"blood test\", \"urine test\",\n",
    "    \"biopsy\", \"pathology\", \"lab\", \"laboratory\", \"biomarker\", \"indicator\", \"prognosis\",\n",
    "    \n",
    "    # Treatments and interventions\n",
    "    \"treatment\", \"therapy\", \"intervention\", \"procedure\", \"surgery\", \"operation\", \"medication\",\n",
    "    \"drug\", \"pharmaceutical\", \"prescription\", \"dose\", \"dosage\", \"regimen\", \"protocol\",\n",
    "    \"rehabilitation\", \"physical therapy\", \"occupational therapy\", \"speech therapy\", \"psychotherapy\",\n",
    "    \"counseling\", \"alternative medicine\", \"complementary medicine\", \"holistic\", \"integrative\",\n",
    "    \n",
    "    # Healthcare providers and settings\n",
    "    \"doctor\", \"physician\", \"specialist\", \"surgeon\", \"nurse\", \"practitioner\", \"therapist\",\n",
    "    \"pharmacist\", \"technician\", \"paramedic\", \"EMT\", \"caregiver\", \"hospital\", \"clinic\",\n",
    "    \"emergency room\", \"ER\", \"ICU\", \"intensive care\", \"operating room\", \"OR\", \"ward\",\n",
    "    \"outpatient\", \"inpatient\", \"ambulatory\", \"long-term care\", \"nursing home\", \"hospice\",\n",
    "    \n",
    "    # Health systems and policy\n",
    "    \"healthcare system\", \"health policy\", \"public health\", \"population health\", \"community health\",\n",
    "    \"health insurance\", \"medicare\", \"medicaid\", \"affordable care act\", \"ACA\", \"obamacare\",\n",
    "    \"universal healthcare\", \"single-payer\", \"managed care\", \"HMO\", \"PPO\", \"deductible\",\n",
    "    \"copay\", \"premium\", \"coverage\", \"reimbursement\", \"healthcare cost\", \"healthcare access\",\n",
    "    \n",
    "    # Prevention and wellness\n",
    "    \"prevention\", \"preventive\", \"preventative\", \"wellness\", \"wellbeing\", \"health promotion\",\n",
    "    \"healthy lifestyle\", \"diet\", \"nutrition\", \"exercise\", \"physical activity\", \"fitness\",\n",
    "    \"weight management\", \"smoking cessation\", \"alcohol moderation\", \"vaccination\", \"immunization\",\n",
    "    \"screening\", \"check-up\", \"self-care\", \"stress management\", \"sleep hygiene\", \"mindfulness\",\n",
    "    \n",
    "    # Medical research and technology\n",
    "    \"medical research\", \"clinical trial\", \"study\", \"evidence-based\", \"peer-reviewed\",\n",
    "    \"biomedical\", \"biotechnology\", \"pharmaceutical\", \"drug development\", \"medical device\",\n",
    "    \"medical technology\", \"health technology\", \"digital health\", \"telehealth\", \"telemedicine\",\n",
    "    \"electronic health record\", \"EHR\", \"electronic medical record\", \"EMR\", \"health informatics\",\n",
    "    \n",
    "    # Mental health\n",
    "    \"mental health\", \"mental illness\", \"psychiatric\", \"psychological\", \"emotional health\",\n",
    "    \"behavioral health\", \"depression\", \"anxiety\", \"stress\", \"trauma\", \"PTSD\", \"bipolar\",\n",
    "    \"schizophrenia\", \"eating disorder\", \"addiction\", \"substance abuse\", \"therapy\", \"counseling\",\n",
    "    \"psychiatrist\", \"psychologist\", \"therapist\", \"counselor\", \"mental wellbeing\", \"resilience\"\n",
    "]\n",
    "\n",
    "# POLITICS DOMAIN\n",
    "POLITICS_KEYWORDS = [\n",
    "    # Core political terms\n",
    "    \"politics\", \"political\", \"government\", \"governance\", \"policy\", \"policies\", \"administration\",\n",
    "    \"state\", \"nation\", \"country\", \"public\", \"civic\", \"civil\", \"official\", \"authority\",\n",
    "    \n",
    "    # Political systems and ideologies\n",
    "    \"democracy\", \"democratic\", \"republic\", \"republican\", \"monarchy\", \"authoritarian\", \"totalitarian\",\n",
    "    \"dictatorship\", \"socialism\", \"socialist\", \"communism\", \"communist\", \"capitalism\", \"capitalist\",\n",
    "    \"liberalism\", \"liberal\", \"conservatism\", \"conservative\", \"progressivism\", \"progressive\",\n",
    "    \"libertarian\", \"populism\", \"populist\", \"nationalism\", \"nationalist\", \"globalism\", \"globalist\",\n",
    "    \n",
    "    # Government branches and institutions\n",
    "    \"executive\", \"president\", \"presidential\", \"administration\", \"cabinet\", \"legislative\",\n",
    "    \"legislature\", \"congress\", \"parliamentary\", \"parliament\", \"senate\", \"house of representatives\",\n",
    "    \"judicial\", \"judiciary\", \"court\", \"supreme court\", \"constitutional court\", \"federal\",\n",
    "    \"state\", \"local\", \"municipal\", \"agency\", \"department\", \"ministry\", \"commission\", \"bureau\",\n",
    "    \n",
    "    # Political processes\n",
    "    \"election\", \"electoral\", \"vote\", \"voter\", \"voting\", \"ballot\", \"poll\", \"polling\", \"campaign\",\n",
    "    \"candidacy\", \"candidate\", \"primary\", \"caucus\", \"convention\", \"debate\", \"platform\",\n",
    "    \"constituency\", \"district\", \"gerrymandering\", \"referendum\", \"initiative\", \"recall\",\n",
    "    \"impeachment\", \"censure\", \"filibuster\", \"lobbying\", \"lobbyist\", \"special interest\",\n",
    "    \n",
    "    # Political parties and movements\n",
    "    \"party\", \"partisan\", \"bipartisan\", \"nonpartisan\", \"democrat\", \"democratic party\", \"republican\",\n",
    "    \"republican party\", \"third party\", \"independent\", \"green party\", \"libertarian party\",\n",
    "    \"movement\", \"activism\", \"activist\", \"protest\", \"demonstration\", \"rally\", \"march\",\n",
    "    \"grassroots\", \"coalition\", \"bloc\", \"faction\", \"wing\", \"establishment\", \"mainstream\",\n",
    "    \n",
    "    # Legislation and policy\n",
    "    \"law\", \"legislation\", \"bill\", \"act\", \"statute\", \"regulation\", \"deregulation\", \"policy\",\n",
    "    \"reform\", \"amendment\", \"provision\", \"clause\", \"resolution\", \"mandate\", \"prohibition\",\n",
    "    \"legalization\", \"criminalization\", \"enforcement\", \"implementation\", \"compliance\",\n",
    "    \"fiscal\", \"monetary\", \"budget\", \"appropriation\", \"spending\", \"taxation\", \"subsidy\",\n",
    "    \n",
    "    # International relations\n",
    "    \"international\", \"foreign\", \"diplomatic\", \"diplomacy\", \"bilateral\", \"multilateral\",\n",
    "    \"treaty\", \"agreement\", \"alliance\", \"coalition\", \"summit\", \"negotiation\", \"sanction\",\n",
    "    \"embargo\", \"tariff\", \"trade\", \"aid\", \"development\", \"humanitarian\", \"intervention\",\n",
    "    \"peacekeeping\", \"security\", \"defense\", \"military\", \"war\", \"conflict\", \"terrorism\",\n",
    "    \n",
    "    # Political figures and roles\n",
    "    \"president\", \"vice president\", \"prime minister\", \"chancellor\", \"premier\", \"governor\",\n",
    "    \"mayor\", \"senator\", \"representative\", \"congressman\", \"congresswoman\", \"parliamentarian\",\n",
    "    \"minister\", \"secretary\", \"ambassador\", \"diplomat\", \"official\", \"adviser\", \"chief of staff\",\n",
    "    \"spokesperson\", \"press secretary\", \"candidate\", \"incumbent\", \"opponent\", \"leader\", \"whip\",\n",
    "    \n",
    "    # Political issues and topics\n",
    "    \"issue\", \"agenda\", \"platform\", \"position\", \"stance\", \"policy\", \"debate\", \"controversy\",\n",
    "    \"scandal\", \"corruption\", \"ethics\", \"accountability\", \"transparency\", \"oversight\",\n",
    " # Political issues and topics (continued)\n",
    "    \"issue\", \"agenda\", \"platform\", \"position\", \"stance\", \"policy\", \"debate\", \"controversy\",\n",
    "    \"scandal\", \"corruption\", \"ethics\", \"accountability\", \"transparency\", \"oversight\",\n",
    "    \"reform\", \"rights\", \"civil rights\", \"human rights\", \"equality\", \"inequality\", \"justice\",\n",
    "    \"injustice\", \"discrimination\", \"immigration\", \"environment\", \"climate\", \"healthcare\",\n",
    "    \"education\", \"economy\", \"security\", \"defense\", \"welfare\", \"social security\", \"taxation\"\n",
    "]\n",
    "\n",
    "SPORTS_KEYWORDS = [\n",
    "    # Core sports terms\n",
    "    \"sport\", \"sports\", \"athletic\", \"athlete\", \"competition\", \"competitive\", \"game\", \"match\",\n",
    "    \"tournament\", \"championship\", \"league\", \"team\", \"player\", \"coach\", \"manager\", \"referee\",\n",
    "    \"umpire\", \"official\", \"stadium\", \"arena\", \"field\", \"court\", \"track\", \"pool\", \"rink\",\n",
    "    \n",
    "    # Major sports\n",
    "    \"football\", \"soccer\", \"american football\", \"basketball\", \"baseball\", \"hockey\", \"ice hockey\",\n",
    "    \"tennis\", \"golf\", \"rugby\", \"cricket\", \"volleyball\", \"swimming\", \"track and field\", \"athletics\",\n",
    "    \"boxing\", \"martial arts\", \"wrestling\", \"gymnastics\", \"cycling\", \"skiing\", \"snowboarding\",\n",
    "    \"skateboarding\", \"surfing\", \"motorsport\", \"racing\", \"formula 1\", \"nascar\", \"marathon\",\n",
    "    \n",
    "    # Sports organizations and leagues\n",
    "    \"NFL\", \"National Football League\", \"NBA\", \"National Basketball Association\", \"MLB\",\n",
    "    \"Major League Baseball\", \"NHL\", \"National Hockey League\", \"FIFA\", \"UEFA\", \"Premier League\",\n",
    "    \"La Liga\", \"Bundesliga\", \"Serie A\", \"Ligue 1\", \"MLS\", \"Major League Soccer\", \"ATP\", \"WTA\",\n",
    "    \"PGA\", \"LPGA\", \"Olympics\", \"Olympic Games\", \"Paralympic Games\", \"World Cup\", \"Super Bowl\",\n",
    "    \n",
    "    # Sports performance and statistics\n",
    "    \"score\", \"scoreboard\", \"point\", \"goal\", \"touchdown\", \"basket\", \"home run\", \"hit\", \"shot\",\n",
    "    \"pass\", \"assist\", \"rebound\", \"save\", \"block\", \"tackle\", \"foul\", \"penalty\", \"free throw\",\n",
    "    \"statistic\", \"stat\", \"record\", \"ranking\", \"standing\", \"leaderboard\", \"average\", \"percentage\",\n",
    "    \"win\", \"loss\", \"tie\", \"draw\", \"victory\", \"defeat\", \"champion\", \"runner-up\", \"finalist\",\n",
    "    \n",
    "    # Sports equipment and gear\n",
    "    \"equipment\", \"gear\", \"uniform\", \"jersey\", \"kit\", \"ball\", \"bat\", \"racket\", \"club\", \"stick\",\n",
    "    \"puck\", \"helmet\", \"pad\", \"glove\", \"shoe\", \"cleat\", \"boot\", \"protective gear\", \"goalpost\",\n",
    "    \"net\", \"hoop\", \"basket\", \"goal\", \"flag\", \"whistle\", \"stopwatch\", \"scoreboard\", \"technology\",\n",
    "    \n",
    "    # Sports events and seasons\n",
    "    \"season\", \"preseason\", \"regular season\", \"postseason\", \"playoff\", \"final\", \"semifinal\",\n",
    "    \"quarterfinal\", \"round\", \"match\", \"game\", \"series\", \"tournament\", \"championship\", \"cup\",\n",
    "    \"open\", \"invitational\", \"classic\", \"masters\", \"grand slam\", \"all-star\", \"draft\", \"combine\",\n",
    "    \"training camp\", \"practice\", \"scrimmage\", \"exhibition\", \"friendly\", \"international\",\n",
    "    \n",
    "    # Sports roles and positions\n",
    "    \"player\", \"starter\", \"substitute\", \"reserve\", \"captain\", \"coach\", \"manager\", \"trainer\",\n",
    "    \"staff\", \"front office\", \"general manager\", \"director\", \"scout\", \"agent\", \"referee\",\n",
    "    \"umpire\", \"official\", \"linesman\", \"position\", \"offense\", \"defense\", \"goalkeeper\", \"goalie\",\n",
    "    \"forward\", \"midfielder\", \"defender\", \"pitcher\", \"catcher\", \"infielder\", \"outfielder\",\n",
    "    \n",
    "    # Sports performance and training\n",
    "    \"performance\", \"skill\", \"technique\", \"strategy\", \"tactic\", \"play\", \"formation\", \"system\",\n",
    "    \"training\", \"practice\", \"drill\", \"workout\", \"exercise\", \"conditioning\", \"strength\",\n",
    "    \"endurance\", \"speed\", \"agility\", \"flexibility\", \"fitness\", \"nutrition\", \"recovery\",\n",
    "    \"injury\", \"rehabilitation\", \"physical therapy\", \"sports medicine\", \"sports psychology\",\n",
    "    \n",
    "    # Sports culture and fandom\n",
    "    \"fan\", \"supporter\", \"spectator\", \"audience\", \"crowd\", \"attendance\", \"cheer\", \"chant\",\n",
    "    \"mascot\", \"tradition\", \"rivalry\", \"derby\", \"classic\", \"culture\", \"spirit\", \"pride\",\n",
    "    \"loyalty\", \"merchandise\", \"memorabilia\", \"collectible\", \"autograph\", \"hall of fame\",\n",
    "    \"legacy\", \"history\", \"dynasty\", \"era\", \"generation\", \"legend\", \"icon\", \"superstar\"\n",
    "]\n",
    "\n",
    "# ENTERTAINMENT DOMAIN\n",
    "ENTERTAINMENT_KEYWORDS = [\n",
    "    # Core entertainment terms\n",
    "    \"entertainment\", \"media\", \"show\", \"performance\", \"performer\", \"artist\", \"celebrity\",\n",
    "    \"star\", \"famous\", \"popular\", \"audience\", \"fan\", \"viewer\", \"spectator\", \"critic\", \"review\",\n",
    "    \n",
    "    # Film and television\n",
    "    \"movie\", \"film\", \"cinema\", \"theatrical\", \"feature\", \"short film\", \"documentary\",\n",
    "    \"animation\", \"television\", \"TV\", \"show\", \"series\", \"episode\", \"season\", \"pilot\",\n",
    "    \"sitcom\", \"drama\", \"comedy\", \"action\", \"thriller\", \"horror\", \"sci-fi\", \"fantasy\",\n",
    "    \"romance\", \"western\", \"crime\", \"mystery\", \"biographical\", \"historical\", \"adaptation\",\n",
    "    \n",
    "    # Film and TV production\n",
    "    \"director\", \"producer\", \"writer\", \"screenwriter\", \"cinematographer\", \"editor\", \"production\",\n",
    "    \"post-production\", \"special effects\", \"visual effects\", \"CGI\", \"animation\", \"soundtrack\",\n",
    "    \"score\", \"composer\", \"casting\", \"actor\", \"actress\", \"star\", \"supporting role\", \"cameo\",\n",
    "    \"stunt\", \"crew\", \"set\", \"location\", \"studio\", \"filming\", \"shooting\", \"scene\", \"take\",\n",
    "    \n",
    "    # Music\n",
    "    \"music\", \"song\", \"track\", \"album\", \"EP\", \"single\", \"record\", \"recording\", \"audio\",\n",
    "    \"musician\", \"artist\", \"band\", \"group\", \"singer\", \"vocalist\", \"songwriter\", \"composer\",\n",
    "    \"producer\", \"DJ\", \"rapper\", \"pop\", \"rock\", \"hip-hop\", \"rap\", \"R&B\", \"jazz\", \"blues\",\n",
    "    \"classical\", \"country\", \"folk\", \"electronic\", \"dance\", \"metal\", \"punk\", \"alternative\",\n",
    "    \n",
    "    # Music industry and performance\n",
    "    \"concert\", \"tour\", \"gig\", \"show\", \"performance\", \"live\", \"venue\", \"stage\", \"festival\",\n",
    "    \"music festival\", \"ticket\", \"box office\", \"chart\", \"billboard\", \"hit\", \"platinum\", \"gold\",\n",
    "    \"Grammy\", \"award\", \"nomination\", \"record label\", \"indie\", \"mainstream\", \"underground\",\n",
    "    \"streaming\", \"download\", \"album sales\", \"merchandise\", \"fan base\", \"following\",\n",
    "    \n",
    "    # Theater and performing arts\n",
    "    \"theater\", \"theatre\", \"stage\", \"play\", \"musical\", \"opera\", \"ballet\", \"dance\", \"choreography\",\n",
    "    \"performance\", \"production\", \"show\", \"actor\", \"actress\", \"performer\", \"director\",\n",
    "    \"playwright\", \"script\", \"dialogue\", \"monologue\", \"scene\", \"act\", \"intermission\",\n",
    "    \"Broadway\", \"West End\", \"off-Broadway\", \"touring\", \"repertory\", \"ensemble\", \"cast\",\n",
    "    \n",
    "    # Gaming and interactive entertainment\n",
    "    \"game\", \"video game\", \"gaming\", \"gamer\", \"player\", \"console\", \"PC gaming\", \"mobile gaming\",\n",
    "    \"esports\", \"competitive gaming\", \"multiplayer\", \"single-player\", \"RPG\", \"FPS\", \"MMORPG\",\n",
    "    \"strategy\", \"simulation\", \"adventure\", \"puzzle\", \"developer\", \"publisher\", \"release\",\n",
    "    \"launch\", \"DLC\", \"expansion\", \"update\", \"patch\", \"mod\", \"modding\", \"streaming\", \"Twitch\",\n",
    "    \n",
    "    # Celebrity and pop culture\n",
    "    \"celebrity\", \"star\", \"famous\", \"A-list\", \"icon\", \"public figure\", \"personality\", \"influencer\",\n",
    "    \"fame\", \"popularity\", \"fan\", \"fandom\", \"follower\", \"paparazzi\", \"tabloid\", \"gossip\",\n",
    "    \"rumor\", \"scandal\", \"controversy\", \"interview\", \"press\", \"media coverage\", \"red carpet\",\n",
    "    \"award show\", \"premiere\", \"appearance\", \"endorsement\", \"sponsorship\", \"brand deal\",\n",
    "    \n",
    "    # Entertainment industry\n",
    "    \"industry\", \"business\", \"studio\", \"network\", \"channel\", \"platform\", \"streaming service\",\n",
    "    \"Netflix\", \"Disney+\", \"HBO\", \"Amazon Prime\", \"Hulu\", \"production company\", \"distribution\",\n",
    "    \"box office\", \"rating\", \"viewership\", \"audience\", \"demographic\", \"market\", \"trend\",\n",
    "    \"franchise\", \"sequel\", \"prequel\", \"reboot\", \"remake\", \"adaptation\", \"original content\",\n",
    "    \n",
    "    # Entertainment events and venues\n",
    "    \"event\", \"festival\", \"convention\", \"expo\", \"comic-con\", \"premiere\", \"opening\", \"closing\",\n",
    "    \"ceremony\", \"award show\", \"Oscars\", \"Academy Awards\", \"Emmy\", \"Grammy\", \"Tony\", \"Golden Globe\",\n",
    "    \"venue\", \"theater\", \"cinema\", \"multiplex\", \"arena\", \"stadium\", \"amphitheater\", \"club\",\n",
    "    \"concert hall\", \"gallery\", \"museum\", \"exhibition\", \"installation\", \"performance space\"\n",
    "]\n",
    "\n",
    "# SCIENCE DOMAIN\n",
    "SCIENCE_KEYWORDS = [\n",
    "    # Core science terms\n",
    "    \"science\", \"scientific\", \"research\", \"researcher\", \"scientist\", \"study\", \"experiment\",\n",
    "    \"observation\", \"hypothesis\", \"theory\", \"law\", \"principle\", \"method\", \"methodology\",\n",
    "    \"analysis\", \"data\", \"evidence\", \"empirical\", \"peer-review\", \"publication\", \"journal\",\n",
    "    \n",
    "    # Scientific disciplines\n",
    "    \"physics\", \"chemistry\", \"biology\", \"astronomy\", \"geology\", \"ecology\", \"botany\", \"zoology\",\n",
    "    \"genetics\", \"molecular biology\", \"biochemistry\", \"neuroscience\", \"psychology\", \"sociology\",\n",
    "    \"anthropology\", \"archaeology\", \"mathematics\", \"statistics\", \"computer science\", \"engineering\",\n",
    "    \"environmental science\", \"materials science\", \"nanotechnology\", \"biotechnology\", \"quantum\",\n",
    "    \n",
    "    # Scientific concepts and phenomena\n",
    "    \"matter\", \"energy\", \"force\", \"particle\", \"atom\", \"molecule\", \"cell\", \"organism\", \"species\",\n",
    "    \"evolution\", \"natural selection\", \"adaptation\", \"mutation\", \"gene\", \"DNA\", \"RNA\", \"protein\",\n",
    "    \"enzyme\", \"reaction\", \"catalyst\", \"element\", \"compound\", \"solution\", \"acid\", \"base\",\n",
    "    \"gravity\", \"electromagnetic\", \"quantum\", \"relativity\", \"thermodynamics\", \"entropy\",\n",
    "    \n",
    "    # Research and methodology\n",
    "    \"research\", \"study\", \"experiment\", \"observation\", \"measurement\", \"data\", \"analysis\",\n",
    "    \"statistics\", \"variable\", \"control\", \"sample\", \"population\", \"correlation\", \"causation\",\n",
    "    \"hypothesis\", \"theory\", \"model\", \"simulation\", \"prediction\", \"validation\", \"replication\",\n",
    "    \"peer review\", \"publication\", \"journal\", \"conference\", \"symposium\", \"collaboration\",\n",
    "    \n",
    "    # Scientific equipment and technology\n",
    "    \"laboratory\", \"lab\", \"equipment\", \"instrument\", \"apparatus\", \"device\", \"tool\", \"technique\",\n",
    "    \"microscope\", \"telescope\", \"spectrometer\", \"chromatograph\", \"centrifuge\", \"electrode\",\n",
    "    \"sensor\", \"detector\", \"computer\", \"software\", \"algorithm\", \"database\", \"simulation\",\n",
    "    \"modeling\", \"imaging\", \"scanner\", \"MRI\", \"CT\", \"PET\", \"X-ray\", \"laser\", \"accelerator\",\n",
    "    \n",
    "    # Scientific institutions and roles\n",
    "    \"university\", \"college\", \"institute\", \"laboratory\", \"research center\", \"academy\",\n",
    "    \"department\", \"faculty\", \"professor\", \"researcher\", \"scientist\", \"postdoc\", \"graduate student\",\n",
    "    \"PhD\", \"doctorate\", \"master's\", \"bachelor's\", \"academic\", \"scholar\", \"fellow\", \"grant\",\n",
    "    \"funding\", \"NSF\", \"NIH\", \"NASA\", \"CERN\", \"Max Planck\", \"Royal Society\", \"Nobel Prize\",\n",
    "    \n",
    "    # Scientific progress and innovation\n",
    "    \"discovery\", \"breakthrough\", \"innovation\", \"advancement\", \"progress\", \"development\",\n",
    "    \"frontier\", \"cutting-edge\", \"state-of-the-art\", \"novel\", \"pioneering\", \"revolutionary\",\n",
    "    \"paradigm shift\", \"disruptive\", \"transformative\", \"interdisciplinary\", \"multidisciplinary\",\n",
    "    \"collaboration\", \"international\", \"global\", \"initiative\", \"project\", \"program\", \"mission\",\n",
    "    \n",
    "    # Scientific communication and literacy\n",
    "    \"publication\", \"paper\", \"article\", \"journal\", \"preprint\", \"peer review\", \"citation\",\n",
    "    \"reference\", \"literature\", \"bibliography\", \"abstract\", \"introduction\", \"methodology\",\n",
    "    \"results\", \"discussion\", \"conclusion\", \"figure\", \"table\", \"data\", \"visualization\",\n",
    "    \"communication\", \"dissemination\", \"outreach\", \"education\", \"literacy\", \"public understanding\",\n",
    "    \n",
    "    # Scientific ethics and issues\n",
    "    \"ethics\", \"ethical\", \"integrity\", \"misconduct\", \"plagiarism\", \"fabrication\", \"falsification\",\n",
    "    \"conflict of interest\", \"disclosure\", \"transparency\", \"reproducibility\", \"replication crisis\",\n",
    "    \"open science\", \"open access\", \"open data\", \"intellectual property\", \"patent\", \"copyright\",\n",
    "    \"regulation\", \"policy\", \"funding\", \"grant\", \"commercialization\", \"industry partnership\"\n",
    "]\n",
    "\n",
    "# ENVIRONMENT DOMAIN\n",
    "ENVIRONMENT_KEYWORDS = [\n",
    "    # Core environmental terms\n",
    "    \"environment\", \"environmental\", \"ecology\", \"ecosystem\", \"habitat\", \"biodiversity\",\n",
    "    \"conservation\", \"preservation\", \"sustainability\", \"sustainable\", \"green\", \"eco-friendly\",\n",
    "    \n",
    "    # Climate and atmosphere\n",
    "    \"climate\", \"climate change\", \"global warming\", \"greenhouse gas\", \"carbon\", \"carbon dioxide\",\n",
    "    \"methane\", \"emission\", \"carbon footprint\", \"carbon neutral\", \"carbon offset\", \"atmosphere\",\n",
    "    \"ozone\", \"ozone layer\", \"air quality\", \"air pollution\", \"smog\", \"particulate matter\",\n",
    "    \n",
    "    # Water systems\n",
    "    \"water\", \"freshwater\", \"groundwater\", \"aquifer\", \"watershed\", \"river\", \"lake\", \"ocean\",\n",
    "    \"sea\", \"marine\", \"coastal\", \"wetland\", \"water pollution\", \"water quality\", \"water scarcity\",\n",
    "    \"drought\", \"flood\", \"stormwater\", \"runoff\", \"wastewater\", \"water conservation\",\n",
    "    \n",
    "    # Land and soil\n",
    "    \"land\", \"soil\", \"forest\", \"deforestation\", \"reforestation\", \"afforestation\", \"desertification\",\n",
    "    \"erosion\", \"land degradation\", \"land use\", \"land management\", \"soil health\", \"soil quality\",\n",
    "    \"soil conservation\", \"soil pollution\", \"contamination\", \"brownfield\", \"greenfield\",\n",
    "    \n",
    "    # Biodiversity and ecosystems\n",
    "    \"biodiversity\", \"species\", \"endangered species\", \"extinction\", \"wildlife\", \"flora\", \"fauna\",\n",
    "    \"ecosystem\", \"ecosystem service\", \"habitat\", \"habitat loss\", \"habitat fragmentation\",\n",
    "    \"protected area\", \"national park\", \"wildlife refuge\", \"conservation\", \"preservation\",\n",
    "    \n",
    "    # Pollution and waste\n",
    "    \"pollution\", \"pollutant\", \"contaminant\", \"waste\", \"solid waste\", \"hazardous waste\",\n",
    "    \"e-waste\", \"plastic\", \"plastic pollution\", \"microplastic\", \"recycling\", \"composting\",\n",
    "    \"landfill\", \"incineration\", \"zero waste\", \"circular economy\", \"waste management\",\n",
    "    \n",
    "    # Energy and resources\n",
    "    \"energy\", \"renewable energy\", \"solar\", \"wind\", \"hydroelectric\", \"geothermal\", \"biomass\",\n",
    "    \"fossil fuel\", \"coal\", \"oil\", \"natural gas\", \"nuclear\", \"energy efficiency\", \"resource\",\n",
    "    \"natural resource\", \"resource depletion\", \"resource conservation\", \"sustainable resource\",\n",
    "    \n",
    "    # Environmental policy and management\n",
    "    \"environmental policy\", \"environmental regulation\", \"environmental law\", \"environmental protection\",\n",
    "    \"environmental impact\", \"environmental assessment\", \"environmental monitoring\", \"environmental standard\",\n",
    "    \"epa\", \"environmental protection agency\", \"environmental justice\", \"environmental ethics\",    # Sustainability practices (continued)\n",
    "    \"sustainability\", \"sustainable development\", \"sustainable living\", \"sustainable agriculture\",\n",
    "    \"sustainable forestry\", \"sustainable fishing\", \"sustainable transportation\", \"green building\",\n",
    "    \"leed\", \"eco-label\", \"organic\", \"fair trade\", \"corporate sustainability\", \"esg\",\n",
    "    \n",
    "    # Environmental movements and concepts\n",
    "    \"environmentalism\", \"environmental movement\", \"environmental activist\", \"environmental advocacy\",\n",
    "    \"green movement\", \"conservation movement\", \"deep ecology\", \"ecofeminism\", \"environmental justice\",\n",
    "    \"climate justice\", \"climate action\", \"climate strike\", \"paris agreement\", \"kyoto protocol\"\n",
    "]\n",
    "\n",
    "# WORLD NEWS DOMAIN\n",
    "WORLD_KEYWORDS = [\n",
    "    # Core international terms\n",
    "    \"international\", \"global\", \"world\", \"foreign\", \"overseas\", \"abroad\", \"multinational\",\n",
    "    \"transnational\", \"cross-border\", \"diplomatic\", \"bilateral\", \"multilateral\", \"geopolitical\",\n",
    "    \n",
    "    # International organizations\n",
    "    \"United Nations\", \"UN\", \"Security Council\", \"General Assembly\", \"NATO\", \"European Union\", \"EU\",\n",
    "    \"World Bank\", \"International Monetary Fund\", \"IMF\", \"World Trade Organization\", \"WTO\",\n",
    "    \"World Health Organization\", \"WHO\", \"UNICEF\", \"UNESCO\", \"ASEAN\", \"African Union\", \"AU\",\n",
    "    \"Organization of American States\", \"OAS\", \"Arab League\", \"Commonwealth\", \"G7\", \"G20\",\n",
    "    \n",
    "    # International relations\n",
    "    \"diplomacy\", \"diplomatic\", \"foreign policy\", \"international relations\", \"geopolitics\",\n",
    "    \"alliance\", \"treaty\", \"agreement\", \"accord\", \"pact\", \"convention\", \"protocol\", \"summit\",\n",
    "    \"negotiation\", \"mediation\", \"arbitration\", \"cooperation\", \"collaboration\", \"partnership\",\n",
    "    \"tension\", \"conflict\", \"dispute\", \"crisis\", \"war\", \"peace\", \"peacekeeping\", \"peacebuilding\",\n",
    "    \n",
    "    # Global issues\n",
    "    \"global issue\", \"global challenge\", \"global crisis\", \"climate change\", \"global warming\",\n",
    "    \"pandemic\", \"epidemic\", \"disease\", \"poverty\", \"inequality\", \"hunger\", \"food security\",\n",
    "    \"water scarcity\", \"energy crisis\", \"refugee crisis\", \"migration\", \"immigration\", \"human rights\",\n",
    "    \"terrorism\", \"extremism\", \"radicalization\", \"nuclear proliferation\", \"arms control\",\n",
    "    \n",
    "    # International trade and economics\n",
    "    \"global economy\", \"world economy\", \"international trade\", \"global trade\", \"import\", \"export\",\n",
    "    \"tariff\", \"trade barrier\", \"free trade\", \"protectionism\", \"trade agreement\", \"trade deal\",\n",
    "    \"trade war\", \"sanction\", \"embargo\", \"global market\", \"foreign investment\", \"multinational corporation\",\n",
    "    \"offshore\", \"outsourcing\", \"supply chain\", \"global supply chain\", \"currency\", \"exchange rate\",\n",
    "    \n",
    "    # Regions and continents\n",
    "    \"Asia\", \"Asian\", \"East Asia\", \"Southeast Asia\", \"South Asia\", \"Central Asia\", \"Middle East\",\n",
    "    \"Africa\", \"African\", \"North Africa\", \"Sub-Saharan Africa\", \"Europe\", \"European\", \"Eastern Europe\",\n",
    "    \"Western Europe\", \"North America\", \"South America\", \"Latin America\", \"Caribbean\", \"Oceania\",\n",
    "    \"Pacific\", \"Arctic\", \"Antarctic\", \"Global South\", \"Global North\", \"developing world\", \"developed world\",\n",
    "    \n",
    "    # Countries and nationalities\n",
    "    \"country\", \"nation\", \"state\", \"government\", \"regime\", \"administration\", \"leadership\",\n",
    "    \"president\", \"prime minister\", \"chancellor\", \"monarch\", \"royal\", \"parliament\", \"congress\",\n",
    "    \"election\", \"democracy\", \"autocracy\", \"dictatorship\", \"sovereignty\", \"border\", \"territory\",\n",
    "    \"citizenship\", \"nationality\", \"passport\", \"visa\", \"embassy\", \"consulate\", \"diplomatic mission\",\n",
    "    \n",
    "    # International security\n",
    "    \"security\", \"defense\", \"military\", \"armed forces\", \"army\", \"navy\", \"air force\", \"troops\",\n",
    "    \"soldier\", \"weapon\", \"arms\", \"disarmament\", \"nuclear\", \"missile\", \"drone\", \"terrorism\",\n",
    "    \"counterterrorism\", \"insurgency\", \"civil war\", \"conflict\", \"peacekeeping\", \"NATO\",\n",
    "    \"alliance\", \"coalition\", \"intelligence\", \"surveillance\", \"cyber security\", \"cyber warfare\",\n",
    "    \n",
    "    # International development\n",
    "    \"development\", \"developing country\", \"developed country\", \"emerging economy\", \"BRICS\",\n",
    "    \"aid\", \"foreign aid\", \"humanitarian aid\", \"development assistance\", \"ODA\", \"sustainable development\",\n",
    "    \"millennium development goals\", \"MDGs\", \"sustainable development goals\", \"SDGs\", \"poverty reduction\",\n",
    "    \"capacity building\", \"infrastructure\", \"education\", \"healthcare\", \"economic growth\", \"industrialization\"\n",
    "]\n",
    "\n",
    "# EDUCATION DOMAIN\n",
    "EDUCATION_KEYWORDS = [\n",
    "    # Core education terms\n",
    "    \"education\", \"learning\", \"teaching\", \"school\", \"student\", \"teacher\", \"classroom\",\n",
    "    \"curriculum\", \"instruction\", \"pedagogy\", \"academic\", \"educational\", \"educator\",\n",
    "    \n",
    "    # Educational institutions\n",
    "    \"school\", \"university\", \"college\", \"academy\", \"institute\", \"campus\", \"kindergarten\",\n",
    "    \"preschool\", \"elementary school\", \"primary school\", \"middle school\", \"high school\",\n",
    "    \"secondary school\", \"community college\", \"vocational school\", \"trade school\",\n",
    "    \"boarding school\", \"charter school\", \"private school\", \"public school\", \"homeschool\",\n",
    "    \n",
    "    # Academic levels and programs\n",
    "    \"degree\", \"bachelor\", \"master\", \"doctorate\", \"phd\", \"undergraduate\", \"graduate\",\n",
    "    \"postgraduate\", \"diploma\", \"certificate\", \"major\", \"minor\", \"concentration\",\n",
    "    \"specialization\", \"program\", \"course\", \"class\", \"seminar\", \"lecture\", \"workshop\",\n",
    "    \n",
    "    # Educational roles\n",
    "    \"student\", \"pupil\", \"learner\", \"teacher\", \"professor\", \"instructor\", \"educator\",\n",
    "    \"faculty\", \"staff\", \"administrator\", \"principal\", \"headmaster\", \"dean\", \"provost\",\n",
    "    \"chancellor\", \"superintendent\", \"school board\", \"trustee\", \"counselor\", \"advisor\",\n",
    "    \n",
    "    # Educational activities and methods\n",
    "    \"teaching\", \"learning\", \"studying\", \"research\", \"assignment\", \"homework\", \"project\",\n",
    "    \"exam\", \"test\", \"quiz\", \"assessment\", \"evaluation\", \"grading\", \"feedback\", \"lecture\",\n",
    "    \"discussion\", \"group work\", \"collaboration\", \"presentation\", \"demonstration\",\n",
    "    \n",
    "    # Educational approaches and philosophies\n",
    "    \"pedagogy\", \"curriculum\", \"instruction\", \"methodology\", \"approach\", \"philosophy\",\n",
    "    \"theory\", \"framework\", \"model\", \"standard\", \"benchmark\", \"objective\", \"outcome\",\n",
    "    \"competency\", \"skill\", \"knowledge\", \"understanding\", \"critical thinking\", \"problem-solving\",\n",
    "    \n",
    "    # Educational technology\n",
    "    \"educational technology\", \"edtech\", \"e-learning\", \"online learning\", \"distance learning\",\n",
    "    \"remote learning\", \"virtual classroom\", \"learning management system\", \"digital learning\",\n",
    "    \"blended learning\", \"hybrid learning\", \"mooc\", \"adaptive learning\", \"personalized learning\",\n",
    "    \n",
    "    # Educational policy and administration\n",
    "    \"education policy\", \"education reform\", \"education law\", \"education funding\", \"education budget\",\n",
    "    \"department of education\", \"ministry of education\", \"school district\", \"accreditation\",\n",
    "    \"standardized testing\", \"common core\", \"no child left behind\", \"every student succeeds act\",\n",
    "    \n",
    "    # Educational issues and concepts\n",
    "    \"access to education\", \"educational equity\", \"educational equality\", \"educational opportunity\",\n",
    "    \"achievement gap\", \"digital divide\", \"literacy\", \"numeracy\", \"stem education\", \"arts education\",\n",
    "    \"special education\", \"inclusive education\", \"multicultural education\", \"bilingual education\",\n",
    "    \n",
    "    # Lifelong learning\n",
    "    \"lifelong learning\", \"continuing education\", \"adult education\", \"professional development\",\n",
    "    \"training\", \"upskilling\", \"reskilling\", \"certification\", \"recertification\", \"credential\",\n",
    "    \"competency-based education\", \"experiential learning\", \"self-directed learning\"\n",
    "]\n",
    "\n",
    "# FOOD DOMAIN\n",
    "FOOD_KEYWORDS = [\n",
    "    # Core food terms\n",
    "    \"food\", \"cuisine\", \"dish\", \"meal\", \"recipe\", \"ingredient\", \"cooking\", \"baking\",\n",
    "    \"chef\", \"restaurant\", \"dining\", \"menu\", \"taste\", \"flavor\", \"delicious\", \"culinary\",\n",
    "    \n",
    "    # Food types\n",
    "    \"vegetarian\", \"vegan\", \"organic\", \"gluten-free\", \"dairy-free\", \"pescatarian\",\n",
    "    \"keto\", \"paleo\", \"mediterranean\", \"plant-based\", \"farm-to-table\", \"locally-sourced\",\n",
    "    \n",
    "    # Cooking methods\n",
    "    \"roast\", \"bake\", \"fry\", \"grill\", \"sauté\", \"boil\", \"steam\", \"simmer\", \"braise\",\n",
    "    \"marinate\", \"ferment\", \"pickle\", \"smoke\", \"sous-vide\", \"pressure cook\", \"slow cook\",\n",
    "    \n",
    "    # Meal types\n",
    "    \"breakfast\", \"lunch\", \"dinner\", \"brunch\", \"appetizer\", \"entree\", \"dessert\",\n",
    "    \"snack\", \"side dish\", \"main course\", \"buffet\", \"tasting menu\", \"prix fixe\",\n",
    "    \n",
    "    # Food categories\n",
    "    \"fruit\", \"vegetable\", \"meat\", \"poultry\", \"seafood\", \"grain\", \"legume\", \"dairy\",\n",
    "    \"bread\", \"pasta\", \"rice\", \"noodle\", \"soup\", \"salad\", \"sandwich\", \"stew\", \"curry\",\n",
    "    \n",
    "    # Specific foods\n",
    "    \"beef\", \"chicken\", \"pork\", \"lamb\", \"fish\", \"shrimp\", \"tofu\", \"cheese\", \"yogurt\",\n",
    "    \"butter\", \"egg\", \"milk\", \"cream\", \"chocolate\", \"sugar\", \"flour\", \"olive oil\",\n",
    "    \n",
    "    # Cuisines\n",
    "    \"italian\", \"french\", \"chinese\", \"japanese\", \"indian\", \"mexican\", \"thai\", \"korean\",\n",
    "    \"vietnamese\", \"greek\", \"spanish\", \"middle eastern\", \"mediterranean\", \"cajun\", \"creole\",\n",
    "    \"southern\", \"soul food\", \"bbq\", \"barbecue\", \"fusion\",\n",
    "    \n",
    "    # Restaurant terms\n",
    "    \"michelin\", \"bistro\", \"cafe\", \"diner\", \"eatery\", \"food truck\", \"pop-up\", \"takeout\",\n",
    "    \"delivery\", \"catering\", \"fine dining\", \"casual dining\", \"fast food\", \"fast casual\",\n",
    "    \n",
    "    # Food industry\n",
    "    \"food industry\", \"food supply\", \"food production\", \"food processing\", \"food safety\",\n",
    "    \"food security\", \"food waste\", \"sustainable food\", \"organic farming\", \"food system\",\n",
    "    \n",
    "    # Food trends\n",
    "    \"food trend\", \"superfood\", \"artisanal\", \"craft\", \"gourmet\", \"foodie\", \"food blogger\",\n",
    "    \"food photography\", \"food styling\", \"food review\", \"food critic\", \"food award\"\n",
    "]\n",
    "\n",
    "# TRAVEL DOMAIN\n",
    "TRAVEL_KEYWORDS = [\n",
    "    # Core travel terms\n",
    "    \"travel\", \"tourism\", \"tourist\", \"trip\", \"journey\", \"vacation\", \"holiday\", \"getaway\",\n",
    "    \"tour\", \"sightseeing\", \"excursion\", \"expedition\", \"adventure\", \"exploration\", \"wanderlust\",\n",
    "    \n",
    "    # Transportation\n",
    "    \"transportation\", \"transit\", \"travel\", \"commute\", \"flight\", \"airplane\", \"aircraft\",\n",
    "    \"airport\", \"airline\", \"train\", \"railway\", \"railroad\", \"station\", \"bus\", \"coach\",\n",
    "    \"subway\", \"metro\", \"underground\", \"cruise\", \"ship\", \"boat\", \"ferry\", \"car\", \"rental car\",\n",
    "    \"road trip\", \"drive\", \"highway\", \"freeway\", \"route\", \"itinerary\", \"navigation\", \"map\",\n",
    "    \n",
    "    # Accommodation\n",
    "    \"accommodation\", \"lodging\", \"stay\", \"hotel\", \"motel\", \"inn\", \"resort\", \"spa\",\n",
    "    \"bed and breakfast\", \"B&B\", \"guesthouse\", \"hostel\", \"vacation rental\", \"Airbnb\",\n",
    "    \"camping\", \"glamping\", \"cabin\", \"cottage\", \"villa\", \"apartment\", \"condo\", \"suite\",\n",
    "    \"room\", \"booking\", \"reservation\", \"check-in\", \"check-out\", \"concierge\", \"amenity\",\n",
    "    \n",
    "    # Destinations\n",
    "    \"destination\", \"location\", \"place\", \"spot\", \"site\", \"attraction\", \"landmark\",\n",
    "    \"monument\", \"museum\", \"gallery\", \"park\", \"garden\", \"beach\", \"coast\", \"mountain\",\n",
    "    \"hill\", \"valley\", \"forest\", \"jungle\", \"desert\", \"island\", \"lake\", \"river\", \"waterfall\",\n",
    "    \"hot spring\", \"geyser\", \"cave\", \"canyon\", \"volcano\", \"glacier\", \"wildlife\", \"safari\",\n",
    "    \n",
    "    # Travel activities\n",
    "    \"activity\", \"experience\", \"adventure\", \"tour\", \"guided tour\", \"self-guided\",\n",
    "    \"sightseeing\", \"excursion\", \"day trip\", \"hiking\", \"trekking\", \"climbing\", \"camping\",\n",
    "    \"backpacking\", \"cycling\", \"biking\", \"kayaking\", \"canoeing\", \"rafting\", \"sailing\",\n",
    "    \"cruising\", \"diving\", \"snorkeling\", \"swimming\", \"surfing\", \"skiing\", \"snowboarding\",\n",
    "    \n",
    "    # Travel planning and services\n",
    "    \"planning\", \"itinerary\", \"schedule\", \"booking\", \"reservation\", \"travel agent\",\n",
    "    \"tour operator\", \"guide\", \"travel guide\", \"guidebook\", \"map\", \"navigation\", \"GPS\",\n",
    "    \"travel insurance\", \"visa\", \"passport\", \"customs\", \"immigration\", \"border control\",\n",
    "    \"exchange rate\", \"currency\", \"money\", \"budget\", \"expense\", \"cost\", \"fee\", \"discount\",\n",
    "    \n",
    "    # Travel experiences and concepts\n",
    "    \"experience\", \"memory\", \"photo\", \"photography\", \"souvenir\", \"memento\", \"culture\",\n",
    "    \"cultural\", \"local\", \"authentic\", \"traditional\", \"heritage\", \"history\", \"historical\",\n",
    "    \"architecture\", \"art\", \"cuisine\", \"food\", \"dining\", \"restaurant\", \"cafe\", \"street food\",\n",
    "    \"market\", \"shopping\", \"craft\", \"artisan\", \"festival\", \"event\", \"celebration\", \"holiday\",\n",
    "    \n",
    "    # Travel industry\n",
    "    \"tourism industry\", \"hospitality\", \"travel business\", \"travel company\", \"travel agency\",\n",
    "    \"tour company\", \"airline\", \"hotel chain\", \"resort group\", \"cruise line\", \"transportation\",\n",
    "    \"destination management\", \"tourism board\", \"convention bureau\", \"visitor center\",\n",
    "    \"sustainable tourism\", \"ecotourism\", \"responsible travel\", \"overtourism\", \"tourism impact\"\n",
    "]\n",
    "\n",
    "# AUTOMOTIVE DOMAIN\n",
    "AUTOMOTIVE_KEYWORDS = [\n",
    "    # Core automotive terms\n",
    "    \"automotive\", \"automobile\", \"car\", \"vehicle\", \"motor vehicle\", \"auto\", \"motor\",\n",
    "    \"driving\", \"driver\", \"ride\", \"transportation\", \"mobility\", \"commute\", \"travel\",\n",
    "    \n",
    "    # Vehicle types\n",
    "    \"car\", \"sedan\", \"coupe\", \"hatchback\", \"wagon\", \"convertible\", \"SUV\", \"crossover\",\n",
    "    \"truck\", \"pickup\", \"van\", \"minivan\", \"bus\", \"motorcycle\", \"scooter\", \"moped\",\n",
    "    \"RV\", \"recreational vehicle\", \"camper\", \"trailer\", \"semi-truck\", \"tractor-trailer\",\n",
    "    \"commercial vehicle\", \"fleet vehicle\", \"luxury vehicle\", \"sports car\", \"supercar\",\n",
    "    \n",
    "    # Vehicle components and systems\n",
    "    \"engine\", \"motor\", \"powertrain\", \"transmission\", \"drivetrain\", \"chassis\", \"frame\",\n",
    "    \"body\", \"interior\", \"exterior\", \"wheel\", \"tire\", \"brake\", \"suspension\", \"steering\",\n",
    "    \"exhaust\", \"muffler\", \"catalytic converter\", \"battery\", \"alternator\", \"starter\",\n",
    "    \"fuel system\", \"cooling system\", \"electrical system\", \"ignition\", \"spark plug\",\n",
    "    \n",
    "    # Vehicle performance and specifications\n",
    "    \"performance\", \"speed\", \"acceleration\", \"horsepower\", \"torque\", \"RPM\", \"MPG\",\n",
    "    \"fuel economy\", \"fuel efficiency\", \"range\", \"top speed\", \"0-60\", \"quarter mile\",\n",
    "    \"handling\", \"braking\", \"cornering\", \"aerodynamics\", \"drag coefficient\", \"downforce\",\n",
    "    \"weight\", \"curb weight\", \"payload\", \"towing capacity\", \"ground clearance\", \"dimensions\",\n",
    "    \n",
    "    # Automotive technology\n",
    "    \"technology\", \"tech\", \"innovation\", \"feature\", \"system\", \"infotainment\", \"navigation\",\n",
    "    \"GPS\", \"Bluetooth\", \"Wi-Fi\", \"USB\", \"touchscreen\", \"display\", \"dashboard\", \"HUD\",\n",
    "    \"head-up display\", \"driver assistance\", \"ADAS\", \"safety system\", \"airbag\", \"seatbelt\",\n",
    "    \"cruise control\", \"adaptive cruise\", \"lane assist\", \"blind spot\", \"parking assist\",\n",
    "    \n",
    "        # Alternative powertrains (continued)\n",
    "    \"electric vehicle\", \"EV\", \"battery electric\", \"hybrid\", \"plug-in hybrid\", \"PHEV\",\n",
    "    \"hydrogen\", \"fuel cell\", \"FCEV\", \"alternative fuel\", \"biofuel\", \"ethanol\",\n",
    "    \"biodiesel\", \"natural gas\", \"CNG\", \"LPG\", \"propane\", \"clean diesel\", \"zero emission\",\n",
    "    \"low emission\", \"carbon neutral\", \"charging\", \"charging station\", \"supercharger\",\n",
    "    \n",
    "    # Automotive industry\n",
    "    \"industry\", \"manufacturer\", \"automaker\", \"brand\", \"make\", \"model\", \"production\",\n",
    "    \"assembly\", \"factory\", \"plant\", \"supply chain\", \"parts supplier\", \"OEM\", \"aftermarket\",\n",
    "    \"dealer\", \"dealership\", \"showroom\", \"sales\", \"leasing\", \"financing\", \"warranty\",\n",
    "    \"recall\", \"service\", \"maintenance\", \"repair\", \"mechanic\", \"technician\", \"diagnostics\",\n",
    "    \n",
    "    # Automotive culture and lifestyle\n",
    "    \"car culture\", \"automotive enthusiast\", \"gearhead\", \"petrolhead\", \"car lover\",\n",
    "    \"car collector\", \"classic car\", \"vintage car\", \"antique car\", \"hot rod\", \"custom car\",\n",
    "    \"modified car\", \"tuner car\", \"restoration\", \"car show\", \"auto show\", \"concours\",\n",
    "    \"racing\", \"motorsport\", \"F1\", \"NASCAR\", \"rally\", \"drift\", \"drag racing\", \"off-road\"\n",
    "]\n",
    "\n",
    "# REAL ESTATE DOMAIN\n",
    "REAL_ESTATE_KEYWORDS = [\n",
    "    # Core real estate terms\n",
    "    \"real estate\", \"property\", \"home\", \"house\", \"housing\", \"residential\", \"commercial\",\n",
    "    \"industrial\", \"land\", \"lot\", \"parcel\", \"building\", \"structure\", \"development\",\n",
    "    \n",
    "    # Property types\n",
    "    \"house\", \"single-family home\", \"detached house\", \"townhouse\", \"townhome\", \"duplex\",\n",
    "    \"triplex\", \"fourplex\", \"multifamily\", \"apartment\", \"condo\", \"condominium\", \"co-op\",\n",
    "    \"cooperative\", \"loft\", \"studio\", \"mobile home\", \"manufactured home\", \"tiny house\",\n",
    "    \"mansion\", \"estate\", \"villa\", \"cottage\", \"cabin\", \"bungalow\", \"ranch\", \"colonial\",\n",
    "    \n",
    "    # Commercial real estate\n",
    "    \"commercial property\", \"office\", \"office building\", \"office space\", \"retail\",\n",
    "    \"retail space\", \"store\", \"shop\", \"mall\", \"shopping center\", \"industrial\", \"warehouse\",\n",
    "    \"distribution center\", \"manufacturing\", \"factory\", \"hotel\", \"motel\", \"resort\",\n",
    "    \"restaurant\", \"gas station\", \"self-storage\", \"multi-use\", \"mixed-use\",\n",
    "    \n",
    "    # Property features and specifications\n",
    "    \"square footage\", \"square feet\", \"sqft\", \"acreage\", \"acre\", \"lot size\", \"floor plan\",\n",
    "    \"layout\", \"bedroom\", \"bathroom\", \"kitchen\", \"living room\", \"dining room\", \"family room\",\n",
    "    \"basement\", \"attic\", \"garage\", \"carport\", \"driveway\", \"yard\", \"garden\", \"patio\",\n",
    "    \"deck\", \"porch\", \"balcony\", \"pool\", \"spa\", \"fireplace\", \"hardwood floors\", \"carpet\",\n",
    "    \n",
    "    # Real estate transactions\n",
    "    \"buy\", \"purchase\", \"sell\", \"sale\", \"listing\", \"offer\", \"bid\", \"negotiation\",\n",
    "    \"contract\", \"agreement\", \"closing\", \"escrow\", \"title\", \"deed\", \"settlement\",\n",
    "    \"transaction\", \"commission\", \"fee\", \"price\", \"asking price\", \"list price\", \"sale price\",\n",
    "    \"market value\", \"appraisal\", \"inspection\", \"contingency\", \"due diligence\",\n",
    "    \n",
    "    # Real estate financing\n",
    "    \"mortgage\", \"loan\", \"financing\", \"lender\", \"bank\", \"credit union\", \"interest rate\",\n",
    "    \"APR\", \"term\", \"amortization\", \"down payment\", \"earnest money\", \"deposit\",\n",
    "    \"principal\", \"interest\", \"taxes\", \"insurance\", \"PITI\", \"escrow account\",\n",
    "    \"pre-approval\", \"pre-qualification\", \"underwriting\", \"closing costs\", \"points\",\n",
    "    \n",
    "    # Real estate professionals\n",
    "    \"real estate agent\", \"realtor\", \"broker\", \"listing agent\", \"buyer's agent\",\n",
    "    \"real estate attorney\", \"title company\", \"escrow officer\", \"appraiser\", \"inspector\",\n",
    "    \"mortgage broker\", \"loan officer\", \"property manager\", \"leasing agent\", \"developer\",\n",
    "    \"builder\", \"contractor\", \"architect\", \"interior designer\", \"stager\", \"photographer\",\n",
    "    \n",
    "    # Real estate market\n",
    "    \"market\", \"housing market\", \"buyer's market\", \"seller's market\", \"inventory\",\n",
    "    \"supply\", \"demand\", \"competition\", \"hot market\", \"cool market\", \"appreciation\",\n",
    "    \"depreciation\", \"equity\", \"investment\", \"return on investment\", \"ROI\", \"cash flow\",\n",
    "    \"rental income\", \"vacancy rate\", \"cap rate\", \"capitalization rate\", \"flip\", \"flipping\",\n",
    "    \n",
    "    # Location and community\n",
    "    \"location\", \"neighborhood\", \"community\", \"subdivision\", \"development\", \"master-planned\",\n",
    "    \"gated community\", \"homeowners association\", \"HOA\", \"condo association\", \"amenities\",\n",
    "    \"school district\", \"zoning\", \"urban\", \"suburban\", \"rural\", \"downtown\", \"uptown\",\n",
    "    \"city\", \"town\", \"village\", \"municipality\", \"county\", \"metro area\", \"region\"\n",
    "]\n",
    "\n",
    "# CYBERSECURITY DOMAIN\n",
    "CYBERSECURITY_KEYWORDS = [\n",
    "    # Core cybersecurity terms\n",
    "    \"cybersecurity\", \"security\", \"cyber\", \"information security\", \"infosec\", \"computer security\",\n",
    "    \"network security\", \"data security\", \"IT security\", \"digital security\", \"protection\",\n",
    "    \n",
    "    # Security threats and attacks\n",
    "    \"threat\", \"attack\", \"breach\", \"hack\", \"compromise\", \"exploit\", \"vulnerability\",\n",
    "    \"malware\", \"virus\", \"worm\", \"trojan\", \"ransomware\", \"spyware\", \"adware\", \"rootkit\",\n",
    "    \"keylogger\", \"botnet\", \"DDoS\", \"denial of service\", \"phishing\", \"spear phishing\",\n",
    "    \"social engineering\", \"identity theft\", \"data breach\", \"data leak\", \"insider threat\",\n",
    "    \n",
    "    # Security measures and practices\n",
    "    \"defense\", \"protection\", \"safeguard\", \"countermeasure\", \"mitigation\", \"prevention\",\n",
    "    \"detection\", \"response\", \"recovery\", \"security control\", \"security measure\",\n",
    "    \"security policy\", \"security procedure\", \"security standard\", \"security framework\",\n",
    "    \"security best practice\", \"security awareness\", \"security training\", \"security culture\",\n",
    "    \n",
    "    # Security technologies and tools\n",
    "    \"firewall\", \"antivirus\", \"anti-malware\", \"IDS\", \"intrusion detection\", \"IPS\",\n",
    "    \"intrusion prevention\", \"SIEM\", \"security information and event management\",\n",
    "    \"EDR\", \"endpoint detection and response\", \"XDR\", \"extended detection and response\",\n",
    "    \"WAF\", \"web application firewall\", \"proxy\", \"VPN\", \"virtual private network\",\n",
    "    \n",
    "    # Authentication and access control\n",
    "    \"authentication\", \"authorization\", \"access control\", \"identity management\", \"IAM\",\n",
    "    \"single sign-on\", \"SSO\", \"multi-factor authentication\", \"MFA\", \"two-factor authentication\",\n",
    "    \"2FA\", \"password\", \"passphrase\", \"biometric\", \"fingerprint\", \"facial recognition\",\n",
    "    \"voice recognition\", \"retina scan\", \"iris scan\", \"token\", \"smart card\", \"certificate\",\n",
    "    \n",
    "    # Encryption and cryptography\n",
    "    \"encryption\", \"cryptography\", \"crypto\", \"cipher\", \"ciphertext\", \"plaintext\",\n",
    "    \"key\", \"public key\", \"private key\", \"symmetric encryption\", \"asymmetric encryption\",\n",
    "    \"PKI\", \"public key infrastructure\", \"digital signature\", \"hash\", \"hashing\",\n",
    "    \"SHA\", \"MD5\", \"AES\", \"RSA\", \"TLS\", \"SSL\", \"HTTPS\", \"end-to-end encryption\", \"E2EE\",\n",
    "    \n",
    "    # Network security\n",
    "    \"network security\", \"perimeter security\", \"DMZ\", \"demilitarized zone\", \"segmentation\",\n",
    "    \"VLAN\", \"virtual LAN\", \"router\", \"switch\", \"gateway\", \"packet filtering\", \"deep packet inspection\",\n",
    "    \"DPI\", \"network monitoring\", \"traffic analysis\", \"port scanning\", \"vulnerability scanning\",\n",
    "    \"penetration testing\", \"pen testing\", \"red team\", \"blue team\", \"purple team\",\n",
    "    \n",
    "    # Security operations\n",
    "    \"security operations\", \"SecOps\", \"SOC\", \"security operations center\", \"incident\",\n",
    "    \"incident response\", \"IR\", \"incident handling\", \"forensics\", \"digital forensics\",\n",
    "    \"DFIR\", \"threat intelligence\", \"threat hunting\", \"security monitoring\", \"log analysis\",\n",
    "    \"CSIRT\", \"computer security incident response team\", \"security analyst\", \"security engineer\",\n",
    "    \n",
    "    # Security governance and compliance\n",
    "    \"security governance\", \"security management\", \"security program\", \"security strategy\",\n",
    "    \"risk management\", \"risk assessment\", \"risk analysis\", \"vulnerability management\",\n",
    "    \"compliance\", \"regulatory compliance\", \"audit\", \"security audit\", \"security assessment\",\n",
    "    \"security testing\", \"security certification\", \"security accreditation\", \"security standard\",\n",
    "    \n",
    "    # Security frameworks and standards\n",
    "    \"security framework\", \"security standard\", \"NIST\", \"ISO\", \"ISO 27001\", \"ISO 27002\",\n",
    "    \"CIS\", \"CIS Controls\", \"COBIT\", \"ITIL\", \"PCI DSS\", \"HIPAA\", \"GDPR\", \"CCPA\",\n",
    "    \"SOX\", \"FISMA\", \"FedRAMP\", \"CMMC\", \"zero trust\", \"defense in depth\", \"least privilege\"\n",
    "]\n",
    "\n",
    "# ARTIFICIAL INTELLIGENCE DOMAIN\n",
    "AI_KEYWORDS = [\n",
    "    # Core AI terms\n",
    "    \"artificial intelligence\", \"AI\", \"machine intelligence\", \"computational intelligence\",\n",
    "    \"intelligent system\", \"cognitive computing\", \"cognitive system\", \"smart system\",\n",
    "    \n",
    "    # Machine learning\n",
    "    \"machine learning\", \"ML\", \"deep learning\", \"DL\", \"neural network\", \"neural net\",\n",
    "    \"artificial neural network\", \"ANN\", \"deep neural network\", \"DNN\", \"convolutional neural network\",\n",
    "    \"CNN\", \"recurrent neural network\", \"RNN\", \"long short-term memory\", \"LSTM\",\n",
    "    \"transformer\", \"attention mechanism\", \"self-attention\", \"supervised learning\",\n",
    "    \"unsupervised learning\", \"semi-supervised learning\", \"reinforcement learning\", \"RL\",\n",
    "    \n",
    "    # AI techniques and approaches\n",
    "    \"algorithm\", \"model\", \"training\", \"inference\", \"prediction\", \"classification\",\n",
    "    \"regression\", \"clustering\", \"dimensionality reduction\", \"feature extraction\",\n",
    "    \"feature engineering\", \"feature selection\", \"ensemble learning\", \"boosting\",\n",
    "    \"bagging\", \"random forest\", \"decision tree\", \"support vector machine\", \"SVM\",\n",
    "    \"Bayesian\", \"probabilistic\", \"genetic algorithm\", \"evolutionary algorithm\",\n",
    "    \n",
    "    # AI applications\n",
    "    \"computer vision\", \"CV\", \"image recognition\", \"image classification\", \"object detection\",\n",
    "    \"facial recognition\", \"natural language processing\", \"NLP\", \"natural language understanding\",\n",
    "    \"NLU\", \"natural language generation\", \"NLG\", \"speech recognition\", \"speech synthesis\",\n",
    "    \"text-to-speech\", \"TTS\", \"machine translation\", \"sentiment analysis\", \"chatbot\",\n",
    "    \"conversational AI\", \"recommendation system\", \"recommender system\", \"personalization\",\n",
    "    \n",
    "    # AI systems and platforms\n",
    "    \"AI system\", \"AI platform\", \"AI framework\", \"AI tool\", \"AI service\", \"AI solution\",\n",
    "    \"AI product\", \"AI application\", \"AI software\", \"AI hardware\", \"AI chip\", \"AI processor\",\n",
    "    \"GPU\", \"TPU\", \"FPGA\", \"ASIC\", \"edge AI\", \"cloud AI\", \"AI as a service\", \"AIaaS\",\n",
    "    \"TensorFlow\", \"PyTorch\", \"Keras\", \"scikit-learn\", \"Hugging Face\", \"OpenAI\", \"GPT\",\n",
    "    \n",
    "    # AI development and deployment\n",
    "    \"AI development\", \"AI engineering\", \"AI implementation\", \"AI integration\", \"AI deployment\",\n",
    "    \"AI pipeline\", \"AI workflow\", \"AI lifecycle\", \"AI project\", \"AI team\", \"AI researcher\",\n",
    "    \"AI scientist\", \"AI engineer\", \"AI developer\", \"data scientist\", \"ML engineer\",\n",
    "    \"MLOps\", \"AIOps\", \"DevOps for AI\", \"CI/CD for AI\", \"model management\", \"model versioning\",\n",
    "    \n",
    "    # AI data\n",
    "    \"data\", \"dataset\", \"training data\", \"test data\", \"validation data\", \"labeled data\",\n",
    "    \"unlabeled data\", \"structured data\", \"unstructured data\", \"big data\", \"data mining\",\n",
    "    \"data preprocessing\", \"data cleaning\", \"data augmentation\", \"data annotation\",\n",
    "    \"data labeling\", \"data quality\", \"data bias\", \"data privacy\", \"data security\",\n",
    "    \n",
    "    # AI ethics and responsible AI\n",
    "    \"AI ethics\", \"ethical AI\", \"responsible AI\", \"trustworthy AI\", \"explainable AI\",\n",
    "    \"XAI\", \"interpretable AI\", \"AI transparency\", \"AI accountability\", \"AI fairness\",\n",
    "    \"AI bias\", \"algorithmic bias\", \"AI safety\", \"AI alignment\", \"AI governance\",\n",
    "    \"AI regulation\", \"AI policy\", \"AI standards\", \"AI principles\", \"AI guidelines\",\n",
    "    \n",
    "    # AI impact and future\n",
    "    \"AI impact\", \"AI effect\", \"AI transformation\", \"AI disruption\", \"AI revolution\",\n",
    "    \"AI adoption\", \"AI maturity\", \"AI capability\", \"AI limitation\", \"AI challenge\",\n",
    "    \"AI opportunity\", \"AI benefit\", \"AI risk\", \"AI threat\", \"AI concern\", \"AI future\",\n",
    "    \"artificial general intelligence\", \"AGI\", \"artificial superintelligence\", \"ASI\",\n",
    "    \"singularity\", \"technological singularity\", \"human-level AI\", \"superhuman AI\"\n",
    "]\n",
    "\n",
    "# SPACE DOMAIN\n",
    "SPACE_KEYWORDS = [\n",
    "    # Core space terms\n",
    "    \"space\", \"outer space\", \"cosmos\", \"universe\", \"astronomy\", \"astrophysics\", \"cosmology\",\n",
    "    \"celestial\", \"extraterrestrial\", \"interstellar\", \"intergalactic\", \"orbital\", \"cosmic\",\n",
    "    \n",
    "    # Solar system\n",
    "    \"solar system\", \"sun\", \"star\", \"planet\", \"moon\", \"satellite\", \"asteroid\", \"comet\",\n",
    "    \"meteor\", \"meteorite\", \"meteoroid\", \"dwarf planet\", \"Pluto\", \"Mercury\", \"Venus\",\n",
    "    \"Earth\", \"Mars\", \"Jupiter\", \"Saturn\", \"Uranus\", \"Neptune\", \"terrestrial planet\",\n",
    "    \"gas giant\", \"ice giant\", \"asteroid belt\", \"Kuiper belt\", \"Oort cloud\",\n",
    "    \n",
    "    # Space objects and phenomena\n",
    "    \"galaxy\", \"Milky Way\", \"Andromeda\", \"nebula\", \"star cluster\", \"globular cluster\",\n",
    "    \"black hole\", \"supermassive black hole\", \"neutron star\", \"pulsar\", \"quasar\",\n",
    "    \"supernova\", \"nova\", \"white dwarf\", \"red giant\", \"brown dwarf\", \"exoplanet\",\n",
    "    \"extrasolar planet\", \"planetary system\", \"solar flare\", \"cosmic ray\", \"dark matter\",\n",
    "    \"dark energy\", \"gravitational wave\", \"cosmic microwave background\", \"CMB\",\n",
    "    \n",
    "    # Space exploration\n",
    "    \"space exploration\", \"space mission\", \"space program\", \"space agency\", \"NASA\",\n",
    "    \"ESA\", \"Roscosmos\", \"CNSA\", \"ISRO\", \"JAXA\", \"space race\", \"manned mission\",\n",
    "    \"unmanned mission\", \"robotic mission\", \"space probe\", \"rover\", \"lander\", \"orbiter\",\n",
    "    \"flyby\", \"sample return\", \"space telescope\", \"observatory\", \"space station\",\n",
    "    \"International Space Station\", \"ISS\", \"Mir\", \"Skylab\", \"Tiangong\",\n",
    "    \n",
    "    # Spacecraft and technology\n",
    "    \"spacecraft\", \"spaceship\", \"space vehicle\", \"rocket\", \"launch vehicle\", \"booster\",\n",
    "    \"propulsion\", \"thruster\", \"engine\", \"fuel\", \"propellant\", \"payload\", \"satellite\",\n",
    "    \"communication satellite\", \"weather satellite\", \"spy satellite\", \"GPS satellite\",\n",
    "    \"space shuttle\", \"capsule\", \"module\", \"space suit\", \"EVA\", \"spacewalk\", \"docking\",\n",
    "    \n",
    "    # Space travel and habitation\n",
    "    \"space travel\", \"spaceflight\", \"human spaceflight\", \"crewed spaceflight\", \"uncrewed spaceflight\",\n",
    "    \"astronaut\", \"cosmonaut\", \"taikonaut\", \"space tourist\", \"space tourism\", \"space hotel\",\n",
    "    \"space colony\", \"space settlement\", \"space habitat\", \"space station\", \"lunar base\",\n",
    "    \"Mars base\", \"terraforming\", \"life support\", \"artificial gravity\", \"radiation protection\",\n",
    "    \n",
    "    # Space industry and commerce\n",
    "    \"space industry\", \"commercial space\", \"private space\", \"NewSpace\", \"space economy\",\n",
    "    \"space business\", \"space company\", \"space startup\", \"launch provider\", \"launch service\",\n",
    "    \"SpaceX\", \"Blue Origin\", \"Virgin Galactic\", \"Rocket Lab\", \"satellite operator\",\n",
    "    \"satellite manufacturer\", \"space mining\", \"asteroid mining\", \"space resources\",\n",
    "    \"space manufacturing\", \"in-space manufacturing\", \"space advertising\", \"space entertainment\",\n",
    "    \n",
    "    # Space policy and law\n",
    "    \"space policy\", \"space law\", \"space treaty\", \"Outer Space Treaty\", \"Moon Treaty\",\n",
    "    \"space regulation\", \"space governance\", \"space traffic management\", \"space debris\",\n",
    "    \"space junk\", \"orbital debris\", \"space sustainability\", \"planetary protection\",\n",
    "    \"space defense\", \"space security\", \"space force\", \"space command\", \"space warfare\",\n",
    "    \n",
    "    # Space science and research (continued)\n",
    "    \"space science\", \"space research\", \"astronomy\", \"astrophysics\", \"astrochemistry\",\n",
    "    \"astrobiology\", \"exobiology\", \"SETI\", \"search for extraterrestrial intelligence\",\n",
    "    \"radio astronomy\", \"optical astronomy\", \"infrared astronomy\", \"ultraviolet astronomy\",\n",
    "    \"X-ray astronomy\", \"gamma-ray astronomy\", \"gravitational wave astronomy\", \"cosmology\",\n",
    "    \"planetary science\", \"lunar science\", \"Mars science\", \"heliophysics\", \"space weather\"\n",
    "]\n",
    "\n",
    "# AGRICULTURE DOMAIN\n",
    "AGRICULTURE_KEYWORDS = [\n",
    "    # Core agriculture terms\n",
    "    \"agriculture\", \"agricultural\", \"farming\", \"farm\", \"farmer\", \"crop\", \"livestock\",\n",
    "    \"cultivation\", \"harvest\", \"planting\", \"growing\", \"production\", \"agribusiness\",\n",
    "    \n",
    "    # Crop farming\n",
    "    \"crop\", \"grain\", \"cereal\", \"wheat\", \"corn\", \"maize\", \"rice\", \"barley\", \"oats\",\n",
    "    \"sorghum\", \"millet\", \"soybean\", \"pulse\", \"legume\", \"bean\", \"pea\", \"lentil\",\n",
    "    \"oilseed\", \"canola\", \"rapeseed\", \"sunflower\", \"flax\", \"cotton\", \"fiber crop\",\n",
    "    \"sugar crop\", \"sugarcane\", \"sugar beet\", \"fruit\", \"vegetable\", \"tuber\", \"root crop\",\n",
    "    \n",
    "    # Livestock and animal husbandry\n",
    "    \"livestock\", \"animal husbandry\", \"cattle\", \"cow\", \"beef\", \"dairy\", \"milk\",\n",
    "    \"sheep\", \"lamb\", \"goat\", \"pig\", \"pork\", \"swine\", \"poultry\", \"chicken\", \"turkey\",\n",
    "    \"egg\", \"horse\", \"equine\", \"aquaculture\", \"fish farming\", \"shrimp farming\",\n",
    "    \"apiculture\", \"beekeeping\", \"honey\", \"sericulture\", \"silk\", \"animal breeding\",\n",
    "    \n",
    "    # Farming practices and systems\n",
    "    \"conventional farming\", \"traditional farming\", \"industrial farming\", \"commercial farming\",\n",
    "    \"subsistence farming\", \"family farming\", \"small-scale farming\", \"large-scale farming\",\n",
    "    \"organic farming\", \"sustainable agriculture\", \"regenerative agriculture\", \"conservation agriculture\",\n",
    "    \"precision agriculture\", \"precision farming\", \"smart farming\", \"digital agriculture\",\n",
    "    \"vertical farming\", \"urban farming\", \"hydroponics\", \"aquaponics\", \"aeroponics\",\n",
    "    \n",
    "    # Farm inputs and resources\n",
    "    \"seed\", \"seedling\", \"planting material\", \"fertilizer\", \"nutrient\", \"nitrogen\",\n",
    "    \"phosphorus\", \"potassium\", \"NPK\", \"manure\", \"compost\", \"pesticide\", \"herbicide\",\n",
    "    \"fungicide\", \"insecticide\", \"integrated pest management\", \"IPM\", \"irrigation\",\n",
    "    \"water management\", \"soil\", \"soil health\", \"soil fertility\", \"soil conservation\",\n",
    "    \"land\", \"arable land\", \"pasture\", \"grazing land\", \"farmland\", \"agricultural land\",\n",
    "    \n",
    "    # Farm equipment and technology\n",
    "    \"farm equipment\", \"agricultural machinery\", \"tractor\", \"plow\", \"plough\", \"harrow\",\n",
    "    \"cultivator\", \"seeder\", \"planter\", \"harvester\", \"combine\", \"combine harvester\",\n",
    "    \"thresher\", \"baler\", \"irrigation system\", \"sprinkler\", \"drip irrigation\", \"greenhouse\",\n",
    "    \"barn\", \"silo\", \"storage\", \"drone\", \"agricultural drone\", \"GPS\", \"GIS\", \"remote sensing\",\n",
    "    \"sensor\", \"IoT\", \"Internet of Things\", \"automation\", \"robotics\", \"agricultural robot\",\n",
    "    \n",
    "    # Agricultural economics and business\n",
    "    \"agricultural economics\", \"agribusiness\", \"agricultural market\", \"commodity\",\n",
    "    \"agricultural commodity\", \"agricultural trade\", \"agricultural export\", \"agricultural import\",\n",
    "    \"agricultural policy\", \"agricultural subsidy\", \"price support\", \"farm income\",\n",
    "    \"farm profit\", \"farm cost\", \"farm investment\", \"farm loan\", \"agricultural credit\",\n",
    "    \"agricultural insurance\", \"crop insurance\", \"agricultural cooperative\", \"farmer cooperative\",\n",
    "    \n",
    "    # Agricultural development and issues\n",
    "    \"agricultural development\", \"rural development\", \"food security\", \"food sovereignty\",\n",
    "    \"food system\", \"food supply chain\", \"agricultural productivity\", \"crop yield\",\n",
    "    \"agricultural efficiency\", \"agricultural sustainability\", \"climate-smart agriculture\",\n",
    "    \"agricultural resilience\", \"agricultural adaptation\", \"agricultural mitigation\",\n",
    "    \"agricultural pollution\", \"agricultural runoff\", \"agricultural waste\", \"land degradation\",\n",
    "    \n",
    "    # Agricultural research and education\n",
    "    \"agricultural research\", \"agricultural science\", \"agricultural technology\", \"agtech\",\n",
    "    \"agritech\", \"agricultural innovation\", \"agricultural extension\", \"agricultural education\",\n",
    "    \"agricultural training\", \"agricultural university\", \"agricultural college\", \"agricultural school\",\n",
    "    \"agricultural experiment station\", \"agricultural laboratory\", \"agricultural field trial\"\n",
    "]\n",
    "\n",
    "# MENTAL HEALTH DOMAIN\n",
    "MENTAL_HEALTH_KEYWORDS = [\n",
    "    # Core mental health terms\n",
    "    \"mental health\", \"mental illness\", \"mental disorder\", \"mental wellbeing\", \"psychological\",\n",
    "    \"psychiatric\", \"emotional health\", \"behavioral health\", \"cognitive health\", \"brain health\",\n",
    "    \n",
    "    # Mental health conditions\n",
    "    \"depression\", \"major depressive disorder\", \"MDD\", \"anxiety\", \"anxiety disorder\",\n",
    "    \"generalized anxiety disorder\", \"GAD\", \"panic disorder\", \"panic attack\", \"phobia\",\n",
    "    \"social anxiety\", \"obsessive-compulsive disorder\", \"OCD\", \"post-traumatic stress disorder\",\n",
    "    \"PTSD\", \"trauma\", \"bipolar disorder\", \"bipolar\", \"schizophrenia\", \"psychosis\",\n",
    "    \"eating disorder\", \"anorexia\", \"bulimia\", \"binge eating disorder\", \"ADHD\",\n",
    "    \"attention deficit hyperactivity disorder\", \"autism\", \"autism spectrum disorder\", \"ASD\",\n",
    "    \n",
    "    # Mental health symptoms and experiences\n",
    "    \"symptom\", \"distress\", \"psychological distress\", \"emotional distress\", \"stress\",\n",
    "    \"chronic stress\", \"burnout\", \"mood\", \"mood disorder\", \"mood swing\", \"depression\",\n",
    "    \"depressed\", \"sadness\", \"grief\", \"bereavement\", \"anxiety\", \"anxious\", \"worry\",\n",
    "    \"fear\", \"panic\", \"trauma\", \"traumatic\", \"flashback\", \"nightmare\", \"insomnia\",\n",
    "    \"sleep disturbance\", \"fatigue\", \"irritability\", \"anger\", \"aggression\", \"self-harm\",\n",
    "    \n",
    "    # Mental health treatment and services\n",
    "    \"treatment\", \"therapy\", \"psychotherapy\", \"counseling\", \"intervention\", \"support\",\n",
    "    \"mental health service\", \"psychiatric service\", \"inpatient\", \"outpatient\", \"residential\",\n",
    "    \"day program\", \"crisis intervention\", \"emergency service\", \"hotline\", \"helpline\",\n",
    "    \"telehealth\", \"teletherapy\", \"online therapy\", \"medication\", \"psychiatric medication\",\n",
    "    \"antidepressant\", \"anxiolytic\", \"antipsychotic\", \"mood stabilizer\", \"stimulant\",\n",
    "    \n",
    "    # Mental health professionals\n",
    "    \"mental health professional\", \"mental health provider\", \"psychiatrist\", \"psychologist\",\n",
    "    \"therapist\", \"counselor\", \"social worker\", \"psychiatric nurse\", \"nurse practitioner\",\n",
    "    \"case manager\", \"peer support specialist\", \"mental health worker\", \"mental health team\",\n",
    "    \"multidisciplinary team\", \"treatment team\", \"care coordinator\", \"primary care provider\",\n",
    "    \n",
    "    # Therapy approaches and modalities\n",
    "    \"cognitive behavioral therapy\", \"CBT\", \"dialectical behavior therapy\", \"DBT\",\n",
    "    \"psychodynamic therapy\", \"interpersonal therapy\", \"IPT\", \"acceptance and commitment therapy\",\n",
    "    \"ACT\", \"mindfulness-based therapy\", \"MBCT\", \"MBSR\", \"exposure therapy\", \"EMDR\",\n",
    "    \"eye movement desensitization and reprocessing\", \"family therapy\", \"couples therapy\",\n",
    "    \"group therapy\", \"art therapy\", \"music therapy\", \"play therapy\", \"animal-assisted therapy\",\n",
    "    \n",
    "    # Mental health settings and systems\n",
    "    \"mental health system\", \"mental healthcare\", \"psychiatric hospital\", \"psychiatric ward\",\n",
    "    \"mental health clinic\", \"community mental health center\", \"private practice\",\n",
    "    \"residential treatment\", \"inpatient unit\", \"outpatient clinic\", \"day treatment\",\n",
    "    \"partial hospitalization\", \"intensive outpatient program\", \"IOP\", \"crisis center\",\n",
    "    \"emergency room\", \"psychiatric emergency\", \"commitment\", \"involuntary commitment\",\n",
    "    \n",
    "    # Mental health promotion and prevention\n",
    "    \"mental health promotion\", \"mental health prevention\", \"mental health awareness\",\n",
    "    \"mental health education\", \"mental health literacy\", \"mental health first aid\",\n",
    "    \"suicide prevention\", \"crisis prevention\", \"relapse prevention\", \"early intervention\",\n",
    "    \"screening\", \"assessment\", \"mental health check-up\", \"self-care\", \"wellness\",\n",
    "    \"resilience\", \"coping\", \"coping skill\", \"coping strategy\", \"stress management\",\n",
    "    \n",
    "    # Mental health policy and advocacy\n",
    "    \"mental health policy\", \"mental health legislation\", \"mental health reform\",\n",
    "    \"mental health advocacy\", \"mental health advocate\", \"mental health activism\",\n",
    "    \"mental health rights\", \"patient rights\", \"parity\", \"mental health parity\",\n",
    "    \"stigma\", \"mental health stigma\", \"discrimination\", \"mental health awareness\",\n",
    "    \"mental health campaign\", \"mental health movement\", \"lived experience\", \"peer support\"\n",
    "]\n",
    "\n",
    "\n",
    "# Expanded keywords for food domain\n",
    "FOOD_KEYWORDS = [\n",
    "    # Core food terms\n",
    "    \"food\", \"cuisine\", \"dish\", \"meal\", \"recipe\", \"ingredient\", \"cooking\", \"baking\",\n",
    "    \"chef\", \"restaurant\", \"dining\", \"menu\", \"taste\", \"flavor\", \"delicious\", \"culinary\",\n",
    "\n",
    "    # Food types\n",
    "    \"vegetarian\", \"vegan\", \"organic\", \"gluten-free\", \"dairy-free\", \"pescatarian\",\n",
    "    \"keto\", \"paleo\", \"mediterranean\", \"plant-based\", \"farm-to-table\", \"locally-sourced\",\n",
    "\n",
    "    # Cooking methods\n",
    "    \"roast\", \"bake\", \"fry\", \"grill\", \"sauté\", \"boil\", \"steam\", \"simmer\", \"braise\",\n",
    "    \"marinate\", \"ferment\", \"pickle\", \"smoke\", \"sous-vide\", \"pressure cook\", \"slow cook\",\n",
    "\n",
    "    # Meal types\n",
    "    \"breakfast\", \"lunch\", \"dinner\", \"brunch\", \"appetizer\", \"entree\", \"dessert\",\n",
    "    \"snack\", \"side dish\", \"main course\", \"buffet\", \"tasting menu\", \"prix fixe\",\n",
    "\n",
    "    # Food categories\n",
    "    \"fruit\", \"vegetable\", \"meat\", \"poultry\", \"seafood\", \"grain\", \"legume\", \"dairy\",\n",
    "    \"bread\", \"pasta\", \"rice\", \"noodle\", \"soup\", \"salad\", \"sandwich\", \"stew\", \"curry\",\n",
    "\n",
    "    # Specific foods\n",
    "    \"beef\", \"chicken\", \"pork\", \"lamb\", \"fish\", \"shrimp\", \"tofu\", \"cheese\", \"yogurt\",\n",
    "    \"butter\", \"egg\", \"milk\", \"cream\", \"chocolate\", \"sugar\", \"flour\", \"olive oil\",\n",
    "\n",
    "    # Cuisines\n",
    "    \"italian\", \"french\", \"chinese\", \"japanese\", \"indian\", \"mexican\", \"thai\", \"korean\",\n",
    "    \"vietnamese\", \"greek\", \"spanish\", \"middle eastern\", \"mediterranean\", \"cajun\", \"creole\",\n",
    "    \"southern\", \"soul food\", \"bbq\", \"barbecue\", \"fusion\",\n",
    "\n",
    "    # Restaurant terms\n",
    "    \"michelin\", \"bistro\", \"cafe\", \"diner\", \"eatery\", \"food truck\", \"pop-up\", \"takeout\",\n",
    "    \"delivery\", \"catering\", \"fine dining\", \"casual dining\", \"fast food\", \"fast casual\",\n",
    "\n",
    "    # Food industry\n",
    "    \"food industry\", \"food supply\", \"food production\", \"food processing\", \"food safety\",\n",
    "    \"food security\", \"food waste\", \"sustainable food\", \"organic farming\", \"food system\",\n",
    "\n",
    "    # Food trends\n",
    "    \"food trend\", \"superfood\", \"artisanal\", \"craft\", \"gourmet\", \"foodie\", \"food blogger\",\n",
    "    \"food photography\", \"food styling\", \"food review\", \"food critic\", \"food award\"\n",
    "]\n",
    "\n",
    "# Expanded keywords for agriculture domain\n",
    "AGRICULTURE_KEYWORDS = [\n",
    "    # Core agriculture terms\n",
    "    \"agriculture\", \"farming\", \"farm\", \"crop\", \"harvest\", \"cultivation\", \"livestock\",\n",
    "    \"agricultural\", \"farmer\", \"ranch\", \"plantation\", \"orchard\", \"vineyard\", \"greenhouse\",\n",
    "\n",
    "    # Farming types\n",
    "    \"organic farming\", \"conventional farming\", \"sustainable agriculture\", \"regenerative agriculture\",\n",
    "    \"precision agriculture\", \"vertical farming\", \"urban farming\", \"hydroponics\", \"aquaponics\",\n",
    "    \"aeroponics\", \"permaculture\", \"agroforestry\", \"silvopasture\", \"biodynamic farming\",\n",
    "\n",
    "    # Crops and plants\n",
    "    \"wheat\", \"corn\", \"rice\", \"soybean\", \"barley\", \"oat\", \"cotton\", \"sugarcane\", \"potato\",\n",
    "    \"tomato\", \"fruit tree\", \"vegetable\", \"grain\", \"legume\", \"cereal\", \"produce\", \"seed\",\n",
    "    \"seedling\", \"plant\", \"crop rotation\", \"cover crop\", \"cash crop\", \"gmo\", \"hybrid\",\n",
    "\n",
    "    # Livestock and animal husbandry\n",
    "    \"cattle\", \"cow\", \"beef\", \"dairy\", \"poultry\", \"chicken\", \"pig\", \"sheep\", \"goat\",\n",
    "    \"livestock\", \"animal husbandry\", \"breeding\", \"feed\", \"grazing\", \"pasture\", \"free-range\",\n",
    "    \"cage-free\", \"grass-fed\", \"organic meat\", \"veterinary\", \"animal welfare\",\n",
    "\n",
    "    # Farm equipment and technology\n",
    "    \"tractor\", \"combine\", \"harvester\", \"plow\", \"irrigation\", \"sprinkler\", \"drone\",\n",
    "    \"sensor\", \"gps\", \"precision farming\", \"farm equipment\", \"agricultural machinery\",\n",
    "    \"smart farming\", \"agtech\", \"agricultural technology\", \"farm management software\",\n",
    "\n",
    "    # Agricultural practices\n",
    "    \"irrigation\", \"fertilizer\", \"pesticide\", \"herbicide\", \"insecticide\", \"fungicide\",\n",
    "    \"soil management\", \"composting\", \"mulching\", \"tilling\", \"no-till\", \"crop rotation\",\n",
    "    \"intercropping\", \"companion planting\", \"integrated pest management\", \"sustainable practices\",\n",
    "\n",
    "    # Agricultural economics\n",
    "    \"agribusiness\", \"agricultural economics\", \"farm income\", \"farm subsidy\", \"agricultural policy\",\n",
    "    \"commodity\", \"agricultural market\", \"agricultural trade\", \"agricultural export\", \"agricultural import\",\n",
    "    \"farm bill\", \"agricultural cooperative\", \"farm-to-table\", \"community supported agriculture\",\n",
    "\n",
    "    # Agricultural challenges\n",
    "    \"drought\", \"flood\", \"pest\", \"disease\", \"climate change\", \"soil erosion\", \"soil degradation\",\n",
    "    \"water scarcity\", \"food security\", \"food sovereignty\", \"land use\", \"deforestation\",\n",
    "\n",
    "    # Agricultural organizations\n",
    "    \"usda\", \"fao\", \"department of agriculture\", \"agricultural extension\", \"4-h\", \"ffa\",\n",
    "    \"farm bureau\", \"agricultural research\", \"land grant university\", \"agricultural education\"\n",
    "]\n",
    "\n",
    "# Expanded keywords for environment domain\n",
    "ENVIRONMENT_KEYWORDS = [\n",
    "    # Core environmental terms\n",
    "    \"environment\", \"environmental\", \"ecology\", \"ecosystem\", \"habitat\", \"biodiversity\",\n",
    "    \"conservation\", \"preservation\", \"sustainability\", \"sustainable\", \"green\", \"eco-friendly\",\n",
    "\n",
    "    # Climate and atmosphere\n",
    "    \"climate\", \"climate change\", \"global warming\", \"greenhouse gas\", \"carbon\", \"carbon dioxide\",\n",
    "    \"methane\", \"emission\", \"carbon footprint\", \"carbon neutral\", \"carbon offset\", \"atmosphere\",\n",
    "    \"ozone\", \"ozone layer\", \"air quality\", \"air pollution\", \"smog\", \"particulate matter\",\n",
    "\n",
    "    # Water systems\n",
    "    \"water\", \"freshwater\", \"groundwater\", \"aquifer\", \"watershed\", \"river\", \"lake\", \"ocean\",\n",
    "    \"sea\", \"marine\", \"coastal\", \"wetland\", \"water pollution\", \"water quality\", \"water scarcity\",\n",
    "    \"drought\", \"flood\", \"stormwater\", \"runoff\", \"wastewater\", \"water conservation\",\n",
    "\n",
    "    # Land and soil\n",
    "    \"land\", \"soil\", \"forest\", \"deforestation\", \"reforestation\", \"afforestation\", \"desertification\",\n",
    "    \"erosion\", \"land degradation\", \"land use\", \"land management\", \"soil health\", \"soil quality\",\n",
    "    \"soil conservation\", \"soil pollution\", \"contamination\", \"brownfield\", \"greenfield\",\n",
    "\n",
    "    # Biodiversity and ecosystems\n",
    "    \"biodiversity\", \"species\", \"endangered species\", \"extinction\", \"wildlife\", \"flora\", \"fauna\",\n",
    "    \"ecosystem\", \"ecosystem service\", \"habitat\", \"habitat loss\", \"habitat fragmentation\",\n",
    "    \"protected area\", \"national park\", \"wildlife refuge\", \"conservation\", \"preservation\",\n",
    "\n",
    "    # Pollution and waste\n",
    "    \"pollution\", \"pollutant\", \"contaminant\", \"waste\", \"solid waste\", \"hazardous waste\",\n",
    "    \"e-waste\", \"plastic\", \"plastic pollution\", \"microplastic\", \"recycling\", \"composting\",\n",
    "    \"landfill\", \"incineration\", \"zero waste\", \"circular economy\", \"waste management\",\n",
    "\n",
    "    # Energy and resources\n",
    "    \"energy\", \"renewable energy\", \"solar\", \"wind\", \"hydroelectric\", \"geothermal\", \"biomass\",\n",
    "    \"fossil fuel\", \"coal\", \"oil\", \"natural gas\", \"nuclear\", \"energy efficiency\", \"resource\",\n",
    "    \"natural resource\", \"resource depletion\", \"resource conservation\", \"sustainable resource\",\n",
    "\n",
    "    # Environmental policy and management\n",
    "    \"environmental policy\", \"environmental regulation\", \"environmental law\", \"environmental protection\",\n",
    "    \"environmental impact\", \"environmental assessment\", \"environmental monitoring\", \"environmental standard\",\n",
    "    \"epa\", \"environmental protection agency\", \"environmental justice\", \"environmental ethics\",\n",
    "\n",
    "    # Sustainability practices\n",
    "    \"sustainability\", \"sustainable development\", \"sustainable living\", \"sustainable agriculture\",\n",
    "    \"sustainable forestry\", \"sustainable fishing\", \"sustainable transportation\", \"green building\",\n",
    "    \"leed\", \"eco-label\", \"organic\", \"fair trade\", \"corporate sustainability\", \"esg\",\n",
    "\n",
    "    # Environmental movements and concepts\n",
    "    \"environmentalism\", \"environmental movement\", \"environmental activist\", \"environmental advocacy\",\n",
    "    \"green movement\", \"conservation movement\", \"deep ecology\", \"ecofeminism\", \"environmental justice\",\n",
    "    \"climate justice\", \"climate action\", \"climate strike\", \"paris agreement\", \"kyoto protocol\"\n",
    "]\n",
    "\n",
    "# Expanded keywords for education domain\n",
    "EDUCATION_KEYWORDS = [\n",
    "    # Core education terms\n",
    "    \"education\", \"learning\", \"teaching\", \"school\", \"student\", \"teacher\", \"classroom\",\n",
    "    \"curriculum\", \"instruction\", \"pedagogy\", \"academic\", \"educational\", \"educator\",\n",
    "\n",
    "    # Educational institutions\n",
    "    \"school\", \"university\", \"college\", \"academy\", \"institute\", \"campus\", \"kindergarten\",\n",
    "    \"preschool\", \"elementary school\", \"primary school\", \"middle school\", \"high school\",\n",
    "    \"secondary school\", \"community college\", \"vocational school\", \"trade school\",\n",
    "    \"boarding school\", \"charter school\", \"private school\", \"public school\", \"homeschool\",\n",
    "\n",
    "    # Academic levels and programs\n",
    "    \"degree\", \"bachelor\", \"master\", \"doctorate\", \"phd\", \"undergraduate\", \"graduate\",\n",
    "    \"postgraduate\", \"diploma\", \"certificate\", \"major\", \"minor\", \"concentration\",\n",
    "    \"specialization\", \"program\", \"course\", \"class\", \"seminar\", \"lecture\", \"workshop\",\n",
    "\n",
    "    # Educational roles\n",
    "    \"student\", \"pupil\", \"learner\", \"teacher\", \"professor\", \"instructor\", \"educator\",\n",
    "    \"faculty\", \"staff\", \"administrator\", \"principal\", \"headmaster\", \"dean\", \"provost\",\n",
    "    \"chancellor\", \"superintendent\", \"school board\", \"trustee\", \"counselor\", \"advisor\",\n",
    "\n",
    "    # Educational activities and methods\n",
    "    \"teaching\", \"learning\", \"studying\", \"research\", \"assignment\", \"homework\", \"project\",\n",
    "    \"exam\", \"test\", \"quiz\", \"assessment\", \"evaluation\", \"grading\", \"feedback\", \"lecture\",\n",
    "    \"discussion\", \"group work\", \"collaboration\", \"presentation\", \"demonstration\",\n",
    "\n",
    "    # Educational approaches and philosophies\n",
    "    \"pedagogy\", \"curriculum\", \"instruction\", \"methodology\", \"approach\", \"philosophy\",\n",
    "    \"theory\", \"framework\", \"model\", \"standard\", \"benchmark\", \"objective\", \"outcome\",\n",
    "    \"competency\", \"skill\", \"knowledge\", \"understanding\", \"critical thinking\", \"problem-solving\",\n",
    "\n",
    "    # Educational technology\n",
    "    \"educational technology\", \"edtech\", \"e-learning\", \"online learning\", \"distance learning\",\n",
    "    \"remote learning\", \"virtual classroom\", \"learning management system\", \"digital learning\",\n",
    "    \"blended learning\", \"hybrid learning\", \"mooc\", \"adaptive learning\", \"personalized learning\",\n",
    "\n",
    "    # Educational policy and administration\n",
    "    \"education policy\", \"education reform\", \"education law\", \"education funding\", \"education budget\",\n",
    "    \"department of education\", \"ministry of education\", \"school district\", \"accreditation\",\n",
    "    \"standardized testing\", \"common core\", \"no child left behind\", \"every student succeeds act\",\n",
    "\n",
    "    # Educational issues and concepts\n",
    "    \"access to education\", \"educational equity\", \"educational equality\", \"educational opportunity\",\n",
    "    \"achievement gap\", \"digital divide\", \"literacy\", \"numeracy\", \"stem education\", \"arts education\",\n",
    "    \"special education\", \"inclusive education\", \"multicultural education\", \"bilingual education\",\n",
    "\n",
    "    # Lifelong learning\n",
    "    \"lifelong learning\", \"continuing education\", \"adult education\", \"professional development\",\n",
    "    \"training\", \"upskilling\", \"reskilling\", \"certification\", \"recertification\", \"credential\",\n",
    "    \"competency-based education\", \"experiential learning\", \"self-directed learning\"\n",
    "]\n",
    "\n",
    "# Expanded keywords for mental health domain\n",
    "MENTAL_HEALTH_KEYWORDS = [\n",
    "    # Core mental health terms\n",
    "    \"mental health\", \"mental illness\", \"mental disorder\", \"mental wellbeing\", \"psychological\",\n",
    "    \"psychiatric\", \"emotional health\", \"behavioral health\", \"cognitive health\", \"neurodevelopmental\",\n",
    "\n",
    "    # Mental health conditions\n",
    "    \"depression\", \"anxiety\", \"bipolar disorder\", \"schizophrenia\", \"ptsd\", \"trauma\",\n",
    "    \"ocd\", \"obsessive-compulsive\", \"adhd\", \"attention deficit\", \"autism\", \"eating disorder\",\n",
    "    \"anorexia\", \"bulimia\", \"binge eating\", \"addiction\", \"substance abuse\", \"alcoholism\",\n",
    "    \"personality disorder\", \"borderline\", \"narcissistic\", \"psychosis\", \"panic attack\", \"phobia\",\n",
    "\n",
    "    # Mental health professionals\n",
    "    \"psychiatrist\", \"psychologist\", \"therapist\", \"counselor\", \"social worker\", \"mental health provider\",\n",
    "    \"psychiatric nurse\", \"behavioral therapist\", \"clinical psychologist\", \"neuropsychologist\",\n",
    "    \"psychotherapist\", \"mental health practitioner\", \"mental health specialist\",\n",
    "\n",
    "    # Treatment and interventions\n",
    "    \"therapy\", \"psychotherapy\", \"counseling\", \"treatment\", \"intervention\", \"medication\",\n",
    "    \"antidepressant\", \"antipsychotic\", \"mood stabilizer\", \"anxiolytic\", \"cognitive behavioral therapy\",\n",
    "    \"cbt\", \"dialectical behavior therapy\", \"dbt\", \"exposure therapy\", \"emdr\", \"mindfulness\",\n",
    "    \"group therapy\", \"family therapy\", \"couples therapy\", \"art therapy\", \"music therapy\",\n",
    "\n",
    "    # Mental health services and settings\n",
    "    \"mental health service\", \"psychiatric service\", \"inpatient\", \"outpatient\", \"psychiatric hospital\",\n",
    "    \"mental health clinic\", \"community mental health\", \"crisis intervention\", \"suicide prevention\",\n",
    "    \"hotline\", \"teletherapy\", \"telehealth\", \"residential treatment\", \"day program\", \"partial hospitalization\",\n",
    "\n",
    "    # Mental health concepts\n",
    "    \"stigma\", \"discrimination\", \"awareness\", \"advocacy\", \"recovery\", \"resilience\", \"coping\",\n",
    "    \"self-care\", \"stress management\", \"emotional regulation\", \"emotional intelligence\",\n",
    "    \"psychological distress\", \"psychological wellbeing\", \"mental fitness\", \"mental hygiene\",\n",
    "\n",
    "    # Mental health assessment\n",
    "    \"diagnosis\", \"assessment\", \"evaluation\", \"screening\", \"psychological testing\", \"neuropsychological testing\",\n",
    "    \"dsm\", \"diagnostic and statistical manual\", \"icd\", \"international classification of diseases\",\n",
    "    \"mental status exam\", \"psychological assessment\", \"risk assessment\", \"suicide assessment\",\n",
    "\n",
    "    # Mental health policy and systems\n",
    "    \"mental health policy\", \"mental health system\", \"mental health reform\", \"mental health legislation\",\n",
    "    \"mental health funding\", \"mental health insurance\", \"parity\", \"community-based care\",\n",
    "    \"deinstitutionalization\", \"integrated care\", \"collaborative care\", \"mental health workforce\",\n",
    "\n",
    "    # Mental health in specific populations\n",
    "    \"child mental health\", \"adolescent mental health\", \"youth mental health\", \"adult mental health\",\n",
    "    \"geriatric mental health\", \"maternal mental health\", \"perinatal mental health\", \"postpartum\",\n",
    "    \"veteran mental health\", \"military mental health\", \"workplace mental health\", \"student mental health\",\n",
    "\n",
    "    # Mental health promotion and prevention\n",
    "    \"mental health promotion\", \"mental health prevention\", \"mental health education\", \"mental health literacy\",\n",
    "    \"mental health first aid\", \"suicide prevention\", \"bullying prevention\", \"stress reduction\",\n",
    "    \"mindfulness\", \"meditation\", \"relaxation\", \"work-life balance\", \"burnout prevention\"\n",
    "]\n",
    "# Make these available globally\n",
    "globals()['TECHNOLOGY_KEYWORDS'] = TECHNOLOGY_KEYWORDS\n",
    "globals()['BUSINESS_KEYWORDS'] = BUSINESS_KEYWORDS\n",
    "globals()['HEALTH_KEYWORDS '] = HEALTH_KEYWORDS \n",
    "globals()['SPORTS_KEYWORDS'] = SPORTS_KEYWORDS\n",
    "globals()['ENTERTAINMENT_KEYWORDS'] = ENTERTAINMENT_KEYWORDS\n",
    "globals()['SCIENCE_KEYWORDS'] = SCIENCE_KEYWORDS\n",
    "globals()['WORLD_KEYWORDS'] = WORLD_KEYWORDS\n",
    "globals()['EDUCATION_KEYWORDS'] = EDUCATION_KEYWORDS\n",
    "globals()['FOOD_KEYWORDS'] = FOOD_KEYWORDS\n",
    "globals()['TRAVEL_KEYWORDS'] = TRAVEL_KEYWORDS\n",
    "globals()['AUTOMOTIVE_KEYWORDS'] = AUTOMOTIVE_KEYWORDS\n",
    "globals()['REAL_ESTATE_KEYWORDS'] = REAL_ESTATE_KEYWORDS\n",
    "globals()['CYBERSECURITY_KEYWORDS'] = CYBERSECURITY_KEYWORDS\n",
    "globals()['AI_KEYWORDS'] = AI_KEYWORDS\n",
    "globals()['SPACE_KEYWORDS'] = SPACE_KEYWORDS\n",
    "globals()['AGRICULTURE_KEYWORDS'] = AGRICULTURE_KEYWORDS\n",
    "globals()['MENTAL_HEALTH_KEYWORDS'] = MENTAL_HEALTH_KEYWORDS\n",
    "\n",
    "\n",
    "# Set a flag to indicate expanded keywords are available\n",
    "globals()['has_expanded_keywords'] = True\n",
    "print(\"Defined comprehensive domain keyword lists in global namespace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T06:27:37.799266Z",
     "iopub.status.busy": "2025-04-12T06:27:37.799045Z",
     "iopub.status.idle": "2025-04-12T06:27:37.838846Z",
     "shell.execute_reply": "2025-04-12T06:27:37.837771Z",
     "shell.execute_reply.started": "2025-04-12T06:27:37.799246Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "class DomainFallbackClassifier:\n",
    "    def __init__(self):\n",
    "        self.domain_patterns = {\n",
    "            # TECHNOLOGY\n",
    "            \"technology\": [\n",
    "                r'\\b(?:software|hardware|app|application)\\s+(?:development|engineer|developer|programming|code)\\b',\n",
    "                r'\\b(?:artificial intelligence|machine learning|deep learning|neural network)\\s+(?:model|algorithm|system|application)\\b',\n",
    "                r'\\b(?:data|cloud|server|network|internet)\\s+(?:security|infrastructure|architecture|protocol|service)\\b',\n",
    "                r'\\b(?:mobile|web|desktop|cross-platform)\\s+(?:app|application|development|interface|experience)\\b',\n",
    "                r'\\b(?:tech|technology|digital|IT)\\s+(?:company|industry|sector|giant|startup|innovation)\\b'\n",
    "            ],\n",
    "            \n",
    "            # BUSINESS\n",
    "            \"business\": [\n",
    "                r'\\b(?:company|corporation|firm|enterprise)\\s+(?:announced|reported|launched|acquired|merged)\\b',\n",
    "                r'\\b(?:market|stock|share|investor|shareholder)\\s+(?:value|price|growth|decline|performance|return)\\b',\n",
    "                r'\\b(?:revenue|profit|earnings|sales|income)\\s+(?:growth|decline|increase|decrease|report|quarter|year)\\b',\n",
    "                r'\\b(?:CEO|executive|board|management|leadership)\\s+(?:team|decision|strategy|vision|announced|stated)\\b',\n",
    "                r'\\b(?:merger|acquisition|partnership|deal|agreement)\\s+(?:between|with|valued at|worth|announced|completed)\\b'\n",
    "            ],\n",
    "            \n",
    "            # HEALTH\n",
    "            \"health\": [\n",
    "                r'\\b(?:medical|clinical|health|healthcare|patient)\\s+(?:treatment|care|procedure|outcome|record|data)\\b',\n",
    "                r'\\b(?:disease|condition|disorder|syndrome|illness)\\s+(?:symptoms|diagnosis|treatment|prevention|management)\\b',\n",
    "                r'\\b(?:doctor|physician|surgeon|specialist|nurse)\\s+(?:recommended|prescribed|diagnosed|treated|examined)\\b',\n",
    "                r'\\b(?:drug|medication|vaccine|therapy|treatment)\\s+(?:approved|developed|prescribed|administered|effective)\\b',\n",
    "                r'\\b(?:study|research|trial|experiment|analysis)\\s+(?:showed|found|suggested|indicated|demonstrated)\\s+(?:health|medical|clinical)\\b'\n",
    "            ],\n",
    "            \n",
    "            # POLITICS\n",
    "            \"politics\": [\n",
    "                r'\\b(?:president|senator|congressman|representative|governor)\\s+(?:said|announced|proposed|signed|vetoed)\\b',\n",
    "                r'\\b(?:election|campaign|vote|ballot|poll)\\s+(?:result|outcome|turnout|system|fraud|integrity)\\b',\n",
    "                r'\\b(?:bill|legislation|law|policy|regulation)\\s+(?:passed|approved|signed|proposed|introduced|amended)\\b',\n",
    "                r'\\b(?:democrat|republican|liberal|conservative|progressive)\\s+(?:party|lawmaker|voter|base|agenda|platform)\\b',\n",
    "                r'\\b(?:government|administration|cabinet|congress|parliament)\\s+(?:official|decision|policy|action|response)\\b'\n",
    "            ],\n",
    "            \n",
    "            # SPORTS\n",
    "            \"sports\": [\n",
    "                r'\\b(?:team|player|coach|athlete|roster)\\s+(?:won|lost|defeated|beat|signed|traded|drafted)\\b',\n",
    "                r'\\b(?:game|match|tournament|championship|series)\\s+(?:victory|defeat|win|loss|title|trophy)\\b',\n",
    "                r'\\b(?:score|point|goal|touchdown|basket|run)\\s+(?:in the|during the|late in|early in|to win|to tie)\\b',\n",
    "                r'\\b(?:season|playoff|league|division|conference)\\s+(?:record|standing|title|championship|performance)\\b',\n",
    "                r'\\b(?:injury|contract|trade|draft|free agent|salary cap)\\s+(?:report|update|news|deal|agreement|situation)\\b'\n",
    "            ],\n",
    "            \n",
    "            # ENTERTAINMENT\n",
    "            \"entertainment\": [\n",
    "                r'\\b(?:movie|film|show|series|episode)\\s+(?:premiered|debuted|released|directed|produced|starred)\\b',\n",
    "                r'\\b(?:actor|actress|director|producer|celebrity)\\s+(?:starred|appeared|played|portrayed|directed|produced)\\b',\n",
    "                r'\\b(?:award|oscar|emmy|grammy|golden globe)\\s+(?:winner|nominee|ceremony|nomination|category)\\b',\n",
    "                r'\\b(?:box office|rating|review|critic|audience)\\s+(?:success|failure|hit|flop|reception|response)\\b',\n",
    "                r'\\b(?:music|song|album|track|artist)\\s+(?:released|debuted|topped|chart|billboard|streaming)\\b'\n",
    "            ],\n",
    "            \n",
    "            # SCIENCE\n",
    "            \"science\": [\n",
    "                r'\\b(?:scientist|researcher|study|experiment|discovery)\\s+(?:found|discovered|observed|demonstrated|published)\\b',\n",
    "                r'\\b(?:research|study|analysis|experiment|investigation)\\s+(?:published in|journal|peer-reviewed|scientific|academic)\\b',\n",
    "                r'\\b(?:theory|hypothesis|model|concept|principle)\\s+(?:suggests|predicts|explains|describes|proposes)\\b',\n",
    "                r'\\b(?:data|evidence|result|finding|observation)\\s+(?:suggests|indicates|shows|demonstrates|confirms)\\b',\n",
    "                r'\\b(?:physics|chemistry|biology|astronomy|geology)\\s+(?:principle|law|theory|concept|phenomenon)\\b'\n",
    "            ],\n",
    "            \n",
    "            # ENVIRONMENT\n",
    "            \"environment\": [\n",
    "                r'\\b(?:climate change|global warming|greenhouse gas|carbon emission|fossil fuel)\\s+(?:impact|effect|threat|crisis|action)\\b',\n",
    "                r'\\b(?:environmental|conservation|preservation|protection|sustainability)\\s+(?:effort|initiative|program|policy|regulation)\\b',\n",
    "                r'\\b(?:renewable|clean|green|sustainable|alternative)\\s+(?:energy|power|electricity|source|technology)\\b',\n",
    "                r'\\b(?:pollution|contamination|waste|plastic|emission)\\s+(?:level|reduction|management|problem|crisis)\\b',\n",
    "                r'\\b(?:ecosystem|biodiversity|habitat|species|wildlife)\\s+(?:protection|conservation|loss|threatened|endangered)\\b'\n",
    "            ],\n",
    "            \n",
    "            # WORLD NEWS\n",
    "            \"world\": [\n",
    "                r'\\b(?:country|nation|government|state|regime)\\s+(?:announced|declared|imposed|condemned|responded)\\b',\n",
    "                r'\\b(?:international|global|diplomatic|bilateral|multilateral)\\s+(?:relation|agreement|cooperation|tension|conflict)\\b',\n",
    "                r'\\b(?:foreign|diplomatic|international|global|geopolitical)\\s+(?:policy|affair|crisis|conflict|tension)\\b',\n",
    "                r'\\b(?:war|conflict|crisis|tension|dispute)\\s+(?:between|involving|escalated|resolved|ongoing)\\b',\n",
    "                r'\\b(?:UN|EU|NATO|WHO|World Bank)\\s+(?:resolution|decision|statement|report|meeting|summit)\\b'\n",
    "            ],\n",
    "            \n",
    "            # EDUCATION\n",
    "            \"education\": [\n",
    "                r'\\b(?:student|teacher|professor|educator|faculty)\\s+(?:learning|teaching|performance|achievement|development)\\b',\n",
    "                r'\\b(?:school|university|college|campus|institution)\\s+(?:program|course|curriculum|degree|education)\\b',\n",
    "                r'\\b(?:education|educational|academic|learning|teaching)\\s+(?:system|policy|reform|standard|quality)\\b',\n",
    "                r'\\b(?:classroom|course|curriculum|program|instruction)\\s+(?:design|development|implementation|assessment|evaluation)\\b',\n",
    "                r'\\b(?:test|exam|assessment|evaluation|grade)\\s+(?:score|result|performance|standard|improvement)\\b'\n",
    "            ],\n",
    "            \n",
    "            # FOOD\n",
    "            \"food\": [\n",
    "                r'\\b(?:recipe|dish|meal|cuisine|food)\\s+(?:preparation|cooking|serving|tasting|pairing)\\b',\n",
    "                r'\\b(?:ingredient|spice|herb|seasoning|flavor)\\s+(?:fresh|dried|ground|chopped|minced|mixed)\\b',\n",
    "                r'\\b(?:cook|bake|roast|grill|fry|simmer|boil)\\s+(?:until|for|over|with|in)\\s+(?:\\d+|low|medium|high)\\b',\n",
    "                r'\\b(?:restaurant|chef|kitchen|dining|culinary)\\s+(?:experience|scene|trend|technique|tradition)\\b',\n",
    "                r'\\b(?:cup|tablespoon|teaspoon|ounce|pound|gram)\\s+(?:of)?\\s+(?:\\w+)\\s+(?:chopped|minced|diced|sliced|grated)\\b'\n",
    "            ],\n",
    "            \n",
    "            # TRAVEL\n",
    "            \"travel\": [\n",
    "                r'\\b(?:travel|trip|journey|vacation|holiday)\\s+(?:destination|experience|planning|booking|package)\\b',\n",
    "                r'\\b(?:hotel|resort|accommodation|lodging|stay)\\s+(?:luxury|budget|boutique|all-inclusive|reservation)\\b',\n",
    "                r'\\b(?:flight|airline|airport|airfare|ticket)\\s+(?:booking|reservation|cancellation|delay|schedule)\\b',\n",
    "                r'\\b(?:tourist|traveler|visitor|guest|passenger)\\s+(?:attraction|experience|guide|visa|destination)\\b',\n",
    "                r'\\b(?:destination|location|place|spot|attraction)\\s+(?:popular|hidden|must-see|off-the-beaten-path|bucket-list)\\b'\n",
    "            ],\n",
    "            \n",
    "            # AUTOMOTIVE\n",
    "            \"automotive\": [\n",
    "                r'\\b(?:car|vehicle|automobile|model|SUV|sedan|truck)\\s+(?:manufacturer|brand|company|maker|production)\\b',\n",
    "                r'\\b(?:engine|motor|powertrain|transmission|drivetrain)\\s+(?:performance|power|efficiency|technology|system)\\b',\n",
    "                r'\\b(?:electric|hybrid|gas|diesel|fuel)\\s+(?:vehicle|car|model|engine|efficiency|economy)\\b',\n",
    "                r'\\b(?:safety|performance|efficiency|reliability|technology)\\s+(?:feature|system|rating|standard|improvement)\\b',\n",
    "                r'\\b(?:driving|driver|ride|handling|comfort)\\s+(?:experience|assistance|mode|quality|technology)\\b'\n",
    "            ],\n",
    "            \n",
    "            # REAL ESTATE\n",
    "            \"real estate\": [\n",
    "                r'\\b(?:home|house|property|real estate|housing)\\s+(?:market|price|value|sale|purchase)\\b',\n",
    "                r'\\b(?:buyer|seller|agent|broker|realtor)\\s+(?:market|offer|negotiation|commission|representation)\\b',\n",
    "                r'\\b(?:mortgage|loan|financing|interest rate|down payment)\\s+(?:approval|application|term|rate|option)\\b',\n",
    "                r'\\b(?:listing|sale|purchase|transaction|closing)\\s+(?:price|agreement|process|date|cost)\\b',\n",
    "                r'\\b(?:commercial|residential|industrial|retail|office)\\s+(?:property|space|building|development|market)\\b'\n",
    "            ],\n",
    "            \n",
    "            # CYBERSECURITY\n",
    "            \"cybersecurity\": [\n",
    "                r'\\b(?:cyber|security|data|network|system)\\s+(?:attack|breach|threat|vulnerability|protection)\\b',\n",
    "                r'\\b(?:hacker|attacker|threat actor|cybercriminal|adversary)\\s+(?:targeted|compromised|exploited|accessed|stole)\\b',\n",
    "                r'\\b(?:malware|ransomware|virus|trojan|spyware)\\s+(?:attack|infection|detection|prevention|removal)\\b',\n",
    "                r'\\b(?:password|authentication|encryption|firewall|VPN)\\s+(?:protection|security|system|solution|technology)\\b',\n",
    "                r'\\b(?:data|information|system|network|infrastructure)\\s+(?:protection|security|privacy|breach|vulnerability)\\b'\n",
    "            ],\n",
    "            \n",
    "            # ARTIFICIAL INTELLIGENCE\n",
    "            \"artificial intelligence\": [\n",
    "                r'\\b(?:artificial intelligence|AI|machine learning|ML|deep learning)\\s+(?:model|system|algorithm|application|technology)\\b',\n",
    "                r'\\b(?:neural network|algorithm|model|system|framework)\\s+(?:training|inference|prediction|classification|performance)\\b',\n",
    "                r'\\b(?:data|training data|dataset|input|output)\\s+(?:processing|analysis|preparation|augmentation|validation)\\b',\n",
    "                r'\\b(?:natural language processing|NLP|computer vision|CV|speech recognition)\\s+(?:model|system|application|technology)\\b',\n",
    "                r'\\b(?:AI|artificial intelligence)\\s+(?:ethics|bias|fairness|transparency|accountability|regulation)\\b'\n",
    "            ],\n",
    "            \n",
    "            # SPACE\n",
    "            \"space\": [\n",
    "                r'\\b(?:space|mission|launch|rocket|spacecraft)\\s+(?:exploration|program|agency|company|technology)\\b',\n",
    "                r'\\b(?:NASA|SpaceX|ESA|Roscosmos|Blue Origin)\\s+(?:mission|launch|program|spacecraft|astronaut)\\b',\n",
    "                r'\\b(?:planet|moon|Mars|Jupiter|Saturn|asteroid|comet)\\s+(?:exploration|mission|surface|atmosphere|orbit)\\b',\n",
    "                r'\\b(?:astronaut|cosmonaut|space traveler|crew|commander)\\s+(?:mission|aboard|spacewalk|returned|launched)\\b',\n",
    "                r'\\b(?:telescope|observatory|satellite|probe|rover)\\s+(?:image|data|observation|discovery|exploration)\\b'\n",
    "            ],\n",
    "            \n",
    "            # AGRICULTURE\n",
    "            \"agriculture\": [\n",
    "                r'\\b(?:farm|farming|agricultural|crop|livestock)\\s+(?:production|yield|management|practice|system)\\b',\n",
    "                r'\\b(?:farmer|grower|producer|rancher|breeder)\\s+(?:growing|producing|harvesting|raising|breeding)\\b',\n",
    "                r'\\b(?:crop|harvest|yield|production|cultivation)\\s+(?:season|rotation|protection|insurance|subsidy)\\b',\n",
    "                r'\\b(?:soil|water|nutrient|fertilizer|pesticide)\\s+(?:management|conservation|quality|application|runoff)\\b',\n",
    "                r'\\b(?:sustainable|organic|conventional|precision|regenerative)\\s+(?:agriculture|farming|practice|method|technique)\\b'\n",
    "            ],\n",
    "            \n",
    "            # MENTAL HEALTH\n",
    "            \"mental health\": [\n",
    "                r'\\b(?:mental health|psychological|psychiatric|emotional|behavioral)\\s+(?:condition|disorder|illness|wellbeing|treatment)\\b',\n",
    "                r'\\b(?:therapy|counseling|treatment|intervention|support)\\s+(?:for|of)\\s+(?:mental|psychological|emotional|behavioral)\\b',\n",
    "                r'\\b(?:depression|anxiety|trauma|PTSD|bipolar|schizophrenia)\\s+(?:disorder|symptoms|diagnosis|treatment|therapy)\\b',\n",
    "                r'\\b(?:psychiatrist|psychologist|therapist|counselor|mental health professional)\\s+(?:treatment|diagnosis|therapy|practice|approach)\\b',\n",
    "                r'\\b(?:mental health|psychological|emotional)\\s+(?:awareness|stigma|support|service|resource|crisis)\\b'\n",
    "            ]\n",
    "        }\n",
    "        self.domain_entities = {\n",
    "            \"technology\": [\n",
    "                \"Apple\", \"Google\", \"Microsoft\", \"Amazon\", \"Facebook\", \"Meta\", \"Tesla\", \"Intel\", \"AMD\", \"NVIDIA\",\n",
    "                \"IBM\", \"Oracle\", \"Cisco\", \"Samsung\", \"Huawei\", \"TikTok\", \"Twitter\", \"LinkedIn\", \"GitHub\", \"Stack Overflow\",\n",
    "                \"AI\", \"ML\", \"API\", \"IoT\", \"5G\", \"Cloud\", \"SaaS\", \"DevOps\", \"Blockchain\", \"Cryptocurrency\"\n",
    "            ],\n",
    "            \n",
    "            # BUSINESS\n",
    "            \"business\": [\n",
    "                \"Wall Street\", \"NYSE\", \"NASDAQ\", \"Dow Jones\", \"S&P 500\", \"Fortune 500\", \"Forbes\", \"Bloomberg\", \"CNBC\", \"Financial Times\",\n",
    "                \"CEO\", \"CFO\", \"COO\", \"CTO\", \"IPO\", \"M&A\", \"ROI\", \"B2B\", \"B2C\", \"GDP\",\n",
    "                \"Federal Reserve\", \"Treasury\", \"SEC\", \"IMF\", \"World Bank\", \"WTO\", \"OPEC\", \"Berkshire Hathaway\", \"JPMorgan\", \"Goldman Sachs\"\n",
    "            ],\n",
    "            \n",
    "            # HEALTH\n",
    "            \"health\": [\n",
    "                \"WHO\", \"CDC\", \"FDA\", \"NIH\", \"AMA\", \"NHS\", \"Mayo Clinic\", \"Cleveland Clinic\", \"Johns Hopkins\", \"WebMD\",\n",
    "                \"COVID-19\", \"Coronavirus\", \"Pandemic\", \"Vaccine\", \"Pfizer\", \"Moderna\", \"Johnson & Johnson\", \"Merck\", \"Novartis\", \"Roche\",\n",
    "                \"Medicare\", \"Medicaid\", \"Affordable Care Act\", \"Obamacare\", \"Health Insurance\", \"Big Pharma\", \"Mental Health\", \"Wellness\", \"Fitness\", \"Nutrition\"\n",
    "            ],\n",
    "            \n",
    "            # POLITICS\n",
    "            \"politics\": [\n",
    "                \"White House\", \"Congress\", \"Senate\", \"House of Representatives\", \"Supreme Court\", \"Capitol Hill\", \"Pentagon\", \"State Department\", \"United Nations\", \"NATO\",\n",
    "                \"Democrat\", \"Republican\", \"GOP\", \"Liberal\", \"Conservative\", \"Progressive\", \"Election\", \"Campaign\", \"Poll\", \"Ballot\",\n",
    "                \"President\", \"Vice President\", \"Secretary of State\", \"Attorney General\", \"Speaker of the House\", \"Majority Leader\", \"Minority Leader\", \"Filibuster\", \"Gerrymander\", \"Lobbyist\"\n",
    "            ],\n",
    "            \n",
    "            # SPORTS\n",
    "            \"sports\": [\n",
    "                \"NFL\", \"NBA\", \"MLB\", \"NHL\", \"MLS\", \"FIFA\", \"UEFA\", \"Olympics\", \"Super Bowl\", \"World Cup\",\n",
    "                \"ESPN\", \"Sports Illustrated\", \"Bleacher Report\", \"Draft\", \"Free Agency\", \"All-Star\", \"MVP\", \"Championship\", \"Playoff\", \"Tournament\",\n",
    "                \"Manchester United\", \"Real Madrid\", \"Barcelona\", \"Lakers\", \"Yankees\", \"Cowboys\", \"Patriots\", \"LeBron James\", \"Tom Brady\", \"Serena Williams\"\n",
    "            ],\n",
    "            \n",
    "            # ENTERTAINMENT\n",
    "            \"entertainment\": [\n",
    "                \"Hollywood\", \"Netflix\", \"Disney\", \"HBO\", \"Warner Bros\", \"Universal\", \"Paramount\", \"Sony Pictures\", \"Marvel\", \"DC\",\n",
    "                \"Oscar\", \"Emmy\", \"Grammy\", \"Golden Globe\", \"Cannes\", \"Sundance\", \"Billboard\", \"Box Office\", \"Streaming\", \"Prime Time\",\n",
    "                \"Celebrity\", \"Actor\", \"Actress\", \"Director\", \"Producer\", \"Star\", \"Movie\", \"Film\", \"TV Show\", \"Series\"\n",
    "            ],\n",
    "            \n",
    "            # SCIENCE\n",
    "            \"science\": [\n",
    "                \"NASA\", \"CERN\", \"NIH\", \"NSF\", \"Nature\", \"Science\", \"Scientific American\", \"AAAS\", \"Royal Society\", \"Max Planck Institute\",\n",
    "                \"Physics\", \"Chemistry\", \"Biology\", \"Astronomy\", \"Geology\", \"Neuroscience\", \"Genetics\", \"Quantum\", \"Particle\", \"Molecule\",\n",
    "                \"Research\", \"Experiment\", \"Laboratory\", \"Hypothesis\", \"Theory\", \"Peer Review\", \"Publication\", \"Journal\", \"Conference\", \"Symposium\"\n",
    "            ],\n",
    "            \n",
    "            # ENVIRONMENT\n",
    "            \"environment\": [\n",
    "                \"EPA\", \"IPCC\", \"Greenpeace\", \"Sierra Club\", \"WWF\", \"UNEP\", \"National Geographic\", \"Climate Change\", \"Global Warming\", \"Carbon Footprint\",\n",
    "                \"Renewable Energy\", \"Solar Power\", \"Wind Power\", \"Fossil Fuel\", \"Greenhouse Gas\", \"Emission\", \"Pollution\", \"Conservation\", \"Sustainability\", \"Biodiversity\",\n",
    "                \"Paris Agreement\", \"Kyoto Protocol\", \"COP26\", \"Green New Deal\", \"Carbon Tax\", \"Cap and Trade\", \"Recycling\", \"Plastic Waste\", \"Deforestation\", \"Endangered Species\"\n",
    "            ],\n",
    "            \n",
    "            # WORLD NEWS\n",
    "            \"world\": [\n",
    "                \"United Nations\", \"European Union\", \"NATO\", \"G7\", \"G20\", \"World Bank\", \"IMF\", \"WHO\", \"WTO\", \"OPEC\",\n",
    "                \"Foreign Policy\", \"Diplomacy\", \"International Relations\", \"Global Affairs\", \"Geopolitics\", \"Embassy\", \"Ambassador\", \"Treaty\", \"Summit\", \"Bilateral\",\n",
    "                \"BBC World\", \"CNN International\", \"Al Jazeera\", \"Reuters\", \"Associated Press\", \"AFP\", \"Foreign Correspondent\", \"International\", \"Global\", \"Worldwide\"\n",
    "            ],\n",
    "            \n",
    "            # EDUCATION\n",
    "            \"education\": [\n",
    "                \"Department of Education\", \"Board of Education\", \"School District\", \"University\", \"College\", \"Campus\", \"Faculty\", \"Student\", \"Teacher\", \"Professor\",\n",
    "                \"Curriculum\", \"Syllabus\", \"Course\", \"Degree\", \"Diploma\", \"Certificate\", \"Accreditation\", \"SAT\", \"ACT\", \"GRE\",\n",
    "                \"Harvard\", \"Stanford\", \"MIT\", \"Oxford\", \"Cambridge\", \"Yale\", \"Princeton\", \"Berkeley\", \"Public School\", \"Private School\"\n",
    "            ],\n",
    "            \n",
    "            # FOOD\n",
    "            \"food\": [\n",
    "                \"Food Network\", \"Bon Appétit\", \"Epicurious\", \"Michelin\", \"James Beard\", \"Zagat\", \"Yelp\", \"TripAdvisor\", \"OpenTable\", \"DoorDash\",\n",
    "                \"Chef\", \"Restaurant\", \"Cuisine\", \"Recipe\", \"Ingredient\", \"Menu\", \"Dish\", \"Meal\", \"Cooking\", \"Baking\",\n",
    "                \"Vegetarian\", \"Vegan\", \"Gluten-Free\", \"Organic\", \"Farm-to-Table\", \"Sustainable\", \"Foodie\", \"Culinary\", \"Gourmet\", \"Gastronomy\"\n",
    "            ],\n",
    "            \n",
    "            # TRAVEL\n",
    "            \"travel\": [\n",
    "                \"Expedia\", \"Booking.com\", \"Airbnb\", \"TripAdvisor\", \"Lonely Planet\", \"Travel + Leisure\", \"Condé Nast Traveler\", \"National Geographic Traveler\", \"Rick Steves\", \"Fodor's\",\n",
    "                \"Hotel\", \"Resort\", \"Accommodation\", \"Flight\", \"Airline\", \"Airport\", \"Cruise\", \"Tour\", \"Vacation\", \"Holiday\",\n",
    "                \"Destination\", \"Tourism\", \"Tourist\", \"Traveler\", \"Passport\", \"Visa\", \"Itinerary\", \"Excursion\", \"Adventure\", \"Sightseeing\"\n",
    "            ],\n",
    "            \n",
    "            # AUTOMOTIVE\n",
    "            \"automotive\": [\n",
    "                \"Toyota\", \"Volkswagen\", \"Ford\", \"GM\", \"Tesla\", \"BMW\", \"Mercedes-Benz\", \"Honda\", \"Hyundai\", \"Kia\",\n",
    "                \"Car and Driver\", \"Motor Trend\", \"Automotive News\", \"Kelley Blue Book\", \"Edmunds\", \"AutoTrader\", \"J.D. Power\", \"NHTSA\", \"EPA\", \"IIHS\",\n",
    "                \"Electric Vehicle\", \"Hybrid\", \"SUV\", \"Sedan\", \"Truck\", \"Autonomous\", \"Self-Driving\", \"Horsepower\", \"Torque\", \"MPG\"\n",
    "            ],\n",
    "            \n",
    "            # REAL ESTATE\n",
    "            \"real estate\": [\n",
    "                \"Zillow\", \"Redfin\", \"Realtor.com\", \"Trulia\", \"Century 21\", \"RE/MAX\", \"Keller Williams\", \"Coldwell Banker\", \"Sotheby's\", \"Berkshire Hathaway HomeServices\",\n",
    "                \"MLS\", \"NAR\", \"NAHB\", \"Fannie Mae\", \"Freddie Mac\", \"HUD\", \"FHA\", \"VA Loan\", \"Mortgage\", \"Down Payment\",\n",
    "                \"Real Estate Agent\", \"Broker\", \"Realtor\", \"Property\", \"Home\", \"House\", \"Condo\", \"Apartment\", \"Commercial\", \"Residential\"\n",
    "            ],\n",
    "            \n",
    "            # CYBERSECURITY\n",
    "            \"cybersecurity\": [\n",
    "                \"CISA\", \"NSA\", \"FBI Cyber Division\", \"Interpol\", \"Europol\", \"NIST\", \"ISO\", \"SANS Institute\", \"Black Hat\", \"DEF CON\",\n",
    "                \"Firewall\", \"Antivirus\", \"VPN\", \"Encryption\", \"Authentication\", \"Phishing\", \"Malware\", \"Ransomware\", \"DDoS\", \"Zero-day\",\n",
    "                \"Cyber Attack\", \"Data Breach\", \"Vulnerability\", \"Patch\", \"Security\", \"Privacy\", \"Hacker\", \"Threat Actor\", \"Penetration Testing\", \"Security Operations Center\"\n",
    "            ],\n",
    "            \n",
    "            # ARTIFICIAL INTELLIGENCE\n",
    "            \"artificial intelligence\": [\n",
    "                \"OpenAI\", \"DeepMind\", \"NVIDIA AI\", \"IBM Watson\", \"Google AI\", \"Microsoft AI\", \"Meta AI\", \"Anthropic\", \"Hugging Face\", \"PyTorch\",\n",
    "                \"TensorFlow\", \"GPT\", \"BERT\", \"DALL-E\", \"Stable Diffusion\", \"Midjourney\", \"ChatGPT\", \"LLM\", \"Neural Network\", \"Deep Learning\",\n",
    "                \"Machine Learning\", \"NLP\", \"Computer Vision\", \"Reinforcement Learning\", \"AI Ethics\", \"AGI\", \"Transformer\", \"Diffusion Model\", \"Generative AI\", \"AI Alignment\"\n",
    "            ],\n",
    "            \n",
    "            # SPACE\n",
    "            \"space\": [\n",
    "                \"NASA\", \"SpaceX\", \"Blue Origin\", \"ESA\", \"Roscosmos\", \"ISRO\", \"JAXA\", \"Virgin Galactic\", \"Rocket Lab\", \"ULA\",\n",
    "                \"ISS\", \"Hubble\", \"James Webb\", \"Perseverance\", \"Curiosity\", \"Artemis\", \"Apollo\", \"Falcon 9\", \"Starship\", \"SLS\",\n",
    "                \"Mars\", \"Moon\", \"Jupiter\", \"Saturn\", \"Asteroid\", \"Comet\", \"Galaxy\", \"Exoplanet\", \"Black Hole\", \"Nebula\"\n",
    "            ],\n",
    "            \n",
    "            # AGRICULTURE\n",
    "            \"agriculture\": [\n",
    "                \"USDA\", \"FAO\", \"Farm Bureau\", \"4-H\", \"FFA\", \"Land Grant University\", \"Cooperative Extension\", \"Monsanto\", \"Bayer\", \"John Deere\",\n",
    "                \"Farm\", \"Crop\", \"Livestock\", \"Harvest\", \"Irrigation\", \"Fertilizer\", \"Pesticide\", \"GMO\", \"Organic\", \"Sustainable\",\n",
    "                \"Corn\", \"Wheat\", \"Soybean\", \"Rice\", \"Cotton\", \"Cattle\", \"Dairy\", \"Poultry\", \"Farm Bill\", \"Subsidy\"\n",
    "            ],\n",
    "            \n",
    "            # MENTAL HEALTH\n",
    "            \"mental health\": [\n",
    "                \"NIMH\", \"APA\", \"WHO Mental Health\", \"NAMI\", \"SAMHSA\", \"Mental Health America\", \"Psychology Today\", \"Crisis Text Line\", \"National Suicide Prevention Lifeline\", \"988\",\n",
    "                \"Depression\", \"Anxiety\", \"PTSD\", \"Bipolar\", \"Schizophrenia\", \"OCD\", \"ADHD\", \"Autism\", \"Therapy\", \"Counseling\",\n",
    "                \"Psychiatrist\", \"Psychologist\", \"Therapist\", \"Mental Illness\", \"Mental Wellbeing\", \"Stigma\", \"Awareness\", \"Self-Care\", \"Mindfulness\", \"Resilience\"\n",
    "            ]\n",
    "        }\n",
    "        self.domain_term_thresholds = {\n",
    "            \"technology\": 4,\n",
    "            \"business\": 4,\n",
    "            \"health\": 4,\n",
    "            \"politics\": 4,\n",
    "            \"sports\": 3,\n",
    "            \"entertainment\": 3,\n",
    "            \"science\": 4,\n",
    "            \"environment\": 3,\n",
    "            \"world\": 4,\n",
    "            \"education\": 4,\n",
    "            \"food\": 3,\n",
    "            \"travel\": 3,\n",
    "            \"automotive\": 3,\n",
    "            \"real estate\": 3,\n",
    "            \"cybersecurity\": 3,\n",
    "            \"artificial intelligence\": 3,\n",
    "            \"space\": 3,\n",
    "            \"agriculture\": 3,\n",
    "            \"mental health\": 3\n",
    "        }\n",
    "        self.domain_terminology = {\n",
    "            # TECHNOLOGY\n",
    "            \"technology\": [\n",
    "                \"technology\", \"tech\", \"digital\", \"software\", \"hardware\", \"app\", \"device\", \"computer\", \"mobile\", \"internet\",\n",
    "                \"data\", \"cloud\", \"AI\", \"algorithm\", \"code\", \"programming\", \"developer\", \"platform\", \"system\", \"network\",\n",
    "                \"security\", \"cyber\", \"innovation\", \"startup\", \"gadget\", \"smart\", \"virtual\", \"interface\", \"user experience\", \"automation\"\n",
    "            ],\n",
    "            \n",
    "            # BUSINESS\n",
    "            \"business\": [\n",
    "                \"business\", \"company\", \"market\", \"industry\", \"corporate\", \"firm\", \"enterprise\", \"startup\", \"revenue\", \"profit\",\n",
    "                \"investment\", \"investor\", \"shareholder\", \"stock\", \"finance\", \"financial\", \"economy\", \"economic\", \"CEO\", \"executive\",\n",
    "                \"management\", \"strategy\", \"growth\", \"expansion\", \"acquisition\", \"merger\", \"partnership\", \"client\", \"customer\", \"consumer\"\n",
    "            ],\n",
    "            \n",
    "            # HEALTH\n",
    "            \"health\": [\n",
    "                \"health\", \"medical\", \"healthcare\", \"patient\", \"doctor\", \"hospital\", \"clinic\", \"treatment\", \"therapy\", \"medication\",\n",
    "                \"disease\", \"condition\", \"symptom\", \"diagnosis\", \"prevention\", \"wellness\", \"care\", \"specialist\", \"surgery\", \"prescription\",\n",
    "                \"drug\", \"pharmaceutical\", \"clinical\", \"research\", \"study\", \"trial\", \"vaccine\", \"immunity\", \"chronic\", \"acute\"\n",
    "            ],\n",
    "            \n",
    "            # POLITICS\n",
    "            \"politics\": [\n",
    "                \"politics\", \"political\", \"government\", \"policy\", \"election\", \"campaign\", \"vote\", \"voter\", \"candidate\", \"president\",\n",
    "                \"congress\", \"senate\", \"representative\", \"democrat\", \"republican\", \"liberal\", \"conservative\", \"legislation\", \"law\", \"bill\",\n",
    "                \"administration\", \"official\", \"diplomat\", \"foreign policy\", \"domestic policy\", \"regulation\", \"reform\", \"party\", \"poll\", \"approval rating\"\n",
    "            ],\n",
    "            \n",
    "            # SPORTS\n",
    "            \"sports\": [\n",
    "                \"sports\", \"game\", \"team\", \"player\", \"coach\", \"athlete\", \"championship\", \"tournament\", \"league\", \"season\",\n",
    "                \"win\", \"loss\", \"score\", \"point\", \"goal\", \"match\", \"competition\", \"stadium\", \"arena\", \"fan\",\n",
    "                \"draft\", \"trade\", \"contract\", \"injury\", \"performance\", \"record\", \"title\", \"MVP\", \"all-star\", \"playoff\"\n",
    "            ],\n",
    "            \n",
    "            # ENTERTAINMENT\n",
    "            \"entertainment\": [\n",
    "                \"entertainment\", \"movie\", \"film\", \"show\", \"series\", \"episode\", \"actor\", \"actress\", \"director\", \"producer\",\n",
    "                \"celebrity\", \"star\", \"Hollywood\", \"box office\", \"streaming\", \"music\", \"song\", \"album\", \"artist\", \"band\",\n",
    "                \"concert\", \"tour\", \"performance\", \"award\", \"critic\", \"review\", \"rating\", \"audience\", \"fan\", \"premiere\"\n",
    "            ],\n",
    "            \n",
    "            # SCIENCE\n",
    "            \"science\": [\n",
    "                \"science\", \"scientific\", \"research\", \"study\", \"experiment\", \"laboratory\", \"scientist\", \"researcher\", \"discovery\", \"innovation\",\n",
    "                \"theory\", \"hypothesis\", \"data\", \"analysis\", \"evidence\", \"observation\", \"physics\", \"chemistry\", \"biology\", \"astronomy\",\n",
    "                \"genetics\", \"molecule\", \"atom\", \"particle\", \"cell\", \"organism\", \"species\", \"evolution\", \"ecosystem\", \"climate\"\n",
    "            ],\n",
    "            \n",
    "            # ENVIRONMENT\n",
    "            \"environment\": [\n",
    "                \"environment\", \"environmental\", \"climate\", \"climate change\", \"global warming\", \"carbon\", \"emission\", \"pollution\", \"renewable\", \"sustainable\",\n",
    "                \"conservation\", \"ecosystem\", \"biodiversity\", \"species\", \"habitat\", \"wildlife\", \"forest\", \"ocean\", \"water\", \"air\",\n",
    "                \"energy\", \"solar\", \"wind\", \"fossil fuel\", \"green\", \"recycling\", \"waste\", \"plastic\", \"protection\", \"preservation\"\n",
    "            ],\n",
    "            \n",
    "            # WORLD NEWS\n",
    "            \"world\": [\n",
    "                \"international\", \"global\", \"world\", \"foreign\", \"country\", \"nation\", \"government\", \"leader\", \"president\", \"prime minister\",\n",
    "                \"diplomacy\", \"diplomatic\", \"relations\", \"treaty\", \"agreement\", \"conflict\", \"crisis\", \"war\", \"peace\", \"security\",\n",
    "                \"trade\", \"economy\", \"development\", \"aid\", \"refugee\", \"immigration\", \"border\", \"sanction\", \"alliance\", \"summit\"\n",
    "            ],\n",
    "            \n",
    "            # EDUCATION\n",
    "            \"education\": [\n",
    "                            # EDUCATION (continued)\n",
    "                \"education\", \"school\", \"university\", \"college\", \"student\", \"teacher\", \"professor\", \"classroom\", \"campus\", \"learning\",\n",
    "                \"teaching\", \"academic\", \"curriculum\", \"course\", \"degree\", \"program\", \"study\", \"research\", \"grade\", \"exam\",\n",
    "                \"test\", \"assignment\", \"scholarship\", \"tuition\", \"faculty\", \"administration\", \"board\", \"district\", \"literacy\", \"STEM\"\n",
    "            ],\n",
    "            \n",
    "            # FOOD\n",
    "            \"food\": [\n",
    "                \"food\", \"recipe\", \"cooking\", \"baking\", \"chef\", \"restaurant\", \"cuisine\", \"dish\", \"meal\", \"ingredient\",\n",
    "                \"flavor\", \"taste\", \"delicious\", \"kitchen\", \"dining\", \"menu\", \"appetizer\", \"entree\", \"dessert\", \"beverage\",\n",
    "                \"vegetarian\", \"vegan\", \"organic\", \"fresh\", \"local\", \"seasonal\", \"gourmet\", \"culinary\", \"nutritious\", \"homemade\"\n",
    "            ],\n",
    "            \n",
    "            # TRAVEL\n",
    "            \"travel\": [\n",
    "                \"travel\", \"trip\", \"vacation\", \"holiday\", \"destination\", \"tourism\", \"tourist\", \"hotel\", \"resort\", \"accommodation\",\n",
    "                \"flight\", \"airline\", \"airport\", \"cruise\", \"tour\", \"adventure\", \"experience\", \"sightseeing\", \"attraction\", \"landmark\",\n",
    "                \"international\", \"domestic\", \"passport\", \"visa\", \"itinerary\", \"booking\", \"reservation\", \"guide\", \"excursion\", \"journey\"\n",
    "            ],\n",
    "            \n",
    "            # AUTOMOTIVE\n",
    "            \"automotive\": [\n",
    "                \"car\", \"vehicle\", \"automobile\", \"automotive\", \"driver\", \"driving\", \"engine\", \"motor\", \"transmission\", \"model\",\n",
    "                \"brand\", \"manufacturer\", \"dealership\", \"sedan\", \"SUV\", \"truck\", \"electric\", \"hybrid\", \"gas\", \"diesel\",\n",
    "                \"horsepower\", \"torque\", \"performance\", \"safety\", \"technology\", \"feature\", \"design\", \"interior\", \"exterior\", \"test drive\"\n",
    "            ],\n",
    "            \n",
    "            # REAL ESTATE\n",
    "            \"real estate\": [\n",
    "                \"real estate\", \"property\", \"home\", \"house\", \"apartment\", \"condo\", \"townhouse\", \"residential\", \"commercial\", \"industrial\",\n",
    "                \"buyer\", \"seller\", \"agent\", \"broker\", \"market\", \"listing\", \"sale\", \"purchase\", \"mortgage\", \"loan\",\n",
    "                \"interest rate\", \"down payment\", \"closing\", \"inspection\", \"appraisal\", \"investment\", \"rental\", \"landlord\", \"tenant\", \"development\"\n",
    "            ],\n",
    "            \n",
    "            # CYBERSECURITY\n",
    "            \"cybersecurity\": [\n",
    "                \"cybersecurity\", \"security\", \"cyber\", \"hack\", \"breach\", \"attack\", \"threat\", \"vulnerability\", \"malware\", \"ransomware\",\n",
    "                \"phishing\", \"data\", \"protection\", \"encryption\", \"firewall\", \"authentication\", \"password\", \"privacy\", \"risk\", \"compliance\",\n",
    "                \"defense\", \"detection\", \"response\", \"prevention\", \"incident\", \"forensics\", \"patch\", \"update\", \"secure\", \"compromise\"\n",
    "            ],\n",
    "            \n",
    "            # ARTIFICIAL INTELLIGENCE\n",
    "            \"artificial intelligence\": [\n",
    "                \"artificial intelligence\", \"AI\", \"machine learning\", \"ML\", \"deep learning\", \"neural network\", \"algorithm\", \"model\", \"training\", \"data\",\n",
    "                \"prediction\", \"classification\", \"recognition\", \"natural language processing\", \"NLP\", \"computer vision\", \"automation\", \"robotics\", \"intelligent\", \"smart\",\n",
    "                \"GPT\", \"transformer\", \"supervised\", \"unsupervised\", \"reinforcement\", \"dataset\", \"feature\", \"parameter\", \"inference\", \"generative\"\n",
    "            ],\n",
    "            \n",
    "            # SPACE\n",
    "            \"space\": [\n",
    "                \"space\", \"astronomy\", \"cosmos\", \"universe\", \"galaxy\", \"star\", \"planet\", \"moon\", \"Mars\", \"solar system\",\n",
    "                \"rocket\", \"spacecraft\", \"satellite\", \"telescope\", \"astronaut\", \"mission\", \"launch\", \"orbit\", \"exploration\", \"discovery\",\n",
    "                \"NASA\", \"SpaceX\", \"astronomical\", \"celestial\", \"cosmic\", \"gravitational\", \"interstellar\", \"extraterrestrial\", \"rover\", \"probe\"\n",
    "            ],\n",
    "            \n",
    "            # AGRICULTURE\n",
    "            \"agriculture\": [\n",
    "                \"agriculture\", \"farming\", \"farm\", \"crop\", \"harvest\", \"livestock\", \"soil\", \"seed\", \"plant\", \"grow\",\n",
    "                \"farmer\", \"agricultural\", \"cultivation\", \"irrigation\", \"fertilizer\", \"pesticide\", \"organic\", \"sustainable\", \"yield\", \"production\",\n",
    "                \"rural\", \"field\", \"pasture\", \"grain\", \"dairy\", \"cattle\", \"poultry\", \"horticulture\", \"agribusiness\", \"food production\"\n",
    "            ],\n",
    "            \n",
    "            # MENTAL HEALTH\n",
    "            \"mental health\": [\n",
    "                \"mental health\", \"psychological\", \"emotional\", \"behavioral\", \"psychiatric\", \"therapy\", \"counseling\", \"depression\", \"anxiety\", \"stress\",\n",
    "                \"trauma\", \"disorder\", \"condition\", \"treatment\", \"support\", \"wellbeing\", \"wellness\", \"coping\", \"resilience\", \"mindfulness\",\n",
    "                \"psychiatrist\", \"psychologist\", \"therapist\", \"counselor\", \"diagnosis\", \"symptom\", \"recovery\", \"self-care\", \"awareness\", \"stigma\"\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    def detect_domain(self, text: str) -> Tuple[Optional[str], float]:\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "       \n",
    "        pattern_matches = {}\n",
    "        for domain, patterns in self.domain_patterns.items():\n",
    "            pattern_matches[domain] = 0\n",
    "            for pattern in patterns:\n",
    "                matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "                pattern_matches[domain] += len(matches)\n",
    "        \n",
    "     \n",
    "        entity_matches = {}\n",
    "        for domain, entities in self.domain_entities.items():\n",
    "            entity_matches[domain] = 0\n",
    "            for entity in entities:\n",
    "                if entity.lower() in text_lower:\n",
    "                    entity_matches[domain] += 1\n",
    "                    \n",
    "        \n",
    "        term_matches = {}\n",
    "        for domain, terms in self.domain_terminology.items():\n",
    "            term_matches[domain] = 0\n",
    "            for term in terms:\n",
    "                if term.lower() in text_lower:\n",
    "                    term_matches[domain] += 1\n",
    "        \n",
    "        \n",
    "        domain_scores = {}\n",
    "        for domain in self.domain_patterns.keys():\n",
    "            \n",
    "            pattern_score = min(1.0, pattern_matches[domain] / 3) * 0.6\n",
    "            \n",
    "            \n",
    "            entity_score = min(1.0, entity_matches[domain] / 2) * 0.3\n",
    "            \n",
    "           \n",
    "            term_threshold = self.domain_term_thresholds.get(domain, 4)\n",
    "            term_score = min(1.0, term_matches[domain] / term_threshold) * 0.1\n",
    "            \n",
    "        \n",
    "        \n",
    "        if domain_scores:\n",
    "            best_domain = max(domain_scores.items(), key=lambda x: x[1])\n",
    "            domain, score = best_domain\n",
    "            \n",
    "           \n",
    "           \n",
    "            top_domains = sorted(domain_scores.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "            print(f\"Top domain scores: {top_domains}\")\n",
    "            print(f\"Pattern matches for top domain: {pattern_matches[domain]}\")\n",
    "            print(f\"Entity matches for top domain: {entity_matches[domain]}\")\n",
    "            print(f\"Term matches for top domain: {term_matches[domain]}\")\n",
    "        e\n",
    "        return None, 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ab code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T06:27:37.841878Z",
     "iopub.status.busy": "2025-04-12T06:27:37.841618Z",
     "iopub.status.idle": "2025-04-12T06:27:38.255945Z",
     "shell.execute_reply": "2025-04-12T06:27:38.254959Z",
     "shell.execute_reply.started": "2025-04-12T06:27:37.841855Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import re\n",
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from typing import List, Dict, Tuple, Set, Optional, Union, Any\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import set_seed\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "import math\n",
    "import numpy as np\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from transformers import pipeline\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "try:\n",
    "    nltk.data.find('stopwords')\n",
    "    nltk.data.find('punkt')\n",
    "    nltk.data.find('wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('wordnet')\n",
    "\n",
    "class AbstractiveKeyphraseExtractor:\n",
    "    PROMPT_TEMPLATES = [\n",
    "        # Template 0: Comprehensive Few-Shot Prompt with Examples and Constraints\n",
    "        \"\"\"Extract as many relevant keyphrases as possible (1-5 words each) from this text.\n",
    "Focus on quantity - aim for at least 15-20 keyphrases covering all important concepts.\n",
    "Include both specific entities and broader topics.\n",
    "\n",
    "Text: {text}\n",
    "\n",
    "Keyphrases (comma-separated):\"\"\"\n",
    "\n",
    "        # Template 1: Focused Extraction with Clear Constraints\n",
    "        \"\"\"Extract all important keyphrases from this article. Each keyphrase must be 1-5 words long.\n",
    "\n",
    "    Return ONLY keyphrases separated by commas. Focus on specific concepts, entities, and technical terms that capture the essence of the article.\n",
    "\n",
    "    DO NOT include:\n",
    "    - Generic phrases like \"important development\" or \"significant impact\"\n",
    "    - Common reporting verbs like \"said\", \"announced\", \"reported\"\n",
    "    - Vague descriptors or general statements\n",
    "    - Full sentences or lengthy explanations\n",
    "\n",
    "    Example:\n",
    "    Article: Global stock markets fell sharply as inflation concerns and rising interest rates affected investor confidence. Central banks in several countries have signaled further monetary tightening to combat persistent inflation pressures.\n",
    "    Keyphrases: global stock markets, inflation concerns, rising interest rates, investor confidence, central banks, monetary tightening, inflation pressures\n",
    "\n",
    "    Article: {text}\n",
    "\n",
    "    Keyphrases:\"\"\",\n",
    "\n",
    "        # Template 2: Minimal Prompt with Clear Instructions\n",
    "        \"\"\"Keyphrases (1-5 words each, comma-separated):\n",
    "\n",
    "    Extract all key concepts, entities, and technical terms that best represent the main topics in the text.\n",
    "    Focus on nouns and specific terminology rather than actions or descriptions.\n",
    "\n",
    "    Example: quantum computing, climate change legislation, neural networks, supply chain disruption\n",
    "\n",
    "    Text: {text}\n",
    "    Keyphrases:\"\"\",\n",
    "\n",
    "        # Template 3: Domain-Aware Extraction with Semantic Focus\n",
    "        \"\"\"Identify all important concepts, entities, and terminology (1-5 words each) from this article.\n",
    "\n",
    "    Focus on extracting terms that:\n",
    "    - Represent specific ideas, technologies, or entities\n",
    "    - Would be useful for categorizing or searching for this content\n",
    "    - Capture the unique aspects of the subject matter\n",
    "    - Include proper nouns, technical terms, and domain-specific concepts\n",
    "\n",
    "    Avoid:\n",
    "    - Generic phrases and common words\n",
    "    - Vague descriptions or subjective statements\n",
    "    - Full sentences or lengthy explanations\n",
    "\n",
    "    List all keyphrases separated by commas.\n",
    "\n",
    "    Article: {text}\n",
    "    Keyphrases:\"\"\",\n",
    "\n",
    "        # Template 4: Comprehensive Extraction with Semantic Categories\n",
    "        \"\"\"Extract all significant keyphrases (1-5 words each) from this article, considering these categories:\n",
    "\n",
    "    1. Main topics and concepts\n",
    "    2. Named entities (people, organizations, locations)\n",
    "    3. Technical terminology and domain-specific terms\n",
    "    4. Key events, products, or innovations mentioned\n",
    "    5. Important attributes or characteristics\n",
    "\n",
    "    Provide only the keyphrases as a comma-separated list without category labels or explanations.\n",
    "\n",
    "    Article: {text}\n",
    "    Keyphrases:\"\"\"\n",
    "    ]\n",
    "\n",
    "    def __init__(\n",
    "    self,\n",
    "    model_name: str = \"google/flan-t5-large\",\n",
    "    use_gpu: bool = True,\n",
    "    use_mdeberta_domain_detection: bool = True,\n",
    "    max_length: int = 512,\n",
    "    num_beams: int = 20,  # Increased from 20 to 28 for significantly more candidates\n",
    "    top_k: int = 100,\n",
    "    top_p: float = 0.95,\n",
    "    temperature: float = 0.8,\n",
    "    repetition_penalty: float = 1.5,  # Increased from 1.3 to 1.5 to reduce repetition\n",
    "    length_penalty: float = 1.0,  # Increased from 0.8 to 1.0 to favor longer outputs\n",
    "    max_new_tokens: int = 300,  # Increased from 250 to 300 for more output\n",
    "    prompt_template_idx: int = 0,\n",
    "    batch_size: int = 1,\n",
    "    seed: int = 42,\n",
    "    use_8bit: bool = False,\n",
    "    use_fp16: bool = True,\n",
    "    max_input_length: int = 1024,\n",
    "    use_chunking: bool = True,\n",
    "    chunk_overlap: int = 50,\n",
    "    post_process: bool = True,\n",
    "    filter_stopwords: bool = True,\n",
    "    min_phrase_length: int = 1,\n",
    "    max_phrase_length: int = 5,\n",
    "    prioritize_multi_word: bool = True,\n",
    "    use_lemmatization: bool = True,\n",
    "    use_ner: bool = True,\n",
    "    ner_model: str = \"en_core_web_sm\",\n",
    "    use_sampling: bool = False,  # Keep pure beam search\n",
    "    num_beam_groups: int = 5,    # Increased from 5 to 7 for more diverse candidates\n",
    "    diversity_penalty: float = 1.8,  # Increased from 1.5 to 1.8 for more diversity\n",
    "    ):\n",
    "        self.model_name = model_name\n",
    "        self.use_gpu = use_gpu\n",
    "        self.max_length = max_length\n",
    "        self.num_beams = num_beams\n",
    "        self.top_k = top_k\n",
    "        self.top_p = top_p\n",
    "        self.temperature = temperature\n",
    "        self.repetition_penalty = repetition_penalty\n",
    "        self.length_penalty = length_penalty\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "        self.prompt_template_idx = prompt_template_idx\n",
    "        self.batch_size = batch_size\n",
    "        self.seed = seed\n",
    "        self.use_8bit = use_8bit\n",
    "        self.use_fp16 = use_fp16\n",
    "        self.max_input_length = max_input_length\n",
    "        self.use_chunking = use_chunking\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.post_process = post_process\n",
    "        self.filter_stopwords = filter_stopwords\n",
    "        self.min_phrase_length = min_phrase_length\n",
    "        self.max_phrase_length = max_phrase_length\n",
    "        self.prioritize_multi_word = prioritize_multi_word\n",
    "        self.use_lemmatization = use_lemmatization\n",
    "        self.use_ner = use_ner\n",
    "        self.ner_model = ner_model\n",
    "        self.use_sampling = use_sampling\n",
    "        self.use_mdeberta_domain_detection = use_mdeberta_domain_detection\n",
    "        # Add to __init__ method:\n",
    "        self.debug_redundancy = False  # Set to True to see detailed redundancy filtering info\n",
    "\n",
    "        self.has_domain_classifier = False\n",
    "        self.initialize_domain_components()\n",
    "        if self.use_ner:\n",
    "            try:\n",
    "                print(f\"Loading NER model: {ner_model}\")\n",
    "                self.nlp = spacy.load(ner_model)\n",
    "                # Only disable parser to maintain POS tagging for lemmatization\n",
    "                self.nlp.disable_pipes([\"parser\"])\n",
    "                print(\"NER model loaded successfully\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not load NER model: {str(e)}\")\n",
    "                print(\"Falling back to basic NER functionality\")\n",
    "                self.use_ner = False\n",
    "\n",
    "        print(f\"Loading sentence transformer model...\")\n",
    "        self.sentence_model = SentenceTransformer('all-mpnet-base-v2')\n",
    "        self.use_sampling = use_sampling\n",
    "        if num_beam_groups > 1:\n",
    "            self.num_beam_groups = num_beam_groups\n",
    "            self.diversity_penalty = diversity_penalty\n",
    "\n",
    "        # Set random seed for reproducibility\n",
    "        set_seed(seed)\n",
    "        self._quality_percentiles = {\n",
    "        \"technology\": 5,  # Further reduced from 15 to 10\n",
    "        \"science\": 5,     # Further reduced from 15 to 10\n",
    "        \"health\": 5,      # Further reduced from 15 to 10\n",
    "        \"business\": 5,    # Further reduced from 15 to 10\n",
    "        \"politics\": 5,    # Further reduced from 15 to 10\n",
    "        \"sports\": 5,       # Further reduced from 10 to 8\n",
    "        \"entertainment\": 5, # Further reduced from 10 to 8\n",
    "        \"world\": 5,       # Further reduced from 15 to 10\n",
    "        \"default\": 5      # Further reduced from 15 to 10\n",
    "        }\n",
    "        # Set device\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() and use_gpu else \"cpu\"\n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "        # Initialize tokenizer\n",
    "        print(f\"Loading tokenizer: {model_name}\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "        # Initialize model with memory optimizations\n",
    "        print(f\"Loading model: {model_name}\")\n",
    "\n",
    "        # Configure model loading options based on parameters\n",
    "        model_kwargs = {\n",
    "            \"device_map\": \"auto\" if self.device == \"cuda\" else None,\n",
    "        }\n",
    "\n",
    "        # Add quantization if requested\n",
    "        if use_8bit and self.device == \"cuda\":\n",
    "            model_kwargs[\"load_in_8bit\"] = True\n",
    "\n",
    "        try:\n",
    "            self.model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "                model_name,\n",
    "                **model_kwargs\n",
    "            )\n",
    "\n",
    "            # Use half-precision if requested and not using 8-bit quantization\n",
    "            if use_fp16 and not use_8bit and self.device == \"cuda\":\n",
    "                self.model = self.model.half()\n",
    "\n",
    "            if self.device == \"cuda\" and not use_8bit and \"device_map\" not in model_kwargs:\n",
    "                self.model = self.model.to(self.device)\n",
    "\n",
    "            print(f\"Model loaded successfully on {self.device}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "        # Initialize lemmatizer if needed\n",
    "        if self.use_lemmatization:\n",
    "            self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "        # Initialize stopwords\n",
    "        if self.filter_stopwords:\n",
    "            self.stopwords = set(stopwords.words('english'))\n",
    "            # Add common words that shouldn't be keyphrases\n",
    "            self.stopwords.update([\n",
    "                'said', 'according', 'reported', 'told', 'says', 'say', 'saying',\n",
    "                'stated', 'states', 'state', 'added', 'adds', 'add', 'noted',\n",
    "                'notes', 'note', 'explained', 'explains', 'explain', 'claimed',\n",
    "                'claims', 'claim', 'announced', 'announces', 'announce'\n",
    "            ])\n",
    "        self.domain_generation_params = {\n",
    "            \"technology\": {\n",
    "                \"num_beams\": 24,\n",
    "                \"num_beam_groups\": 6,\n",
    "                \"diversity_penalty\": 1.8,\n",
    "                \"max_new_tokens\": 300\n",
    "            },\n",
    "            \"science\": {\n",
    "                \"num_beams\": 24,\n",
    "                \"num_beam_groups\": 6,\n",
    "                \"diversity_penalty\": 1.8,\n",
    "                \"max_new_tokens\": 300\n",
    "            },\n",
    "            \"health\": {\n",
    "                \"num_beams\": 20,\n",
    "                \"num_beam_groups\": 5,\n",
    "                \"diversity_penalty\": 1.5,\n",
    "                \"max_new_tokens\": 250\n",
    "            },\n",
    "            \"default\": {\n",
    "                \"num_beams\": 16,\n",
    "                \"num_beam_groups\": 4,\n",
    "                \"diversity_penalty\": 1.2,\n",
    "                \"max_new_tokens\": 200\n",
    "            }\n",
    "        }\n",
    "        self.initialize_generic_terms()\n",
    "        # Initialize domain-specific components\n",
    "        self.initialize_domain_components()\n",
    "        print(\"Abstractive Keyphrase Extractor initialized\")\n",
    "\n",
    "\n",
    "    def optimize_for_raw_candidates(self, articles: List[str], num_articles: int = 3) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Run a focused optimization to maximize the number of raw candidates generated.\n",
    "\n",
    "        Args:\n",
    "            articles: List of articles to test\n",
    "            num_articles: Number of articles to use for testing\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with optimization results\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"OPTIMIZING FOR MAXIMUM RAW CANDIDATE GENERATION\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        # Run optimization with focus on raw count\n",
    "        results = self.optimize_generation_across_articles(\n",
    "            articles,\n",
    "            num_articles=num_articles,\n",
    "            focus_on_raw_count=True\n",
    "        )\n",
    "\n",
    "        # Get the best parameters\n",
    "        best_params = results[\"best_params\"]\n",
    "\n",
    "        if best_params:\n",
    "            print(\"\\nBest parameters for maximizing raw candidates:\")\n",
    "            for param, value in best_params.items():\n",
    "                print(f\"  {param}: {value}\")\n",
    "\n",
    "            # Test the best parameters on a sample article\n",
    "            if articles:\n",
    "                sample_article = articles[0]\n",
    "                print(\"\\nTesting best parameters on sample article...\")\n",
    "\n",
    "                # Save original parameters\n",
    "                original_params = {}\n",
    "                for param in best_params:\n",
    "                    if hasattr(self, param):\n",
    "                        original_params[param] = getattr(self, param)\n",
    "\n",
    "                # Apply best parameters\n",
    "                self.apply_optimized_parameters(best_params)\n",
    "\n",
    "                # Generate keyphrases\n",
    "                raw_keyphrases = self.generate_keyphrases(sample_article)\n",
    "                print(f\"\\nGenerated {len(raw_keyphrases)} raw keyphrases with optimized parameters\")\n",
    "\n",
    "                # Examine raw output\n",
    "                self.examine_raw_output(sample_article, params=best_params)\n",
    "\n",
    "                # Restore original parameters\n",
    "                for param, value in original_params.items():\n",
    "                    setattr(self, param, value)\n",
    "\n",
    "        return results\n",
    "\n",
    "    def clean_memory(self):\n",
    "        \"\"\"Clean up GPU memory\"\"\"\n",
    "        if self.device == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Preprocess text for keyphrase extraction.\n",
    "\n",
    "        Args:\n",
    "            text: Input text\n",
    "\n",
    "        Returns:\n",
    "            Preprocessed text\n",
    "        \"\"\"\n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "        # Truncate if too long\n",
    "        if len(text) > self.max_input_length:\n",
    "            text = text[:self.max_input_length]\n",
    "\n",
    "        return text\n",
    "\n",
    "    def optimize_generation_across_articles(self, articles: List[str], num_articles: int = 5, focus_on_raw_count: bool = True) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Run parameter optimization across multiple articles to find the best general settings.\n",
    "\n",
    "        Args:\n",
    "            articles: List of articles to test\n",
    "            num_articles: Number of articles to use for testing (to limit runtime)\n",
    "            focus_on_raw_count: Whether to prioritize raw candidate count over quality score\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with best parameters and statistics\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"OPTIMIZING GENERATION PARAMETERS ACROSS MULTIPLE ARTICLES\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        # Limit the number of articles to test\n",
    "        test_articles = articles[:num_articles] if len(articles) > num_articles else articles\n",
    "        print(f\"Testing on {len(test_articles)} articles\")\n",
    "\n",
    "        # Track results for each parameter combination\n",
    "        param_results = {}\n",
    "\n",
    "        # Define parameter combinations to test\n",
    "        param_combinations = [\n",
    "            # Pure beam search with varying beam sizes\n",
    "            {\n",
    "                \"name\": \"Beam Search (8 beams)\",\n",
    "                \"params\": {\n",
    "                    \"use_sampling\": False,\n",
    "                    \"num_beams\": 8,\n",
    "                    \"length_penalty\": 0.8,\n",
    "                    \"prompt_template_idx\": 0,\n",
    "                    \"repetition_penalty\": 1.3,\n",
    "                    \"max_new_tokens\": 150,\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Beam Search (12 beams)\",\n",
    "                \"params\": {\n",
    "                    \"use_sampling\": False,\n",
    "                    \"num_beams\": 12,\n",
    "                    \"length_penalty\": 0.8,\n",
    "                    \"prompt_template_idx\": 0,\n",
    "                    \"repetition_penalty\": 1.3,\n",
    "                    \"max_new_tokens\": 150,\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Beam Search (16 beams)\",\n",
    "                \"params\": {\n",
    "                    \"use_sampling\": False,\n",
    "                    \"num_beams\": 16,\n",
    "                    \"length_penalty\": 0.8,\n",
    "                    \"prompt_template_idx\": 0,\n",
    "                    \"repetition_penalty\": 1.3,\n",
    "                    \"max_new_tokens\": 150,\n",
    "                }\n",
    "            },\n",
    "            # Diverse beam search with varying configurations\n",
    "            {\n",
    "                \"name\": \"Diverse Beam (8 beams, 2 groups, penalty=0.8)\",\n",
    "                \"params\": {\n",
    "                    \"use_sampling\": False,\n",
    "                    \"num_beams\": 8,\n",
    "                    \"num_beam_groups\": 2,\n",
    "                    \"diversity_penalty\": 0.8,\n",
    "                    \"length_penalty\": 0.8,\n",
    "                    \"prompt_template_idx\": 0,\n",
    "                    \"repetition_penalty\": 1.3,\n",
    "                    \"max_new_tokens\": 150,\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Diverse Beam (8 beams, 4 groups, penalty=1.0)\",\n",
    "                \"params\": {\n",
    "                    \"use_sampling\": False,\n",
    "                    \"num_beams\": 8,\n",
    "                    \"num_beam_groups\": 4,\n",
    "                    \"diversity_penalty\": 1.0,\n",
    "                    \"length_penalty\": 0.8,\n",
    "                    \"prompt_template_idx\": 0,\n",
    "                    \"repetition_penalty\": 1.3,\n",
    "                    \"max_new_tokens\": 150,\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Diverse Beam (12 beams, 3 groups, penalty=1.0)\",\n",
    "                \"params\": {\n",
    "                    \"use_sampling\": False,\n",
    "                    \"num_beams\": 12,\n",
    "                    \"num_beam_groups\": 3,\n",
    "                    \"diversity_penalty\": 1.0,\n",
    "                    \"length_penalty\": 0.8,\n",
    "                    \"prompt_template_idx\": 0,\n",
    "                    \"repetition_penalty\": 1.3,\n",
    "                    \"max_new_tokens\": 150,\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Diverse Beam (12 beams, 4 groups, penalty=1.0)\",\n",
    "                \"params\": {\n",
    "                    \"use_sampling\": False,\n",
    "                    \"num_beams\": 12,\n",
    "                    \"num_beam_groups\": 4,\n",
    "                    \"diversity_penalty\": 1.0,\n",
    "                    \"length_penalty\": 0.8,\n",
    "                    \"prompt_template_idx\": 0,\n",
    "                    \"repetition_penalty\": 1.3,\n",
    "                    \"max_new_tokens\": 150,\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Diverse Beam (12 beams, 4 groups, penalty=1.5)\",\n",
    "                \"params\": {\n",
    "                    \"use_sampling\": False,\n",
    "                    \"num_beams\": 12,\n",
    "                    \"num_beam_groups\": 4,\n",
    "                    \"diversity_penalty\": 1.5,\n",
    "                    \"length_penalty\": 0.8,\n",
    "                    \"prompt_template_idx\": 0,\n",
    "                    \"repetition_penalty\": 1.3,\n",
    "                    \"max_new_tokens\": 150,\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Diverse Beam (16 beams, 4 groups, penalty=1.0)\",\n",
    "                \"params\": {\n",
    "                    \"use_sampling\": False,\n",
    "                    \"num_beams\": 16,\n",
    "                    \"num_beam_groups\": 4,\n",
    "                    \"diversity_penalty\": 1.0,\n",
    "                    \"length_penalty\": 0.8,\n",
    "                    \"prompt_template_idx\": 0,\n",
    "                    \"repetition_penalty\": 1.3,\n",
    "                    \"max_new_tokens\": 150,\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Diverse Beam (16 beams, 8 groups, penalty=1.2)\",\n",
    "                \"params\": {\n",
    "                    \"use_sampling\": False,\n",
    "                    \"num_beams\": 16,\n",
    "                    \"num_beam_groups\": 8,\n",
    "                    \"diversity_penalty\": 1.2,\n",
    "                    \"length_penalty\": 0.8,\n",
    "                    \"prompt_template_idx\": 0,\n",
    "                    \"repetition_penalty\": 1.3,\n",
    "                    \"max_new_tokens\": 150,\n",
    "                }\n",
    "            },\n",
    "            # Sampling with varying temperatures\n",
    "            {\n",
    "                \"name\": \"Sampling (temp=0.6, top_p=0.95)\",\n",
    "                \"params\": {\n",
    "                    \"use_sampling\": True,\n",
    "                    \"temperature\": 0.6,\n",
    "                    \"top_p\": 0.95,\n",
    "                    \"top_k\": 100,\n",
    "                    \"repetition_penalty\": 1.3,\n",
    "                    \"prompt_template_idx\": 0,\n",
    "                    \"max_new_tokens\": 150,\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Sampling (temp=0.7, top_p=0.95)\",\n",
    "                \"params\": {\n",
    "                    \"use_sampling\": True,\n",
    "                    \"temperature\": 0.7,\n",
    "                    \"top_p\": 0.95,\n",
    "                    \"top_k\": 100,\n",
    "                    \"repetition_penalty\": 1.3,\n",
    "                    \"prompt_template_idx\": 0,\n",
    "                    \"max_new_tokens\": 150,\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Sampling (temp=0.8, top_p=0.95)\",\n",
    "                \"params\": {\n",
    "                    \"use_sampling\": True,\n",
    "                    \"temperature\": 0.8,\n",
    "                    \"top_p\": 0.95,\n",
    "                    \"top_k\": 100,\n",
    "                    \"repetition_penalty\": 1.3,\n",
    "                    \"prompt_template_idx\": 0,\n",
    "                    \"max_new_tokens\": 150,\n",
    "                }\n",
    "            },\n",
    "            # Different prompt templates with best generation strategies\n",
    "            {\n",
    "                \"name\": \"Simplified Prompt (Diverse Beam 12/4)\",\n",
    "                \"params\": {\n",
    "                    \"use_sampling\": False,\n",
    "                    \"num_beams\": 12,\n",
    "                    \"num_beam_groups\": 4,\n",
    "                    \"diversity_penalty\": 1.0,\n",
    "                    \"length_penalty\": 0.8,\n",
    "                    \"prompt_template_idx\": 5 if len(self.PROMPT_TEMPLATES) > 5 else 0,\n",
    "                    \"repetition_penalty\": 1.3,\n",
    "                    \"max_new_tokens\": 150,\n",
    "                }\n",
    "            },\n",
    "            # Increased max_new_tokens for more output\n",
    "            {\n",
    "                \"name\": \"Diverse Beam (12/4) with 200 tokens\",\n",
    "                \"params\": {\n",
    "                    \"use_sampling\": False,\n",
    "                    \"num_beams\": 12,\n",
    "                    \"num_beam_groups\": 4,\n",
    "                    \"diversity_penalty\": 1.0,\n",
    "                    \"length_penalty\": 0.8,\n",
    "                    \"prompt_template_idx\": 0,\n",
    "                    \"repetition_penalty\": 1.3,\n",
    "                    \"max_new_tokens\": 200,  # Increased from 150 to 200\n",
    "                }\n",
    "            },\n",
    "            # Hybrid approach: Diverse beam with slight sampling\n",
    "            {\n",
    "                \"name\": \"Hybrid (Diverse Beam + Low Temp)\",\n",
    "                \"params\": {\n",
    "                    \"use_sampling\": True,\n",
    "                    \"temperature\": 0.6,  # Low temperature\n",
    "                    \"top_p\": 0.95,\n",
    "                    \"top_k\": 100,\n",
    "                    \"num_beams\": 8,\n",
    "                    \"num_beam_groups\": 4,\n",
    "                    \"diversity_penalty\": 1.0,\n",
    "                    \"length_penalty\": 0.8,\n",
    "                    \"prompt_template_idx\": 0,\n",
    "                    \"repetition_penalty\": 1.3,\n",
    "                    \"max_new_tokens\": 150,\n",
    "                }\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        # Save original parameters\n",
    "        original_params = {\n",
    "            'temperature': self.temperature,\n",
    "            'top_p': self.top_p,\n",
    "            'top_k': self.top_k,\n",
    "            'length_penalty': self.length_penalty,\n",
    "            'max_new_tokens': self.max_new_tokens,\n",
    "            'prompt_template_idx': self.prompt_template_idx,\n",
    "            'num_beams': self.num_beams,\n",
    "            'repetition_penalty': self.repetition_penalty,\n",
    "            'use_sampling': getattr(self, 'use_sampling', True),\n",
    "        }\n",
    "\n",
    "        # Initialize results tracking\n",
    "        for combo in param_combinations:\n",
    "            param_results[combo[\"name\"]] = {\n",
    "                \"params\": combo[\"params\"],\n",
    "                \"keyphrase_counts\": [],\n",
    "                \"raw_counts\": [],  # Track raw counts before filtering\n",
    "                \"multi_word_percentages\": [],\n",
    "                \"avg_lengths\": [],\n",
    "                \"quality_scores\": [],\n",
    "                \"processing_times\": [],\n",
    "                \"domains\": []\n",
    "            }\n",
    "\n",
    "        # Test each article\n",
    "        for i, article in enumerate(test_articles):\n",
    "            print(f\"\\nTesting article {i+1}/{len(test_articles)}\")\n",
    "\n",
    "            # Test each parameter combination\n",
    "            for combo in param_combinations:\n",
    "                print(f\"\\n  Testing: {combo['name']}\")\n",
    "\n",
    "                # Apply parameters\n",
    "                for param, value in combo[\"params\"].items():\n",
    "                    setattr(self, param, value)\n",
    "\n",
    "                # Handle beam groups\n",
    "                if \"num_beam_groups\" in combo[\"params\"] and combo[\"params\"][\"num_beam_groups\"] > 1:\n",
    "                    self.num_beam_groups = combo[\"params\"][\"num_beam_groups\"]\n",
    "                    self.diversity_penalty = combo[\"params\"][\"diversity_penalty\"]\n",
    "                else:\n",
    "                    if hasattr(self, 'num_beam_groups'):\n",
    "                        delattr(self, 'num_beam_groups')\n",
    "                    if hasattr(self, 'diversity_penalty'):\n",
    "                        delattr(self, 'diversity_penalty')\n",
    "\n",
    "                # Generate keyphrases\n",
    "                start_time = time.time()\n",
    "\n",
    "                # First detect domain\n",
    "                domain = self.detect_domain(article)\n",
    "\n",
    "                # Generate raw keyphrases\n",
    "                raw_keyphrases = self.generate_keyphrases(article)\n",
    "                raw_count = len(raw_keyphrases)\n",
    "\n",
    "                # Get final keyphrases with scores\n",
    "                keyphrases = self.extract_keyphrases_with_scores(article)\n",
    "\n",
    "                end_time = time.time()\n",
    "                processing_time = end_time - start_time\n",
    "\n",
    "                # Calculate metrics\n",
    "                num_keyphrases = len(keyphrases)\n",
    "                multi_word_count = sum(1 for kp, _ in keyphrases if len(kp.split()) > 1)\n",
    "                multi_word_percentage = multi_word_count / num_keyphrases if num_keyphrases > 0 else 0\n",
    "                avg_length = sum(len(kp.split()) for kp, _ in keyphrases) / num_keyphrases if num_keyphrases > 0 else 0\n",
    "\n",
    "                # Calculate quality score\n",
    "                quantity_score = 0.0\n",
    "                if 15 <= raw_count <= 25:\n",
    "                    quantity_score = 1.0\n",
    "                elif 25 < raw_count <= 35:\n",
    "                    quantity_score = 0.9  # Increased from 0.8 to favor higher counts\n",
    "                elif 35 < raw_count <= 45:\n",
    "                    quantity_score = 0.8  # Added tier for even higher counts\n",
    "                elif 10 <= raw_count < 15:\n",
    "                    quantity_score = 0.7\n",
    "                elif 5 <= raw_count < 10:\n",
    "                    quantity_score = 0.5\n",
    "                elif raw_count > 45:\n",
    "                    quantity_score = 0.7  # Still good but might include noise\n",
    "\n",
    "                length_score = 1.0 - abs(avg_length - 2.5) / 2.5 if avg_length > 0 else 0\n",
    "\n",
    "                quality_score = (\n",
    "                    (quantity_score * 0.5) +\n",
    "                    (multi_word_percentage * 0.3) +\n",
    "                    (length_score * 0.2)\n",
    "                )\n",
    "\n",
    "                # Store results\n",
    "                param_results[combo[\"name\"]][\"keyphrase_counts\"].append(num_keyphrases)\n",
    "                param_results[combo[\"name\"]][\"raw_counts\"].append(raw_count)\n",
    "                param_results[combo[\"name\"]][\"multi_word_percentages\"].append(multi_word_percentage)\n",
    "                param_results[combo[\"name\"]][\"avg_lengths\"].append(avg_length)\n",
    "                param_results[combo[\"name\"]][\"quality_scores\"].append(quality_score)\n",
    "                param_results[combo[\"name\"]][\"processing_times\"].append(processing_time)\n",
    "                param_results[combo[\"name\"]][\"domains\"].append(domain)\n",
    "\n",
    "                # Print results\n",
    "                print(f\"    Generated {raw_count} raw keyphrases, {num_keyphrases} final keyphrases\")\n",
    "                print(f\"    Domain: {domain}\")\n",
    "                print(f\"    Multi-word: {multi_word_percentage:.1%}, Avg length: {avg_length:.1f}, Quality: {quality_score:.2f}\")\n",
    "                print(f\"    Processing time: {processing_time:.2f}s\")\n",
    "\n",
    "        # Calculate averages and find best combination\n",
    "        best_avg_score = 0\n",
    "        best_raw_count = 0\n",
    "        best_combo_name = None\n",
    "        best_combo_name_raw = None\n",
    "\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"RESULTS SUMMARY\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        for name, results in param_results.items():\n",
    "            # Calculate averages\n",
    "            avg_count = sum(results[\"keyphrase_counts\"]) / len(results[\"keyphrase_counts\"])\n",
    "            avg_raw_count = sum(results[\"raw_counts\"]) / len(results[\"raw_counts\"])\n",
    "            avg_multi_word = sum(results[\"multi_word_percentages\"]) / len(results[\"multi_word_percentages\"])\n",
    "            avg_length = sum(results[\"avg_lengths\"]) / len(results[\"avg_lengths\"])\n",
    "            avg_quality = sum(results[\"quality_scores\"]) / len(results[\"quality_scores\"])\n",
    "            avg_time = sum(results[\"processing_times\"]) / len(results[\"processing_times\"])\n",
    "\n",
    "            # Store averages\n",
    "            results[\"avg_count\"] = avg_count\n",
    "            results[\"avg_raw_count\"] = avg_raw_count\n",
    "            results[\"avg_multi_word\"] = avg_multi_word\n",
    "            results[\"avg_length\"] = avg_length\n",
    "            results[\"avg_quality\"] = avg_quality\n",
    "            results[\"avg_time\"] = avg_time\n",
    "\n",
    "            # Print summary\n",
    "            print(f\"\\n{name}:\")\n",
    "            print(f\"  Avg raw keyphrases: {avg_raw_count:.1f}\")\n",
    "            print(f\"  Avg final keyphrases: {avg_count:.1f}\")\n",
    "            print(f\"  Avg multi-word: {avg_multi_word:.1%}\")\n",
    "            print(f\"  Avg length: {avg_length:.1f} words\")\n",
    "            print(f\"  Avg quality score: {avg_quality:.2f}\")\n",
    "            print(f\"  Avg processing time: {avg_time:.2f}s\")\n",
    "\n",
    "            # Update best based on focus\n",
    "            if focus_on_raw_count:\n",
    "                # Prioritize raw count but ensure quality isn't terrible\n",
    "                if avg_raw_count > best_raw_count and avg_quality >= 0.5:\n",
    "                    best_raw_count = avg_raw_count\n",
    "                    best_combo_name_raw = name\n",
    "\n",
    "                # Also track best quality for comparison\n",
    "                combined_score = avg_quality * (0.5 + 0.5 * min(1.0, avg_raw_count / 20.0))\n",
    "                results[\"combined_score\"] = combined_score\n",
    "\n",
    "                if combined_score > best_avg_score:\n",
    "                    best_avg_score = combined_score\n",
    "                    best_combo_name = name\n",
    "            else:\n",
    "                # Original approach: balance quality and quantity\n",
    "                combined_score = avg_quality * (0.5 + 0.5 * min(1.0, avg_raw_count / 20.0))\n",
    "                results[\"combined_score\"] = combined_score\n",
    "\n",
    "                if combined_score > best_avg_score:\n",
    "                    best_avg_score = combined_score\n",
    "                    best_combo_name = name\n",
    "\n",
    "        # Determine which best to use\n",
    "        if focus_on_raw_count and best_combo_name_raw:\n",
    "            best_combo_name = best_combo_name_raw\n",
    "\n",
    "        # Print best combination\n",
    "        if best_combo_name:\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(f\"BEST PARAMETER COMBINATION: {best_combo_name}\")\n",
    "            print(\"=\"*80)\n",
    "            if focus_on_raw_count:\n",
    "                print(f\"Selected based on maximizing raw candidate count\")\n",
    "            else:\n",
    "                print(f\"Selected based on combined quality score\")\n",
    "\n",
    "            print(f\"Average raw keyphrases: {param_results[best_combo_name]['avg_raw_count']:.1f}\")\n",
    "            print(f\"Average final keyphrases: {param_results[best_combo_name]['avg_count']:.1f}\")\n",
    "            print(f\"Average quality score: {param_results[best_combo_name]['avg_quality']:.2f}\")\n",
    "            print(f\"Average multi-word percentage: {param_results[best_combo_name]['avg_multi_word']:.1%}\")\n",
    "            print(f\"Average phrase length: {param_results[best_combo_name]['avg_length']:.1f} words\")\n",
    "            print(f\"Average processing time: {param_results[best_combo_name]['avg_time']:.2f}s\")\n",
    "            print(\"\\nParameters:\")\n",
    "            for param, value in param_results[best_combo_name][\"params\"].items():\n",
    "                print(f\"  {param}: {value}\")\n",
    "\n",
    "        # Restore original parameters\n",
    "        for param, value in original_params.items():\n",
    "            setattr(self, param, value)\n",
    "\n",
    "        # Remove beam group attributes if they weren't in original params\n",
    "        if \"num_beam_groups\" not in original_params and hasattr(self, 'num_beam_groups'):\n",
    "            delattr(self, 'num_beam_groups')\n",
    "            # Remove beam group attributes if they weren't in original params\n",
    "        if \"num_beam_groups\" not in original_params and hasattr(self, 'num_beam_groups'):\n",
    "            delattr(self, 'num_beam_groups')\n",
    "        if \"diversity_penalty\" not in original_params and hasattr(self, 'diversity_penalty'):\n",
    "            delattr(self, 'diversity_penalty')\n",
    "\n",
    "        print(\"\\nParameter optimization complete. Original parameters restored.\")\n",
    "\n",
    "        # Find top 3 parameter sets by raw count for manual inspection\n",
    "        top_raw_count_combos = sorted(\n",
    "            [(name, results[\"avg_raw_count\"]) for name, results in param_results.items()],\n",
    "            key=lambda x: x[1],\n",
    "            reverse=True\n",
    "        )[:3]\n",
    "\n",
    "        print(\"\\nTop 3 parameter sets by raw candidate count:\")\n",
    "        for name, avg_raw_count in top_raw_count_combos:\n",
    "            print(f\"- {name}: {avg_raw_count:.1f} raw candidates\")\n",
    "\n",
    "        print(\"\\nRecommendation: Manually inspect raw output of these top parameter sets using:\")\n",
    "        for name, _ in top_raw_count_combos:\n",
    "            params_str = \", \".join([f\"{k}={v}\" for k, v in param_results[name][\"params\"].items()])\n",
    "            print(f\"extractor.examine_raw_output(text, params={{{params_str}}})\")\n",
    "\n",
    "        # Return results\n",
    "        return {\n",
    "            \"best_combination\": best_combo_name,\n",
    "            \"best_params\": param_results[best_combo_name][\"params\"] if best_combo_name else None,\n",
    "            \"top_raw_count_combos\": top_raw_count_combos,\n",
    "            \"all_results\": param_results\n",
    "        }\n",
    "\n",
    "\n",
    "    def apply_optimized_parameters(self, params: Dict[str, Any]) -> None:\n",
    "        \"\"\"\n",
    "        Apply optimized parameters to the extractor.\n",
    "\n",
    "        Args:\n",
    "            params: Dictionary of parameters to apply\n",
    "        \"\"\"\n",
    "        print(\"\\nApplying optimized parameters:\")\n",
    "\n",
    "        for param, value in params.items():\n",
    "            if hasattr(self, param):\n",
    "                print(f\"  Setting {param} = {value}\")\n",
    "                setattr(self, param, value)\n",
    "            else:\n",
    "                print(f\"  Warning: Parameter '{param}' not found in extractor\")\n",
    "\n",
    "        # Handle special case for beam groups\n",
    "        if \"num_beam_groups\" in params and params[\"num_beam_groups\"] > 1:\n",
    "            self.num_beam_groups = params[\"num_beam_groups\"]\n",
    "            if \"diversity_penalty\" in params:\n",
    "                self.diversity_penalty = params[\"diversity_penalty\"]\n",
    "            else:\n",
    "                self.diversity_penalty = 1.0  # Default value\n",
    "            print(f\"  Setting num_beam_groups = {self.num_beam_groups}\")\n",
    "            print(f\"  Setting diversity_penalty = {self.diversity_penalty}\")\n",
    "        elif hasattr(self, 'num_beam_groups'):\n",
    "            # Remove beam group attributes if not needed\n",
    "            delattr(self, 'num_beam_groups')\n",
    "            if hasattr(self, 'diversity_penalty'):\n",
    "                delattr(self, 'diversity_penalty')\n",
    "\n",
    "        print(\"Parameters applied successfully\")\n",
    "\n",
    "    def test_parameter_improvement(self, text: str) -> None:\n",
    "        \"\"\"\n",
    "        Test the improvement from different parameter settings on a single article.\n",
    "\n",
    "        Args:\n",
    "            text: Article text to test\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"TESTING PARAMETER IMPROVEMENT\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        # Define parameter sets to test\n",
    "        parameter_sets = [\n",
    "            {\n",
    "                \"name\": \"Default Parameters\",\n",
    "                \"params\": {\n",
    "                    \"use_sampling\": True,\n",
    "                    \"temperature\": 0.7,\n",
    "                    \"top_p\": 0.92,\n",
    "                    \"top_k\": 50,\n",
    "                    \"num_beams\": 5,\n",
    "                    \"repetition_penalty\": 1.2,\n",
    "                    \"length_penalty\": 0.8,\n",
    "                    \"max_new_tokens\": 75,\n",
    "                    \"prompt_template_idx\": 0\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Optimized Parameters\",\n",
    "                \"params\": {\n",
    "                    \"use_sampling\": False,\n",
    "                    \"num_beams\": 12,\n",
    "                    \"num_beam_groups\": 4,\n",
    "                    \"diversity_penalty\": 1.0,\n",
    "                    \"repetition_penalty\": 1.3,\n",
    "                    \"length_penalty\": 0.8,\n",
    "                    \"max_new_tokens\": 150,\n",
    "                    \"prompt_template_idx\": 0\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        # Save original parameters\n",
    "        original_params = {\n",
    "            'temperature': self.temperature,\n",
    "            'top_p': self.top_p,\n",
    "            'top_k': self.top_k,\n",
    "            'length_penalty': self.length_penalty,\n",
    "            'max_new_tokens': self.max_new_tokens,\n",
    "            'prompt_template_idx': self.prompt_template_idx,\n",
    "            'num_beams': self.num_beams,\n",
    "            'repetition_penalty': self.repetition_penalty,\n",
    "            'use_sampling': getattr(self, 'use_sampling', True),\n",
    "        }\n",
    "\n",
    "        # Test each parameter set\n",
    "        results = {}\n",
    "\n",
    "        for param_set in parameter_sets:\n",
    "            print(f\"\\nTesting: {param_set['name']}\")\n",
    "\n",
    "            # Apply parameters\n",
    "            for param, value in param_set[\"params\"].items():\n",
    "                setattr(self, param, value)\n",
    "\n",
    "            # Handle beam groups\n",
    "            if \"num_beam_groups\" in param_set[\"params\"] and param_set[\"params\"][\"num_beam_groups\"] > 1:\n",
    "                self.num_beam_groups = param_set[\"params\"][\"num_beam_groups\"]\n",
    "                self.diversity_penalty = param_set[\"params\"][\"diversity_penalty\"]\n",
    "            else:\n",
    "                if hasattr(self, 'num_beam_groups'):\n",
    "                    delattr(self, 'num_beam_groups')\n",
    "                if hasattr(self, 'diversity_penalty'):\n",
    "                    delattr(self, 'diversity_penalty')\n",
    "\n",
    "            # Generate keyphrases\n",
    "            start_time = time.time()\n",
    "            keyphrases = self.generate_keyphrases(text)\n",
    "            end_time = time.time()\n",
    "\n",
    "            # Score keyphrases\n",
    "            scored_keyphrases = self.score_keyphrases_by_relevance(keyphrases, text)\n",
    "\n",
    "            # Calculate metrics\n",
    "            num_keyphrases = len(keyphrases)\n",
    "            multi_word_count = sum(1 for kp in keyphrases if len(kp.split()) > 1)\n",
    "            multi_word_percentage = multi_word_count / num_keyphrases if num_keyphrases > 0 else 0\n",
    "            avg_length = sum(len(kp.split()) for kp in keyphrases) / num_keyphrases if num_keyphrases > 0 else 0\n",
    "            processing_time = end_time - start_time\n",
    "\n",
    "            # Store results\n",
    "            results[param_set[\"name\"]] = {\n",
    "                \"keyphrases\": scored_keyphrases,\n",
    "                \"count\": num_keyphrases,\n",
    "                \"multi_word_percentage\": multi_word_percentage,\n",
    "                \"avg_length\": avg_length,\n",
    "                \"processing_time\": processing_time\n",
    "            }\n",
    "\n",
    "            # Print results\n",
    "            print(f\"  Generated {num_keyphrases} keyphrases in {processing_time:.2f} seconds\")\n",
    "            print(f\"  Multi-word: {multi_word_percentage:.1%}, Avg length: {avg_length:.1f}\")\n",
    "            print(\"\\n  Top keyphrases:\")\n",
    "            for kp, score in sorted(scored_keyphrases, key=lambda x: x[1], reverse=True)[:10]:\n",
    "                print(f\"    - {kp}: {score:.4f}\")\n",
    "\n",
    "        # Calculate improvement\n",
    "        if \"Default Parameters\" in results and \"Optimized Parameters\" in results:\n",
    "            default_count = results[\"Default Parameters\"][\"count\"]\n",
    "            optimized_count = results[\"Optimized Parameters\"][\"count\"]\n",
    "\n",
    "            count_improvement = optimized_count - default_count\n",
    "            count_improvement_percent = (count_improvement / max(1, default_count)) * 100\n",
    "\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(\"IMPROVEMENT SUMMARY\")\n",
    "            print(\"=\"*80)\n",
    "            print(f\"Keyphrase count: {default_count} → {optimized_count} ({count_improvement_percent:+.1f}%)\")\n",
    "\n",
    "            # Compare multi-word percentages\n",
    "            default_multi = results[\"Default Parameters\"][\"multi_word_percentage\"]\n",
    "            optimized_multi = results[\"Optimized Parameters\"][\"multi_word_percentage\"]\n",
    "            multi_improvement = optimized_multi - default_multi\n",
    "            print(f\"Multi-word percentage: {default_multi:.1%} → {optimized_multi:.1%} ({multi_improvement:+.1%})\")\n",
    "\n",
    "            # Compare processing times\n",
    "            default_time = results[\"Default Parameters\"][\"processing_time\"]\n",
    "            optimized_time = results[\"Optimized Parameters\"][\"processing_time\"]\n",
    "            time_difference = optimized_time - default_time\n",
    "            time_difference_percent = (time_difference / default_time) * 100\n",
    "            print(f\"Processing time: {default_time:.2f}s → {optimized_time:.2f}s ({time_difference_percent:+.1f}%)\")\n",
    "\n",
    "        # Restore original parameters\n",
    "        for param, value in original_params.items():\n",
    "            setattr(self, param, value)\n",
    "\n",
    "        # Remove beam group attributes if they weren't in original params\n",
    "        if \"num_beam_groups\" not in original_params and hasattr(self, 'num_beam_groups'):\n",
    "            delattr(self, 'num_beam_groups')\n",
    "        if \"diversity_penalty\" not in original_params and hasattr(self, 'diversity_penalty'):\n",
    "            delattr(self, 'diversity_penalty')\n",
    "\n",
    "        print(\"\\nTest complete. Original parameters restored.\")\n",
    "\n",
    "    def chunk_text(self, text: str, chunk_size: int = 1024, overlap: int = 50) -> List[str]:\n",
    "        \"\"\"\n",
    "        Split long text into chunks with overlap.\n",
    "\n",
    "        Args:\n",
    "            text: Input text\n",
    "            chunk_size: Maximum chunk size in characters\n",
    "            overlap: Overlap between chunks in characters\n",
    "\n",
    "        Returns:\n",
    "            List of text chunks\n",
    "        \"\"\"\n",
    "        if not self.use_chunking or len(text) <= chunk_size:\n",
    "            return [text]\n",
    "\n",
    "        chunks = []\n",
    "        start = 0\n",
    "\n",
    "        while start < len(text):\n",
    "            # Get chunk with specified size\n",
    "            end = start + chunk_size\n",
    "\n",
    "            # If not at the end of text, try to find a good breaking point\n",
    "            if end < len(text):\n",
    "                # Try to find the last period or newline within the last 20% of the chunk\n",
    "                last_period = text.rfind('.', start + int(chunk_size * 0.8), end)\n",
    "                last_newline = text.rfind('\\n', start + int(chunk_size * 0.8), end)\n",
    "\n",
    "                # Use the latest good breaking point\n",
    "                if last_period != -1 or last_newline != -1:\n",
    "                    end = max(last_period, last_newline) + 1\n",
    "\n",
    "            # Add chunk to list\n",
    "            chunks.append(text[start:min(end, len(text))])\n",
    "\n",
    "            # Move start position for next chunk, accounting for overlap\n",
    "            start = end - overlap\n",
    "\n",
    "        return chunks\n",
    "\n",
    "\n",
    "\n",
    "    def calculate_semantic_diversity(self, keyphrases: List[str], embeddings: Optional[np.ndarray] = None) -> float:\n",
    "        \"\"\"\n",
    "        Calculate the semantic diversity of a set of keyphrases.\n",
    "        Higher values indicate more diverse keyphrases.\n",
    "\n",
    "        Args:\n",
    "            keyphrases: List of keyphrases\n",
    "            embeddings: Pre-computed embeddings (optional)\n",
    "\n",
    "        Returns:\n",
    "            Diversity score between 0 and 1\n",
    "        \"\"\"\n",
    "        if not keyphrases or len(keyphrases) < 2:\n",
    "            return 0.0\n",
    "\n",
    "        # Get embeddings if not provided\n",
    "        if embeddings is None:\n",
    "            embeddings = self.sentence_model.encode(keyphrases, show_progress_bar=False)\n",
    "\n",
    "        # Calculate pairwise similarities\n",
    "        similarity_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "        # Calculate average similarity (excluding self-similarity)\n",
    "        n = len(keyphrases)\n",
    "        total_similarity = 0.0\n",
    "        count = 0\n",
    "\n",
    "        for i in range(n):\n",
    "            for j in range(i + 1, n):\n",
    "                total_similarity += similarity_matrix[i, j]\n",
    "                count += 1\n",
    "\n",
    "        avg_similarity = total_similarity / max(1, count)\n",
    "\n",
    "        # Convert to diversity (1 - similarity)\n",
    "        diversity = 1.0 - avg_similarity\n",
    "\n",
    "        return diversity\n",
    "\n",
    "    def enhance_semantic_diversity(self, scored_keyphrases: List[Tuple[str, float]], text: str, min_cluster_count: int = 3, max_similarity_threshold: float = 0.75) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Enhance the semantic diversity of keyphrases by ensuring representation from different semantic clusters.\n",
    "        This method uses semantic clustering to identify groups of similar keyphrases and ensures\n",
    "        representation from each important cluster, while maintaining or increasing the total number of keyphrases.\n",
    "\n",
    "        Args:\n",
    "            scored_keyphrases: List of (keyphrase, score) tuples\n",
    "            text: Original text\n",
    "            min_cluster_count: Minimum number of clusters to identify\n",
    "            max_similarity_threshold: Maximum similarity threshold for clustering\n",
    "\n",
    "        Returns:\n",
    "            List of (keyphrase, score) tuples with enhanced semantic diversity\n",
    "        \"\"\"\n",
    "        if not scored_keyphrases or len(scored_keyphrases) < 3:\n",
    "            return scored_keyphrases\n",
    "\n",
    "        # Extract keyphrases and scores\n",
    "        keyphrases = [kp for kp, _ in scored_keyphrases]\n",
    "        scores = [score for _, score in scored_keyphrases]\n",
    "\n",
    "        # Get embeddings for keyphrases\n",
    "        embeddings = self.sentence_model.encode(keyphrases, show_progress_bar=False)\n",
    "\n",
    "        # Get document embedding\n",
    "        doc_embedding = self.sentence_model.encode([text], show_progress_bar=False)[0]\n",
    "\n",
    "        # Calculate similarity matrix\n",
    "        similarity_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "        # Calculate document relevance for each keyphrase\n",
    "        doc_relevance = cosine_similarity(embeddings, [doc_embedding])\n",
    "\n",
    "        # Perform semantic clustering\n",
    "        clusters = {}\n",
    "        assigned = set()\n",
    "        cluster_id = 0\n",
    "\n",
    "        # Sort keyphrases by score for initial cluster centers\n",
    "        sorted_indices = sorted(range(len(scored_keyphrases)), key=lambda i: scored_keyphrases[i][1], reverse=True)\n",
    "\n",
    "        # First pass: Create initial clusters with high-scoring keyphrases as centers\n",
    "        for idx in sorted_indices:\n",
    "            if idx in assigned:\n",
    "                continue\n",
    "\n",
    "            # Create a new cluster with this keyphrase as center\n",
    "            clusters[cluster_id] = {\n",
    "                'center': idx,\n",
    "                'members': [idx],\n",
    "                'avg_score': scores[idx],\n",
    "                'doc_relevance': doc_relevance[idx][0]\n",
    "            }\n",
    "            assigned.add(idx)\n",
    "\n",
    "            # Find similar keyphrases for this cluster\n",
    "            for j in range(len(keyphrases)):\n",
    "                if j in assigned:\n",
    "                    continue\n",
    "\n",
    "                # If similarity is above threshold, add to cluster\n",
    "                if similarity_matrix[idx][j] > max_similarity_threshold:\n",
    "                    clusters[cluster_id]['members'].append(j)\n",
    "                    clusters[cluster_id]['avg_score'] = sum(scores[m] for m in clusters[cluster_id]['members']) / len(clusters[cluster_id]['members'])\n",
    "                    assigned.add(j)\n",
    "\n",
    "            cluster_id += 1\n",
    "\n",
    "            # If we have enough clusters, stop\n",
    "            if cluster_id >= min_cluster_count and len(assigned) >= len(keyphrases) * 0.8:\n",
    "                break\n",
    "\n",
    "        # Second pass: Assign remaining keyphrases to the closest cluster\n",
    "        for i in range(len(keyphrases)):\n",
    "            if i in assigned:\n",
    "                continue\n",
    "\n",
    "            # Find the closest cluster\n",
    "            best_cluster = -1\n",
    "            best_similarity = -1\n",
    "\n",
    "            for c_id, cluster in clusters.items():\n",
    "                center_idx = cluster['center']\n",
    "                similarity = similarity_matrix[i][center_idx]\n",
    "\n",
    "                if similarity > best_similarity:\n",
    "                    best_similarity = similarity\n",
    "                    best_cluster = c_id\n",
    "\n",
    "            # Assign to the best cluster\n",
    "            if best_cluster != -1:\n",
    "                clusters[best_cluster]['members'].append(i)\n",
    "                clusters[best_cluster]['avg_score'] = sum(scores[m] for m in clusters[best_cluster]['members']) / len(clusters[best_cluster]['members'])\n",
    "                assigned.add(i)\n",
    "            else:\n",
    "                # Create a new cluster if no good match\n",
    "                clusters[cluster_id] = {\n",
    "                    'center': i,\n",
    "                    'members': [i],\n",
    "                    'avg_score': scores[i],\n",
    "                    'doc_relevance': doc_relevance[i][0]\n",
    "                }\n",
    "                assigned.add(i)\n",
    "                cluster_id += 1\n",
    "\n",
    "        # Calculate importance of each cluster based on size, avg score, and doc relevance\n",
    "        for c_id, cluster in clusters.items():\n",
    "            size_factor = min(1.0, len(cluster['members']) / 3)  # Cap at 3 members\n",
    "            cluster['importance'] = (0.4 * cluster['avg_score'] +\n",
    "                                    0.4 * cluster['doc_relevance'] +\n",
    "                                    0.2 * size_factor)\n",
    "\n",
    "        # Sort clusters by importance\n",
    "        sorted_clusters = sorted(clusters.items(), key=lambda x: x[1]['importance'], reverse=True)\n",
    "\n",
    "        # Select representatives from each cluster\n",
    "        selected_indices = set()\n",
    "\n",
    "        # First, select the center of each important cluster\n",
    "        for c_id, cluster in sorted_clusters:\n",
    "            selected_indices.add(cluster['center'])\n",
    "\n",
    "        # Then, select additional high-scoring members from larger clusters\n",
    "        for c_id, cluster in sorted_clusters:\n",
    "            # For larger clusters, select more representatives\n",
    "            target_reps = min(3, max(1, len(cluster['members']) // 2))\n",
    "\n",
    "            # Sort members by score\n",
    "            sorted_members = sorted(cluster['members'], key=lambda i: scores[i], reverse=True)\n",
    "\n",
    "            # Select top members (skipping the center which is already selected)\n",
    "            count = 1  # Start at 1 because we already selected the center\n",
    "            for member in sorted_members:\n",
    "                if member != cluster['center'] and count < target_reps:\n",
    "                    selected_indices.add(member)\n",
    "                    count += 1\n",
    "\n",
    "        # Ensure we don't reduce the number of keyphrases\n",
    "        if len(selected_indices) < len(scored_keyphrases):\n",
    "            # Add back high-scoring keyphrases that weren't selected\n",
    "            remaining = [i for i in range(len(keyphrases)) if i not in selected_indices]\n",
    "            remaining.sort(key=lambda i: scores[i], reverse=True)\n",
    "\n",
    "            # Add until we match the original count\n",
    "            for i in remaining:\n",
    "                if len(selected_indices) >= len(scored_keyphrases):\n",
    "                    break\n",
    "                selected_indices.add(i)\n",
    "\n",
    "        # Create the final list of keyphrases\n",
    "        enhanced_keyphrases = [(keyphrases[i], scores[i]) for i in selected_indices]\n",
    "\n",
    "        # Sort by score\n",
    "        enhanced_keyphrases.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Calculate diversity before and after\n",
    "        original_diversity = self.calculate_semantic_diversity(keyphrases, embeddings)\n",
    "        enhanced_diversity = self.calculate_semantic_diversity([kp for kp, _ in enhanced_keyphrases])\n",
    "\n",
    "        print(f\"Semantic diversity: {original_diversity:.3f} → {enhanced_diversity:.3f}\")\n",
    "        print(f\"Keyphrase count: {len(scored_keyphrases)} → {len(enhanced_keyphrases)}\")\n",
    "\n",
    "        return enhanced_keyphrases\n",
    "\n",
    "\n",
    "    def select_diverse_keyphrases(\n",
    "    self,\n",
    "    scored_keyphrases: List[Tuple[str, float]],\n",
    "    target_count: int,\n",
    "    diversity_weight: float = 0.3\n",
    "    ) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Select keyphrases that maximize both relevance and semantic diversity.\n",
    "        Uses a greedy algorithm that balances relevance scores with diversity.\n",
    "\n",
    "        Args:\n",
    "            scored_keyphrases: List of (keyphrase, score) tuples\n",
    "            target_count: Number of keyphrases to select\n",
    "            diversity_weight: Weight given to diversity vs. relevance (0-1)\n",
    "\n",
    "        Returns:\n",
    "            List of selected (keyphrase, score) tuples\n",
    "        \"\"\"\n",
    "        if not scored_keyphrases:\n",
    "            return []\n",
    "\n",
    "        if len(scored_keyphrases) <= target_count:\n",
    "            return scored_keyphrases\n",
    "\n",
    "        # Get all keyphrases and their original scores\n",
    "        all_keyphrases = [kp for kp, _ in scored_keyphrases]\n",
    "        all_scores = [score for _, score in scored_keyphrases]\n",
    "\n",
    "        # Compute embeddings for all keyphrases (do this once)\n",
    "        all_embeddings = self.sentence_model.encode(all_keyphrases, show_progress_bar=False)\n",
    "\n",
    "        # Start with the highest-scored keyphrase\n",
    "        selected_indices = [0]  # Start with the highest-scored keyphrase\n",
    "        selected_embeddings = [all_embeddings[0]]\n",
    "\n",
    "        # Greedy selection\n",
    "        while len(selected_indices) < target_count:\n",
    "            best_score = -1\n",
    "            best_idx = -1\n",
    "\n",
    "            # Try adding each remaining keyphrase\n",
    "            for i in range(len(all_keyphrases)):\n",
    "                if i in selected_indices:\n",
    "                    continue\n",
    "\n",
    "                # Create a candidate set with this keyphrase added\n",
    "                candidate_indices = selected_indices + [i]\n",
    "                candidate_embeddings = selected_embeddings + [all_embeddings[i]]\n",
    "\n",
    "                # Calculate diversity of this candidate set\n",
    "                candidate_diversity = self.calculate_semantic_diversity(\n",
    "                    [all_keyphrases[idx] for idx in candidate_indices],\n",
    "                    np.array(candidate_embeddings)\n",
    "                )\n",
    "\n",
    "                # Calculate relevance (normalized to 0-1 range)\n",
    "                relevance = all_scores[i] / max(all_scores)\n",
    "\n",
    "                # Combined score: weighted sum of relevance and diversity\n",
    "                combined_score = (1 - diversity_weight) * relevance + diversity_weight * candidate_diversity\n",
    "\n",
    "                # Update best if this is better\n",
    "                if combined_score > best_score:\n",
    "                    best_score = combined_score\n",
    "                    best_idx = i\n",
    "\n",
    "            # Add the best keyphrase\n",
    "            if best_idx != -1:\n",
    "                selected_indices.append(best_idx)\n",
    "                selected_embeddings.append(all_embeddings[best_idx])\n",
    "            else:\n",
    "                break  # Shouldn't happen, but just in case\n",
    "\n",
    "        # Return selected keyphrases with their original scores\n",
    "        return [(all_keyphrases[i], all_scores[i]) for i in selected_indices]\n",
    "\n",
    "    def generate_keyphrases(self, text: str, debug_output: bool = False, original_domain: str = None) -> List[str]:\n",
    "        \"\"\"\n",
    "        Generate keyphrases from text using the T5 model and NER.\n",
    "        Supports both sampling and beam search modes for better candidate generation.\n",
    "        Uses domain-specific parameters to optimize generation for different domains.\n",
    "\n",
    "        Args:\n",
    "            text: Input text\n",
    "            debug_output: Whether to print debug information about raw model output\n",
    "\n",
    "        Returns:\n",
    "            List of generated keyphrases\n",
    "        \"\"\"\n",
    "        # Preprocess text\n",
    "        text = self.preprocess_text(text)\n",
    "\n",
    "        # Detect domain for domain-specific parameters\n",
    "        domain = self.detect_domain(text, original_domain=original_domain)\n",
    "        if debug_output:\n",
    "            if original_domain:\n",
    "                print(f\"Using original domain for generation: {domain}\")\n",
    "                print(f\"IMPORTANT: Using original domain from input data for domain-specific parameters\")\n",
    "            else:\n",
    "                print(f\"Detected domain for generation: {domain}\")\n",
    "\n",
    "        # Save original parameters to restore later\n",
    "        original_params = {\n",
    "            'num_beams': self.num_beams,\n",
    "            'max_new_tokens': self.max_new_tokens,\n",
    "            'repetition_penalty': self.repetition_penalty,\n",
    "            'length_penalty': self.length_penalty\n",
    "        }\n",
    "        if hasattr(self, 'num_beam_groups'):\n",
    "            original_params['num_beam_groups'] = self.num_beam_groups\n",
    "            original_params['diversity_penalty'] = self.diversity_penalty\n",
    "\n",
    "        # Apply domain-specific parameters if available\n",
    "        domain_params_applied = False\n",
    "        if hasattr(self, 'domain_generation_params') and domain in self.domain_generation_params:\n",
    "            domain_params = self.domain_generation_params[domain]\n",
    "            for param, value in domain_params.items():\n",
    "                setattr(self, param, value)\n",
    "            domain_params_applied = True\n",
    "            if debug_output:\n",
    "                print(f\"Applied domain-specific parameters for {domain}:\")\n",
    "                for param, value in domain_params.items():\n",
    "                    print(f\"  - {param}: {value}\")\n",
    "        elif not hasattr(self, 'domain_generation_params'):\n",
    "            # Define domain-specific parameters if not already defined\n",
    "            self.domain_generation_params = {\n",
    "                \"technology\": {\n",
    "                    \"num_beams\": 24,\n",
    "                    \"num_beam_groups\": 6,\n",
    "                    \"diversity_penalty\": 1.8,\n",
    "                    \"max_new_tokens\": 300,\n",
    "                    \"repetition_penalty\": 1.5\n",
    "                },\n",
    "                \"science\": {\n",
    "                    \"num_beams\": 24,\n",
    "                    \"num_beam_groups\": 6,\n",
    "                    \"diversity_penalty\": 1.8,\n",
    "                    \"max_new_tokens\": 300,\n",
    "                    \"repetition_penalty\": 1.5\n",
    "                },\n",
    "                \"health\": {\n",
    "                    \"num_beams\": 20,\n",
    "                    \"num_beam_groups\": 5,\n",
    "                    \"diversity_penalty\": 1.5,\n",
    "                    \"max_new_tokens\": 250,\n",
    "                    \"repetition_penalty\": 1.4\n",
    "                },\n",
    "                \"business\": {\n",
    "                    \"num_beams\": 18,\n",
    "                    \"num_beam_groups\": 6,\n",
    "                    \"diversity_penalty\": 1.5,\n",
    "                    \"max_new_tokens\": 250,\n",
    "                    \"repetition_penalty\": 1.4\n",
    "                },\n",
    "                \"politics\": {\n",
    "                    \"num_beams\": 18,\n",
    "                    \"num_beam_groups\": 6,\n",
    "                    \"diversity_penalty\": 1.5,\n",
    "                    \"max_new_tokens\": 250,\n",
    "                    \"repetition_penalty\": 1.4\n",
    "                },\n",
    "                \"sports\": {\n",
    "                    \"num_beams\": 16,\n",
    "                    \"num_beam_groups\": 4,\n",
    "                    \"diversity_penalty\": 1.2,\n",
    "                    \"max_new_tokens\": 200,\n",
    "                    \"repetition_penalty\": 1.3\n",
    "                },\n",
    "                \"entertainment\": {\n",
    "                    \"num_beams\": 16,\n",
    "                    \"num_beam_groups\": 4,\n",
    "                    \"diversity_penalty\": 1.2,\n",
    "                    \"max_new_tokens\": 200,\n",
    "                    \"repetition_penalty\": 1.3\n",
    "                },\n",
    "                \"default\": {\n",
    "                    \"num_beams\": 20,\n",
    "                    \"num_beam_groups\": 5,\n",
    "                    \"diversity_penalty\": 1.5,\n",
    "                    \"max_new_tokens\": 250,\n",
    "                    \"repetition_penalty\": 1.4\n",
    "                }\n",
    "            }\n",
    "\n",
    "            # Apply domain parameters if domain is recognized\n",
    "            if domain in self.domain_generation_params:\n",
    "                domain_params = self.domain_generation_params[domain]\n",
    "                for param, value in domain_params.items():\n",
    "                    setattr(self, param, value)\n",
    "                domain_params_applied = True\n",
    "                if debug_output:\n",
    "                    print(f\"Applied newly defined domain-specific parameters for {domain}\")\n",
    "            else:\n",
    "                # Apply default parameters\n",
    "                default_params = self.domain_generation_params[\"default\"]\n",
    "                for param, value in default_params.items():\n",
    "                    setattr(self, param, value)\n",
    "                domain_params_applied = True\n",
    "                if debug_output:\n",
    "                    print(f\"Applied default parameters (domain {domain} not recognized)\")\n",
    "\n",
    "        # Extract named entities if enabled\n",
    "        named_entities = []\n",
    "        if self.use_ner:\n",
    "            named_entities = self.extract_named_entities(text)\n",
    "            print(f\"Extracted {len(named_entities)} named entities\")\n",
    "\n",
    "        # Split into chunks if needed\n",
    "        chunks = self.chunk_text(text, chunk_size=self.max_input_length, overlap=self.chunk_overlap)\n",
    "\n",
    "        all_keyphrases = []\n",
    "\n",
    "        # Try multiple prompt templates for more diverse keyphrases\n",
    "        # Use the selected template and also a quantity-focused template\n",
    "        templates_to_try = [self.prompt_template_idx]\n",
    "\n",
    "        # Add quantity-focused template if not already selected\n",
    "        quantity_template_idx = -1\n",
    "        if not hasattr(self, 'QUANTITY_TEMPLATE_ADDED'):\n",
    "            # Add a quantity-focused template if not already present\n",
    "            quantity_template = \"\"\"Extract as many relevant keyphrases as possible (1-5 words each) from this text.\n",
    "    Focus on quantity - aim for at least 15-20 keyphrases covering all important concepts.\n",
    "    Include both specific entities and broader topics.\n",
    "\n",
    "    Text: {text}\n",
    "\n",
    "    Keyphrases (comma-separated):\"\"\"\n",
    "\n",
    "            self.PROMPT_TEMPLATES.append(quantity_template)\n",
    "            quantity_template_idx = len(self.PROMPT_TEMPLATES) - 1\n",
    "            self.QUANTITY_TEMPLATE_ADDED = True\n",
    "            if debug_output:\n",
    "                print(f\"Added quantity-focused template at index {quantity_template_idx}\")\n",
    "        else:\n",
    "            # Find the quantity-focused template\n",
    "            for i, template in enumerate(self.PROMPT_TEMPLATES):\n",
    "                if \"as many relevant keyphrases as possible\" in template and \"15-20 keyphrases\" in template:\n",
    "                    quantity_template_idx = i\n",
    "                    break\n",
    "\n",
    "        if quantity_template_idx >= 0 and quantity_template_idx != self.prompt_template_idx:\n",
    "            templates_to_try.append(quantity_template_idx)\n",
    "\n",
    "        # Process each template\n",
    "        for template_idx in templates_to_try:\n",
    "            if debug_output and len(templates_to_try) > 1:\n",
    "                print(f\"\\nUsing template {template_idx}\" + (\" (quantity-focused)\" if template_idx == quantity_template_idx else \"\"))\n",
    "\n",
    "            # Process each chunk\n",
    "            for chunk in chunks:\n",
    "                # Format prompt using the current template\n",
    "                prompt = self.PROMPT_TEMPLATES[template_idx].format(text=chunk)\n",
    "\n",
    "                # Tokenize input\n",
    "                inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=self.max_length)\n",
    "                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "\n",
    "                # Determine generation strategy\n",
    "                using_diverse_beams = hasattr(self, 'num_beam_groups') and self.num_beam_groups > 1\n",
    "\n",
    "                # Set up generation parameters\n",
    "                generation_kwargs = {\n",
    "                    \"max_new_tokens\": self.max_new_tokens,\n",
    "                    \"num_beams\": self.num_beams,\n",
    "                    \"repetition_penalty\": self.repetition_penalty,\n",
    "                    \"length_penalty\": self.length_penalty,\n",
    "                    \"early_stopping\": True,\n",
    "                    \"no_repeat_ngram_size\": 2,  # Added to reduce repetition\n",
    "                }\n",
    "\n",
    "                # Configure generation strategy\n",
    "                if using_diverse_beams:\n",
    "                    # Diverse beam search (always with do_sample=False)\n",
    "                    generation_kwargs.update({\n",
    "                        \"do_sample\": False,\n",
    "                        \"num_beam_groups\": self.num_beam_groups,\n",
    "                        \"diversity_penalty\": self.diversity_penalty,\n",
    "                    })\n",
    "                elif self.use_sampling:\n",
    "                    # Sampling strategy\n",
    "                    generation_kwargs.update({\n",
    "                        \"do_sample\": True,\n",
    "                        \"top_k\": self.top_k,\n",
    "                        \"top_p\": self.top_p,\n",
    "                        \"temperature\": self.temperature,\n",
    "                    })\n",
    "                else:\n",
    "                    # Pure beam search\n",
    "                    generation_kwargs.update({\n",
    "                        \"do_sample\": False,\n",
    "                    })\n",
    "\n",
    "                # Generate more sequences for more candidates\n",
    "                # Increase from min(5, beams) to min(8, beams)\n",
    "                num_return_sequences = min(8, self.num_beams // self.num_beam_groups if using_diverse_beams else self.num_beams)\n",
    "                generation_kwargs[\"num_return_sequences\"] = num_return_sequences\n",
    "\n",
    "                # Generate output\n",
    "                with torch.no_grad():\n",
    "                    outputs = self.model.generate(\n",
    "                        **inputs,\n",
    "                        **generation_kwargs\n",
    "                    )\n",
    "\n",
    "                # Process each generated sequence\n",
    "                for i in range(num_return_sequences):\n",
    "                    # Decode output\n",
    "                    generated_text = self.tokenizer.decode(outputs[i], skip_special_tokens=True)\n",
    "\n",
    "                    # Print raw output for debugging if requested\n",
    "                    if debug_output and i == 0:  # Only print first sequence to avoid clutter\n",
    "                        print(\"\\n\" + \"=\"*50)\n",
    "                        print(\"RAW MODEL OUTPUT:\")\n",
    "                        print(\"-\"*50)\n",
    "                        print(generated_text)\n",
    "                        print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "                    # Extract keyphrases from generated text\n",
    "                    chunk_keyphrases = self.extract_keyphrases_from_generated_text(generated_text)\n",
    "                    all_keyphrases.extend(chunk_keyphrases)\n",
    "\n",
    "        # Add named entities to keyphrases\n",
    "        all_keyphrases.extend(named_entities)\n",
    "\n",
    "        # Clean up memory\n",
    "        self.clean_memory()\n",
    "\n",
    "        # Post-process keyphrases\n",
    "        if self.post_process:\n",
    "            all_keyphrases = self.post_process_keyphrases(all_keyphrases)\n",
    "\n",
    "        # Remove duplicates while preserving order\n",
    "        seen = set()\n",
    "        unique_keyphrases = []\n",
    "        for kp in all_keyphrases:\n",
    "            kp_lower = kp.lower()\n",
    "            if kp_lower not in seen:\n",
    "                seen.add(kp_lower)\n",
    "                unique_keyphrases.append(kp)\n",
    "\n",
    "        # Restore original parameters\n",
    "        if domain_params_applied:\n",
    "            for param, value in original_params.items():\n",
    "                if hasattr(self, param):\n",
    "                    setattr(self, param, value)\n",
    "            if debug_output:\n",
    "                print(\"Restored original generation parameters\")\n",
    "\n",
    "        if debug_output:\n",
    "            print(f\"Generated {len(unique_keyphrases)} unique keyphrases\")\n",
    "\n",
    "        return unique_keyphrases\n",
    "\n",
    "    def extract_keyphrases_from_generated_text(self, generated_text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Extract keyphrases from generated text using multiple extraction methods.\n",
    "\n",
    "        Args:\n",
    "            generated_text: Generated text from the model\n",
    "\n",
    "        Returns:\n",
    "            List of keyphrases\n",
    "        \"\"\"\n",
    "        all_keyphrases = []\n",
    "\n",
    "        # Method 1: Split by commas (primary method)\n",
    "        if ',' in generated_text:\n",
    "            comma_keyphrases = [kp.strip() for kp in generated_text.split(',')]\n",
    "            comma_keyphrases = [kp for kp in comma_keyphrases if kp and len(kp.split()) <= self.max_phrase_length]\n",
    "            all_keyphrases.extend(comma_keyphrases)\n",
    "\n",
    "        # Method 2: Split by newlines\n",
    "        if '\\n' in generated_text:\n",
    "            newline_keyphrases = [kp.strip() for kp in generated_text.split('\\n')]\n",
    "            newline_keyphrases = [kp for kp in newline_keyphrases if kp and len(kp.split()) <= self.max_phrase_length]\n",
    "            all_keyphrases.extend(newline_keyphrases)\n",
    "\n",
    "        # Method 3: Use regex to find potential phrases\n",
    "        # Find capitalized phrases or phrases in quotes\n",
    "        patterns = [\n",
    "            r'\"([^\"]+)\"',  # Text in double quotes\n",
    "            r'\\'([^\\']+)\\'',  # Text in single quotes\n",
    "            r'([A-Z][a-z]+ (?:[A-Z][a-z]+ )?(?:[A-Z][a-z]+)?)'  # Capitalized phrases\n",
    "        ]\n",
    "\n",
    "        for pattern in patterns:\n",
    "            matches = re.findall(pattern, generated_text)\n",
    "            # Filter by length\n",
    "            matches = [m for m in matches if 1 <= len(m.split()) <= self.max_phrase_length]\n",
    "            all_keyphrases.extend(matches)\n",
    "\n",
    "        # Method 4: Use POS tagging to find noun phrases\n",
    "        try:\n",
    "            words = word_tokenize(generated_text)\n",
    "            pos_tags = pos_tag(words)\n",
    "\n",
    "            i = 0\n",
    "            while i < len(pos_tags):\n",
    "                # Start with nouns\n",
    "                if i < len(pos_tags) and pos_tags[i][1].startswith('NN'):\n",
    "                    start = i\n",
    "                    while i < len(pos_tags) and pos_tags[i][1].startswith('NN'):\n",
    "                        i += 1\n",
    "\n",
    "                    # Extract the noun phrase\n",
    "                    phrase = ' '.join(words[start:i])\n",
    "                    if 1 <= len(phrase.split()) <= self.max_phrase_length:\n",
    "                        all_keyphrases.append(phrase)\n",
    "                    continue\n",
    "\n",
    "                i += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error in POS tagging: {str(e)}\")\n",
    "\n",
    "        # Method 5: Fallback to sliding window for short phrases\n",
    "        if not all_keyphrases and generated_text.strip():\n",
    "            words = generated_text.split()\n",
    "            for i in range(len(words)):\n",
    "                for j in range(1, min(self.max_phrase_length + 1, len(words) - i + 1)):\n",
    "                    phrase = ' '.join(words[i:i+j])\n",
    "                    if j >= self.min_phrase_length and not all(word.lower() in self.stopwords for word in words[i:i+j]):\n",
    "                        all_keyphrases.append(phrase)\n",
    "\n",
    "        # Remove duplicates while preserving order\n",
    "        seen = set()\n",
    "        unique_keyphrases = []\n",
    "        for kp in all_keyphrases:\n",
    "            kp_lower = kp.lower()\n",
    "            if kp_lower not in seen:\n",
    "                seen.add(kp_lower)\n",
    "                unique_keyphrases.append(kp)\n",
    "\n",
    "        # Additional cleaning to remove prompt artifacts and extra punctuation\n",
    "        final_keyphrases = []\n",
    "        for kp in unique_keyphrases:\n",
    "            # Remove prompt artifacts more aggressively\n",
    "            kp_cleaned = re.sub(r'^(Keyphrases|Keywords|Concepts|Topics|Entities|Key phrases)[:\\s]*', '', kp, flags=re.IGNORECASE).strip()\n",
    "\n",
    "            # Remove leading/trailing punctuation more aggressively\n",
    "            kp_cleaned = re.sub(r'^[^\\w\\s(]+|[^\\w\\s)]+$', '', kp_cleaned).strip() # Allow brackets if needed\n",
    "\n",
    "            # Remove list markers (numbers, bullets)\n",
    "            kp_cleaned = re.sub(r'^\\d+[\\.\\)]\\s*|\\*\\s*|•\\s*|→\\s*|-\\s*', '', kp_cleaned).strip()\n",
    "\n",
    "            # Additional check for empty or too short strings after cleaning\n",
    "            if len(kp_cleaned) > 1: # Require more than 1 character\n",
    "                final_keyphrases.append(kp_cleaned)\n",
    "\n",
    "        # Return the newly cleaned list\n",
    "        return final_keyphrases\n",
    "\n",
    "    def post_process_keyphrases(self, keyphrases: List[str]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Post-process keyphrases to improve quality.\n",
    "\n",
    "        Args:\n",
    "            keyphrases: List of keyphrases\n",
    "\n",
    "        Returns:\n",
    "            Improved list of keyphrases\n",
    "        \"\"\"\n",
    "        processed_keyphrases = []\n",
    "\n",
    "        for keyphrase in keyphrases:\n",
    "            # Skip empty keyphrases\n",
    "            if not keyphrase.strip():\n",
    "                continue\n",
    "\n",
    "            # Clean the keyphrase\n",
    "            kp = self.clean_keyphrase(keyphrase)\n",
    "\n",
    "            # Skip if keyphrase is empty after cleaning\n",
    "            if not kp:\n",
    "                continue\n",
    "\n",
    "            # Check phrase length\n",
    "            words = kp.split()\n",
    "            if len(words) < self.min_phrase_length or len(words) > self.max_phrase_length:\n",
    "                continue\n",
    "\n",
    "            # Filter stopwords if enabled\n",
    "            if self.filter_stopwords and len(words) == 1 and words[0].lower() in self.stopwords:\n",
    "                continue\n",
    "\n",
    "            # Add to processed keyphrases\n",
    "            processed_keyphrases.append(kp)\n",
    "\n",
    "        # Remove duplicates while preserving order\n",
    "        seen = set()\n",
    "        unique_keyphrases = []\n",
    "        for kp in processed_keyphrases:\n",
    "            if kp.lower() not in seen:\n",
    "                seen.add(kp.lower())\n",
    "                unique_keyphrases.append(kp)\n",
    "\n",
    "        # Prioritize multi-word phrases if enabled\n",
    "        if self.prioritize_multi_word:\n",
    "            # Sort by number of words (descending) and then by original order\n",
    "            indexed_keyphrases = [(i, kp) for i, kp in enumerate(unique_keyphrases)]\n",
    "            sorted_keyphrases = sorted(\n",
    "                indexed_keyphrases,\n",
    "                key=lambda x: (-len(x[1].split()), x[0])\n",
    "            )\n",
    "            unique_keyphrases = [kp for _, kp in sorted_keyphrases]\n",
    "\n",
    "        return unique_keyphrases\n",
    "\n",
    "    def clean_keyphrase(self, keyphrase: str) -> str:\n",
    "        \"\"\"\n",
    "        Clean a keyphrase by removing unwanted characters and normalizing.\n",
    "\n",
    "        Args:\n",
    "            keyphrase: Input keyphrase\n",
    "\n",
    "        Returns:\n",
    "            Cleaned keyphrase\n",
    "        \"\"\"\n",
    "        # Remove unwanted characters\n",
    "        kp = keyphrase.strip()\n",
    "\n",
    "        # Remove quotes\n",
    "        kp = re.sub(r'^[\"\\']|[\"\\']$', '', kp)\n",
    "\n",
    "        # Remove list markers\n",
    "        kp = re.sub(r'^\\d+\\.\\s*|\\*\\s*', '', kp)\n",
    "\n",
    "        # Remove punctuation at the beginning and end\n",
    "        kp = kp.strip(string.punctuation + ' ')\n",
    "\n",
    "        # Lemmatize if enabled\n",
    "        if self.use_lemmatization:\n",
    "            words = word_tokenize(kp)\n",
    "            lemmatized_words = []\n",
    "            for word in words:\n",
    "                # Only lemmatize if not a proper noun (not starting with capital)\n",
    "                if not word[0].isupper():\n",
    "                    lemmatized_words.append(self.lemmatizer.lemmatize(word.lower()))\n",
    "                else:\n",
    "                    lemmatized_words.append(word)\n",
    "            kp = ' '.join(lemmatized_words)\n",
    "\n",
    "        return kp\n",
    "\n",
    "    def extract_named_entities(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Extract named entities from text using spaCy NER.\n",
    "\n",
    "        Args:\n",
    "            text: Input text\n",
    "\n",
    "        Returns:\n",
    "            List of named entities\n",
    "        \"\"\"\n",
    "        if not self.use_ner:\n",
    "            return []\n",
    "\n",
    "        # Process text with spaCy\n",
    "        doc = self.nlp(text)\n",
    "\n",
    "        # Extract entities\n",
    "        entities = []\n",
    "        for ent in doc.ents:\n",
    "            # Filter by entity type and length\n",
    "            if ent.label_ in [\"PERSON\", \"ORG\", \"GPE\", \"PRODUCT\", \"WORK_OF_ART\", \"EVENT\", \"LAW\", \"FAC\"]:\n",
    "                # Check if entity is within our length constraints\n",
    "                words = ent.text.split()\n",
    "                if self.min_phrase_length <= len(words) <= self.max_phrase_length:\n",
    "                    # Clean the entity text\n",
    "                    clean_entity = self.clean_keyphrase(ent.text)\n",
    "                    if clean_entity:\n",
    "                        entities.append(clean_entity)\n",
    "\n",
    "        # Remove duplicates while preserving order\n",
    "        seen = set()\n",
    "        unique_entities = []\n",
    "        for entity in entities:\n",
    "            if entity.lower() not in seen:\n",
    "                seen.add(entity.lower())\n",
    "                unique_entities.append(entity)\n",
    "\n",
    "        return unique_entities\n",
    "\n",
    "    def get_domain_specific_parameters(self, domain: str, text_length: int) -> Tuple[float, float, int, int]:\n",
    "        \"\"\"\n",
    "        Get domain-specific parameters for keyphrase extraction.\n",
    "        Optimized for each domain to ensure high-quality keyphrases while maintaining quantity.\n",
    "\n",
    "        Args:\n",
    "            domain: Domain of the text (original domain from input or detected domain)\n",
    "            text_length: Length of the input text\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (base_threshold, quality_threshold, percentile, target_count)\n",
    "        \"\"\"\n",
    "        # Default parameters - more lenient to ensure we don't lose quantity\n",
    "        base_threshold = 0.08  # Reduced from 0.10 to ensure more candidates pass initial filtering\n",
    "        quality_threshold = 0.40  # Reduced from 0.45 to ensure more candidates pass quality filtering\n",
    "        percentile = 3  # Reduced from 5 to be more lenient\n",
    "\n",
    "        # Calculate target count based on text length\n",
    "        # For medium-length articles (300-500 words), aim for 8-12 keyphrases\n",
    "        if text_length < 200:\n",
    "            target_count = 8  # Increased from 6 to ensure minimum quantity\n",
    "        elif text_length < 500:\n",
    "            target_count = 10  # Medium texts get medium number of keyphrases\n",
    "        elif text_length < 1000:\n",
    "            target_count = 12  # Longer texts get more keyphrases\n",
    "        else:\n",
    "            target_count = 15  # Very long texts get the most keyphrases\n",
    "\n",
    "        # Domain-specific adjustments\n",
    "        domain = domain.lower() if domain else \"general\"\n",
    "\n",
    "        # Focus domains - carefully tuned parameters based on empirical testing\n",
    "\n",
    "        # Artificial Intelligence - technical domain with specific terminology\n",
    "        if domain in [\"artificial intelligence\", \"ai\", \"machine learning\", \"deep learning\"]:\n",
    "            base_threshold = 0.09  # Reduced from 0.12 to ensure quantity\n",
    "            quality_threshold = 0.45  # Reduced from 0.50 to ensure quantity\n",
    "            percentile = 5  # Reduced from 10 to be more lenient\n",
    "            target_count = 10  # Target 10 keyphrases for AI domain\n",
    "\n",
    "        # Cybersecurity - technical domain with specific terminology\n",
    "        elif domain in [\"cybersecurity\", \"security\", \"cyber\", \"infosec\"]:\n",
    "            base_threshold = 0.08  # Reduced from 0.10 to ensure quantity\n",
    "            quality_threshold = 0.42  # Reduced from 0.48 to ensure quantity\n",
    "            percentile = 5  # Reduced from 10 to be more lenient\n",
    "            target_count = 10  # Target 10 keyphrases for cybersecurity domain\n",
    "\n",
    "        # Automotive - more general domain with mix of technical and common terms\n",
    "        elif domain in [\"automotive\", \"cars\", \"vehicles\", \"auto industry\"]:\n",
    "            base_threshold = 0.07  # Reduced from 0.08 to ensure quantity\n",
    "            quality_threshold = 0.38  # Reduced from 0.40 to ensure quantity\n",
    "            percentile = 4  # Reduced from 8 to be more lenient\n",
    "            target_count = 9  # Target 9 keyphrases for automotive domain\n",
    "\n",
    "        # Food - general domain with specific terminology\n",
    "        elif domain in [\"food\", \"cooking\", \"cuisine\", \"recipe\", \"culinary\"]:\n",
    "            base_threshold = 0.07  # Reduced from 0.08 to ensure quantity\n",
    "            quality_threshold = 0.38  # Reduced from 0.40 to ensure quantity\n",
    "            percentile = 4  # Reduced from 8 to be more lenient\n",
    "            target_count = 9  # Target 9 keyphrases for food domain\n",
    "\n",
    "        # Environment - mix of technical and general terminology\n",
    "        elif domain in [\"environment\", \"climate\", \"sustainability\", \"ecology\"]:\n",
    "            base_threshold = 0.08  # Reduced from 0.09 to ensure quantity\n",
    "            quality_threshold = 0.40  # Reduced from 0.45 to ensure quantity\n",
    "            percentile = 4  # Reduced from 8 to be more lenient\n",
    "            target_count = 9  # Target 9 keyphrases for environment domain\n",
    "\n",
    "        # Real Estate - specific terminology with some general terms\n",
    "        elif domain in [\"real estate\", \"property\", \"housing\", \"real-estate\", \"realty\"]:\n",
    "            base_threshold = 0.08  # Reduced from 0.09 to ensure quantity\n",
    "            quality_threshold = 0.40  # Reduced from 0.45 to ensure quantity\n",
    "            percentile = 4  # Reduced from 8 to be more lenient\n",
    "            target_count = 9  # Target 9 keyphrases for real estate domain\n",
    "\n",
    "        # Entertainment - general domain with specific entities\n",
    "        elif domain in [\"entertainment\", \"movies\", \"film\", \"music\", \"television\", \"tv\", \"media\"]:\n",
    "            base_threshold = 0.07  # Reduced from 0.08 to ensure quantity\n",
    "            quality_threshold = 0.38  # Reduced from 0.40 to ensure quantity\n",
    "            percentile = 3  # Reduced from 5 to be more lenient\n",
    "            target_count = 8  # Target 8 keyphrases for entertainment domain\n",
    "\n",
    "        # Other domains - still optimized but not focus domains\n",
    "\n",
    "        # Technology - broader than AI/Cybersecurity\n",
    "        elif domain in [\"technology\", \"tech\", \"digital\", \"software\", \"hardware\"]:\n",
    "            base_threshold = 0.09  # Reduced from 0.12 to ensure quantity\n",
    "            quality_threshold = 0.42  # Reduced from 0.50 to ensure quantity\n",
    "            percentile = 5  # Reduced from 10 to be more lenient\n",
    "            target_count = 10  # Target 10 keyphrases for technology domain\n",
    "\n",
    "        # Business/Finance - medium thresholds\n",
    "        elif domain in [\"business\", \"finance\", \"economics\", \"economy\", \"market\"]:\n",
    "            base_threshold = 0.08  # Reduced from 0.09 to ensure quantity\n",
    "            quality_threshold = 0.40  # Reduced from 0.45 to ensure quantity\n",
    "            percentile = 4  # Reduced from 8 to be more lenient\n",
    "            target_count = 9  # Target 9 keyphrases for business domain\n",
    "\n",
    "        # Health - medium thresholds\n",
    "        elif domain in [\"health\", \"medical\", \"medicine\", \"healthcare\", \"wellness\"]:\n",
    "            base_threshold = 0.08  # Reduced from 0.09 to ensure quantity\n",
    "            quality_threshold = 0.40  # Reduced from 0.45 to ensure quantity\n",
    "            percentile = 4  # Reduced from 8 to be more lenient\n",
    "            target_count = 9  # Target 9 keyphrases for health domain\n",
    "\n",
    "        # Politics - higher thresholds but still ensuring quantity\n",
    "        elif domain in [\"politics\", \"government\", \"policy\", \"political\"]:\n",
    "            base_threshold = 0.09  # Reduced from 0.10 to ensure quantity\n",
    "            quality_threshold = 0.42  # Reduced from 0.48 to ensure quantity\n",
    "            percentile = 5  # Reduced from 10 to be more lenient\n",
    "            target_count = 9  # Target 9 keyphrases for politics domain\n",
    "\n",
    "        # Sports - lower thresholds\n",
    "        elif domain in [\"sports\", \"sport\", \"athletics\", \"games\"]:\n",
    "            base_threshold = 0.07  # Reduced from 0.08 to ensure quantity\n",
    "            quality_threshold = 0.38  # Reduced from 0.40 to ensure quantity\n",
    "            percentile = 3  # Reduced from 5 to be more lenient\n",
    "            target_count = 8  # Target 8 keyphrases for sports domain\n",
    "\n",
    "        # Science - medium-high thresholds\n",
    "        elif domain in [\"science\", \"scientific\", \"research\", \"biology\", \"chemistry\", \"physics\"]:\n",
    "            base_threshold = 0.09  # Reduced from 0.10 to ensure quantity\n",
    "            quality_threshold = 0.42  # Reduced from 0.45 to ensure quantity\n",
    "            percentile = 4  # Reduced from 8 to be more lenient\n",
    "            target_count = 9  # Target 9 keyphrases for science domain\n",
    "\n",
    "        # Space - medium-high thresholds\n",
    "        elif domain in [\"space\", \"astronomy\", \"cosmos\", \"astrophysics\", \"aerospace\"]:\n",
    "            base_threshold = 0.09  # Reduced from 0.10 to ensure quantity\n",
    "            quality_threshold = 0.42  # Reduced from 0.45 to ensure quantity\n",
    "            percentile = 4  # Reduced from 8 to be more lenient\n",
    "            target_count = 9  # Target 9 keyphrases for space domain\n",
    "\n",
    "        # Agriculture - lower thresholds\n",
    "        elif domain in [\"agriculture\", \"farming\", \"crops\", \"livestock\"]:\n",
    "            base_threshold = 0.07  # Reduced from 0.08 to ensure quantity\n",
    "            quality_threshold = 0.38  # Reduced from 0.40 to ensure quantity\n",
    "            percentile = 3  # Reduced from 5 to be more lenient\n",
    "            target_count = 8  # Target 8 keyphrases for agriculture domain\n",
    "\n",
    "        # Mental Health - medium thresholds\n",
    "        elif domain in [\"mental health\", \"psychology\", \"psychiatry\", \"behavioral health\"]:\n",
    "            base_threshold = 0.08  # Reduced from 0.09 to ensure quantity\n",
    "            quality_threshold = 0.40  # Reduced from 0.45 to ensure quantity\n",
    "            percentile = 4  # Reduced from 8 to be more lenient\n",
    "            target_count = 9  # Target 9 keyphrases for mental health domain\n",
    "\n",
    "        # Content density adjustment - dynamically adjust based on text characteristics\n",
    "        if hasattr(self, 'content_density') and self.content_density is not None:\n",
    "            content_density = self.content_density\n",
    "            # For very dense content (lots of unique words), be more lenient\n",
    "            if content_density > 0.7:\n",
    "                base_threshold = max(0.05, base_threshold - 0.02)\n",
    "                quality_threshold = max(0.35, quality_threshold - 0.03)\n",
    "                percentile = max(2, percentile - 1)\n",
    "            # For very sparse content, be slightly stricter\n",
    "            elif content_density < 0.4:\n",
    "                base_threshold = min(0.12, base_threshold + 0.01)\n",
    "                quality_threshold = min(0.48, quality_threshold + 0.02)\n",
    "                percentile = min(10, percentile + 1)\n",
    "\n",
    "        # Print the domain-specific parameters for debugging\n",
    "        print(f\"\\nUsing parameters for domain '{domain}':\\n\" +\n",
    "              f\"  - base_threshold: {base_threshold:.3f}\\n\" +\n",
    "              f\"  - quality_threshold: {quality_threshold:.3f}\\n\" +\n",
    "              f\"  - percentile: {percentile}\\n\" +\n",
    "              f\"  - target_count: {target_count}\")\n",
    "\n",
    "        return base_threshold, quality_threshold, percentile, target_count\n",
    "\n",
    "    def extract_keyphrases_with_scores(self, text: str, min_keyphrases: int = 5, max_keyphrases: int = 15, min_score: float = 0.1, optimize_params: bool = False, original_domain: str = None) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Extract keyphrases with confidence scores based on semantic relevance.\n",
    "        Uses a quality-driven approach that dynamically determines the number of keyphrases.\n",
    "\n",
    "        Args:\n",
    "            text: Input text\n",
    "            min_keyphrases: Minimum number of keyphrases to return (used as fallback)\n",
    "            max_keyphrases: Maximum number of keyphrases to return (used as upper limit)\n",
    "            min_score: Minimum score threshold for keyphrases\n",
    "            optimize_params: Whether to optimize generation parameters for this text\n",
    "            original_domain: Original domain of the text (if known). This will override the detected domain.\n",
    "\n",
    "        Returns:\n",
    "            List of (keyphrase, score) tuples sorted by score\n",
    "        \"\"\"\n",
    "        # Save original parameters if optimizing\n",
    "        if optimize_params:\n",
    "            original_use_sampling = getattr(self, 'use_sampling', True)\n",
    "            original_template_idx = self.prompt_template_idx\n",
    "\n",
    "            # Try to optimize parameters for this text\n",
    "            print(\"Optimizing generation parameters...\")\n",
    "            self._optimize_generation_params(text)\n",
    "\n",
    "        # Calculate text length for normalization\n",
    "        text_length = len(text.split())\n",
    "\n",
    "        # Use original domain if provided, otherwise detect domain\n",
    "        if original_domain:\n",
    "            domain = original_domain\n",
    "            print(f\"Using original domain: {domain}\")\n",
    "        else:\n",
    "            # Detect domain\n",
    "            domain = self.detect_domain(text)\n",
    "            print(f\"Detected domain: {domain}\")\n",
    "\n",
    "        # Generate keyphrases\n",
    "        keyphrases = self.generate_keyphrases(text, original_domain=original_domain)\n",
    "        print(f\"Generated {len(keyphrases)} candidate keyphrases\")\n",
    "\n",
    "        # If we have too few keyphrases, try a different approach\n",
    "        if len(keyphrases) < min_keyphrases:\n",
    "            print(f\"Too few keyphrases ({len(keyphrases)}), trying different parameters...\")\n",
    "\n",
    "            # Try with different parameters\n",
    "            original_params = {\n",
    "                'use_sampling': getattr(self, 'use_sampling', True),\n",
    "                'num_beams': self.num_beams,\n",
    "                'prompt_template_idx': self.prompt_template_idx\n",
    "            }\n",
    "\n",
    "            # Try diverse beam search if not already using it\n",
    "            if not hasattr(self, 'num_beam_groups') or self.num_beam_groups <= 1:\n",
    "                self.use_sampling = False\n",
    "                self.num_beams = 12\n",
    "                self.num_beam_groups = 4\n",
    "                self.diversity_penalty = 1.0\n",
    "                print(\"Trying diverse beam search...\")\n",
    "            else:\n",
    "                # Try pure beam search if already using diverse beam search\n",
    "                self.use_sampling = False\n",
    "                self.num_beams = 12\n",
    "                if hasattr(self, 'num_beam_groups'):\n",
    "                    delattr(self, 'num_beam_groups')\n",
    "                if hasattr(self, 'diversity_penalty'):\n",
    "                    delattr(self, 'diversity_penalty')\n",
    "                print(\"Trying pure beam search...\")\n",
    "\n",
    "            # Try a different prompt template\n",
    "            self.prompt_template_idx = (self.prompt_template_idx + 1) % len(self.PROMPT_TEMPLATES)\n",
    "\n",
    "            # Generate keyphrases with new parameters\n",
    "            backup_keyphrases = self.generate_keyphrases(text)\n",
    "            print(f\"Generated {len(backup_keyphrases)} keyphrases with alternative parameters\")\n",
    "\n",
    "            # Restore original parameters\n",
    "            self.use_sampling = original_params['use_sampling']\n",
    "            self.num_beams = original_params['num_beams']\n",
    "            self.prompt_template_idx = original_params['prompt_template_idx']\n",
    "\n",
    "            # Remove beam group attributes if they weren't in original params\n",
    "            if 'num_beam_groups' not in original_params and hasattr(self, 'num_beam_groups'):\n",
    "                delattr(self, 'num_beam_groups')\n",
    "            if 'diversity_penalty' not in original_params and hasattr(self, 'diversity_penalty'):\n",
    "                delattr(self, 'diversity_penalty')\n",
    "\n",
    "            # Combine keyphrases\n",
    "            keyphrases.extend(backup_keyphrases)\n",
    "\n",
    "            # Remove duplicates\n",
    "            seen = set()\n",
    "            unique_keyphrases = []\n",
    "            for kp in keyphrases:\n",
    "                if kp.lower() not in seen:\n",
    "                    seen.add(kp.lower())\n",
    "                    unique_keyphrases.append(kp)\n",
    "\n",
    "            keyphrases = unique_keyphrases\n",
    "            print(f\"Combined unique keyphrases: {len(keyphrases)}\")\n",
    "\n",
    "        # Score keyphrases by semantic relevance\n",
    "        scored_keyphrases = self.score_keyphrases_by_relevance(keyphrases, text)\n",
    "\n",
    "        # Boost domain-specific concepts\n",
    "        boosted_keyphrases = self.boost_domain_specific_concepts(scored_keyphrases, domain)\n",
    "\n",
    "        # Enhance semantic coherence\n",
    "        coherent_keyphrases = self.enhance_semantic_coherence(boosted_keyphrases, text)\n",
    "\n",
    "        # Enhance semantic diversity\n",
    "        diverse_keyphrases = self.enhance_semantic_diversity(coherent_keyphrases, text)\n",
    "\n",
    "        # Remove redundant keyphrases (moved before generic filtering for better results)\n",
    "        filtered_keyphrases = self.remove_redundant_keyphrases(diverse_keyphrases, domain=domain)\n",
    "\n",
    "        # Apply minimum score threshold\n",
    "        filtered_keyphrases = [(kp, score) for kp, score in filtered_keyphrases if score >= min_score]\n",
    "\n",
    "        # Filter out generic terms\n",
    "        filtered_keyphrases = self.filter_generic_terms(filtered_keyphrases, domain, text)\n",
    "\n",
    "        # Apply quality-driven filtering and selection\n",
    "        final_keyphrases = self.filter_and_select_by_quality(text, domain, filtered_keyphrases, original_domain=original_domain)\n",
    "\n",
    "        # If we have too few keyphrases, try to extract more with a different approach\n",
    "        if len(final_keyphrases) < min(3, min_keyphrases) and optimize_params:\n",
    "            # Try a different prompt template\n",
    "            backup_template_idx = (self.prompt_template_idx + 1) % len(self.PROMPT_TEMPLATES)\n",
    "            self.prompt_template_idx = backup_template_idx\n",
    "\n",
    "            # Try with pure beam search if we were using sampling, or vice versa\n",
    "            self.use_sampling = not original_use_sampling\n",
    "\n",
    "            backup_keyphrases = self.generate_keyphrases(text, original_domain=original_domain)\n",
    "\n",
    "            # Restore parameters\n",
    "            self.prompt_template_idx = original_template_idx\n",
    "            self.use_sampling = original_use_sampling\n",
    "\n",
    "            if backup_keyphrases:\n",
    "                # Score and filter backup keyphrases\n",
    "                backup_scored = self.score_keyphrases_by_relevance(backup_keyphrases, text)\n",
    "\n",
    "                # Add backup keyphrases that aren't redundant with existing ones\n",
    "                existing_kps = [kp.lower() for kp, _ in final_keyphrases]\n",
    "                for kp, score in backup_scored:\n",
    "                    if kp.lower() not in existing_kps and score >= min_score:\n",
    "                        final_keyphrases.append((kp, score))\n",
    "\n",
    "                # Re-sort by score\n",
    "                final_keyphrases.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Apply a very high absolute limit if needed\n",
    "        max_reasonable_keyphrases = min(max_keyphrases, max(min_keyphrases, int(text_length / 50)))\n",
    "        if len(final_keyphrases) > max_reasonable_keyphrases:\n",
    "            final_keyphrases = final_keyphrases[:max_reasonable_keyphrases]\n",
    "\n",
    "        return final_keyphrases\n",
    "\n",
    "    def extract_keyphrases_optimized(self, text: str, original_domain: str = None) -> List[Tuple[str, float]]:\n",
    "\n",
    "        return self.extract_keyphrases_with_scores(text, optimize_params=True, original_domain=original_domain)\n",
    "\n",
    "    def extract_keyphrases(self, text: str, original_domain: str = None) -> List[str]:\n",
    "        \"\"\"\n",
    "        Extract keyphrases without scores.\n",
    "\n",
    "        Args:\n",
    "            text: Input text\n",
    "            original_domain: Original domain of the text (if known). This will override the detected domain.\n",
    "\n",
    "        Returns:\n",
    "            List of keyphrases\n",
    "        \"\"\"\n",
    "        scored_keyphrases = self.extract_keyphrases_with_scores(text, original_domain=original_domain)\n",
    "        # Ensure we return all keyphrases without truncation\n",
    "        keyphrases = [kp for kp, _ in scored_keyphrases]\n",
    "        final_count = len(keyphrases)\n",
    "        print(f\"Final keyphrase count: {final_count}\")\n",
    "        print(f\"Returning {final_count} keyphrases from extract_keyphrases\")\n",
    "\n",
    "        # IMPORTANT: Make sure the number of keyphrases printed matches the number logged\n",
    "        # This is a critical fix for the count discrepancy bug\n",
    "        return keyphrases\n",
    "\n",
    "    def debug_keyphrase_extraction(self, text: str) -> None:\n",
    "        \"\"\"\n",
    "        Debug the keyphrase extraction process by examining raw outputs and intermediate results.\n",
    "\n",
    "        Args:\n",
    "            text: Input text\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"DEBUGGING KEYPHRASE EXTRACTION\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        # 1. Examine raw T5 output\n",
    "        print(\"\\nSTEP 1: Examining raw T5 output\")\n",
    "        self.examine_raw_output(text)\n",
    "\n",
    "        # 2. Test different generation parameters\n",
    "        print(\"\\nSTEP 2: Testing different generation parameters\")\n",
    "        param_results = self.test_generation_params(text, debug_output=False)\n",
    "\n",
    "        # Find the parameter combination that produced the most keyphrases\n",
    "        best_param_key = max(param_results.items(), key=lambda x: len(x[1]), default=(None, []))[0]\n",
    "        if best_param_key:\n",
    "            print(f\"\\nBest parameter combination: {best_param_key}\")\n",
    "            print(f\"Generated {len(param_results[best_param_key])} keyphrases\")\n",
    "\n",
    "        # 3. Extract keyphrases with optimized parameters\n",
    "        print(\"\\nSTEP 3: Extracting keyphrases with optimized parameters\")\n",
    "        keyphrases = self.extract_keyphrases_optimized(text)\n",
    "\n",
    "        print(f\"\\nFinal keyphrases ({len(keyphrases)}):\")\n",
    "        for kp, score in keyphrases:\n",
    "            print(f\"- {kp}: {score:.4f}\")\n",
    "\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "    def initialize_domain_components(self):\n",
    "        \"\"\"Initialize domain-specific components.\"\"\"\n",
    "        # Add domain-specific prompts\n",
    "        self.add_domain_specific_prompts()\n",
    "\n",
    "        # Initialize domain detection components\n",
    "        self.DOMAIN_LABELS = [\n",
    "            \"technology\", \"business\", \"health\", \"politics\", \"sports\",\n",
    "            \"entertainment\", \"science\", \"environment\", \"world\", \"education\",\n",
    "            \"food\", \"travel\", \"automotive\", \"real estate\", \"cybersecurity\",\n",
    "            \"artificial intelligence\", \"space\", \"agriculture\", \"mental health\"\n",
    "        ]\n",
    "\n",
    "        # Domain-specific confidence thresholds based on empirical testing\n",
    "        self.domain_thresholds = {\n",
    "            'entertainment': 0.3,  # High confidence (0.9892)\n",
    "            'sports': 0.3,         # High confidence (0.9605)\n",
    "            'politics': 0.3,       # High confidence (0.8584)\n",
    "            'education': 0.4,      # Good confidence (0.7171)\n",
    "            'technology': 0.35,    # Medium confidence (0.4192)\n",
    "            'real estate': 0.35,   # Medium confidence (0.4064)\n",
    "            'food': 0.35,          # Medium confidence (0.3942)\n",
    "            'automotive': 0.3,     # Lower confidence (0.3332)\n",
    "            'science': 0.3,        # Lower confidence (0.3321)\n",
    "            'space': 0.3,          # Lower confidence (0.3119)\n",
    "            'health': 0.25,        # Lower confidence (0.2910)\n",
    "            'travel': 0.22,        # Lowest confidence (0.2419)\n",
    "        }\n",
    "\n",
    "        # Default threshold for domains not in the list\n",
    "        self.default_threshold = 0.4\n",
    "\n",
    "        # Initialize mDeBERTa zero-shot classifier if enabled\n",
    "        if hasattr(self, 'use_mdeberta_domain_detection') and self.use_mdeberta_domain_detection:\n",
    "            try:\n",
    "                print(\"Initializing mDeBERTa zero-shot classifier for domain detection...\")\n",
    "                device = 0 if (self.use_gpu and torch.cuda.is_available()) else -1\n",
    "                self.zero_shot_classifier = pipeline(\n",
    "                    \"zero-shot-classification\",\n",
    "                    model=\"MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\",\n",
    "                    device=device\n",
    "                )\n",
    "                print(\"mDeBERTa zero-shot classifier initialized successfully\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error initializing mDeBERTa zero-shot classifier: {str(e)}\")\n",
    "                print(\"Falling back to alternative domain detection methods\")\n",
    "\n",
    "        # Log initialization\n",
    "        print(\"Domain-specific components initialized\")\n",
    "\n",
    "    def try_all_prompts(self, text: str) -> Dict[int, List[str]]:\n",
    "        \"\"\"\n",
    "        Try all prompt templates and return results from each.\n",
    "\n",
    "        Args:\n",
    "            text: Input text\n",
    "\n",
    "        Returns:\n",
    "            Dictionary mapping template index to list of keyphrases\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        original_template_idx = self.prompt_template_idx\n",
    "\n",
    "        for i in range(len(self.PROMPT_TEMPLATES)):\n",
    "            self.prompt_template_idx = i\n",
    "            results[i] = self.generate_keyphrases(text)\n",
    "\n",
    "        # Restore original template\n",
    "        self.prompt_template_idx = original_template_idx\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "    def filter_generic_terms(self, keyphrases: List[Tuple[str, float]], domain: str, text: str) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Filter out generic terms using context-aware semantic filtering.\n",
    "        Improved to be less aggressive while still maintaining quality.\n",
    "\n",
    "        This method uses a combination of:\n",
    "        1. Semantic relevance to the document\n",
    "        2. Domain awareness with domain-specific exceptions\n",
    "        3. Statistical measures (TF-IDF principles)\n",
    "        4. Pattern-based filtering for truly generic constructs\n",
    "        5. Recovery mechanisms to maintain keyphrase quantity\n",
    "\n",
    "        Args:\n",
    "            keyphrases: List of (keyphrase, score) tuples\n",
    "            domain: Detected domain\n",
    "            text: Original text\n",
    "\n",
    "        Returns:\n",
    "            Filtered list of (keyphrase, score) tuples\n",
    "        \"\"\"\n",
    "        if not keyphrases:\n",
    "            return []\n",
    "\n",
    "        # Track original count for recovery mechanism\n",
    "        original_count = len(keyphrases)\n",
    "\n",
    "        # Get generic terms for this domain\n",
    "        domain_generics = set(self.generic_terms.get(domain, self.generic_terms[\"general\"]))\n",
    "        # Also include general generics for all domains\n",
    "        general_generics = set(self.generic_terms[\"general\"])\n",
    "        all_generics = domain_generics.union(general_generics)\n",
    "\n",
    "        # IMPROVEMENT: Domain-specific exceptions - terms that are generic in general\n",
    "        # but important in specific domains\n",
    "        domain_exceptions = {\n",
    "            \"technology\": [\"digital\", \"software\", \"hardware\", \"platform\", \"system\", \"application\", \"device\", \"technology\", \"tech\", \"solution\"],\n",
    "            \"artificial intelligence\": [\"model\", \"algorithm\", \"system\", \"data\", \"learning\", \"intelligence\", \"ai\", \"neural\", \"training\"],\n",
    "            \"cybersecurity\": [\"security\", \"protection\", \"threat\", \"attack\", \"defense\", \"vulnerability\", \"risk\", \"cyber\", \"breach\"],\n",
    "            \"automotive\": [\"vehicle\", \"car\", \"driver\", \"driving\", \"auto\", \"automotive\", \"engine\", \"motor\", \"fuel\"],\n",
    "            \"food\": [\"food\", \"cooking\", \"recipe\", \"ingredient\", \"meal\", \"dish\", \"flavor\", \"taste\", \"cuisine\"],\n",
    "            \"environment\": [\"environment\", \"climate\", \"sustainable\", \"green\", \"energy\", \"conservation\", \"pollution\", \"emission\", \"waste\"],\n",
    "            \"real estate\": [\"property\", \"home\", \"house\", \"market\", \"real estate\", \"housing\", \"mortgage\", \"buyer\", \"seller\"],\n",
    "            \"entertainment\": [\"movie\", \"film\", \"show\", \"music\", \"entertainment\", \"actor\", \"actress\", \"director\", \"performance\"],\n",
    "            \"health\": [\"health\", \"medical\", \"patient\", \"treatment\", \"doctor\", \"hospital\", \"care\", \"disease\", \"condition\"],\n",
    "            \"science\": [\"research\", \"study\", \"scientist\", \"experiment\", \"data\", \"analysis\", \"discovery\", \"finding\", \"evidence\"],\n",
    "            \"sports\": [\"team\", \"player\", \"game\", \"match\", \"season\", \"sport\", \"championship\", \"league\", \"tournament\"],\n",
    "            \"politics\": [\"government\", \"policy\", \"political\", \"election\", \"official\", \"administration\", \"law\", \"legislation\", \"regulation\"]\n",
    "        }\n",
    "\n",
    "        # Get exceptions for this domain\n",
    "        exceptions = set(domain_exceptions.get(domain, []))\n",
    "\n",
    "        # IMPROVEMENT: Add exceptions for similar domains\n",
    "        if domain in [\"artificial intelligence\", \"cybersecurity\"]:\n",
    "            exceptions.update(domain_exceptions[\"technology\"])\n",
    "        elif domain in [\"automotive\"]:\n",
    "            exceptions.update([\"technology\", \"engineering\", \"manufacturing\"])\n",
    "\n",
    "        # Calculate text statistics for contextual filtering\n",
    "        text_lower = text.lower()\n",
    "        text_word_count = len(text.split())\n",
    "\n",
    "        # Create a document embedding for contextual relevance comparison\n",
    "        # Only if we have a sentence transformer model available\n",
    "        doc_embedding = None\n",
    "        if hasattr(self, 'sentence_model') and self.sentence_model is not None:\n",
    "            try:\n",
    "                doc_embedding = self.sentence_model.encode([text], show_progress_bar=False)[0]\n",
    "                # Ensure numpy is available for vector operations\n",
    "                import numpy as np\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not create document embedding: {e}\")\n",
    "\n",
    "        # Calculate term frequency for all words in the document\n",
    "        # This helps us identify which terms are distinctive to this document\n",
    "        words = re.findall(r'\\b\\w+\\b', text_lower)\n",
    "        word_freq = {}\n",
    "        for word in words:\n",
    "            if word not in word_freq:\n",
    "                word_freq[word] = 0\n",
    "            word_freq[word] += 1\n",
    "\n",
    "        # Calculate max frequency for normalization\n",
    "        max_freq = max(word_freq.values()) if word_freq else 1\n",
    "\n",
    "        filtered_keyphrases = []\n",
    "        filtered_out = []\n",
    "\n",
    "        # First pass: calculate contextual importance for each keyphrase\n",
    "        keyphrase_importance = {}\n",
    "        for keyphrase, score in keyphrases:\n",
    "            kp_lower = keyphrase.lower()\n",
    "            kp_words = kp_lower.split()\n",
    "\n",
    "            # Base importance is the semantic score\n",
    "            importance = score\n",
    "\n",
    "            # IMPROVEMENT: Higher length bonus for multi-word phrases\n",
    "            # Factor 1: Length bonus - multi-word phrases are generally more specific\n",
    "            length_factor = min(1.8, 1.0 + (len(kp_words) - 1) * 0.3)  # 1.0, 1.3, 1.6, 1.8 for 1,2,3,4+ words\n",
    "            importance *= length_factor\n",
    "\n",
    "            # Factor 2: TF-IDF-like weighting - terms that appear in the document but aren't too common\n",
    "            tf_idf_factor = 1.0\n",
    "            if kp_words:\n",
    "                # Calculate normalized term frequency across the keyphrase words\n",
    "                term_freqs = [word_freq.get(word, 0) / max_freq for word in kp_words]\n",
    "                avg_tf = sum(term_freqs) / len(term_freqs) if term_freqs else 0\n",
    "\n",
    "                # IMPROVEMENT: More balanced TF-IDF weighting\n",
    "                # We want terms that appear in the document (high tf) but aren't too common\n",
    "                if avg_tf > 0:\n",
    "                    # Penalize very common terms (close to max_freq) but less aggressively\n",
    "                    if avg_tf > 0.9:  # Changed from 0.8 to 0.9\n",
    "                        tf_idf_factor = 0.85  # Less penalty (was 0.8)\n",
    "                    # Boost terms with moderate frequency - wider range\n",
    "                    elif 0.15 <= avg_tf <= 0.8:  # Changed from 0.2-0.7 to 0.15-0.8\n",
    "                        tf_idf_factor = 1.25  # Higher boost (was 1.2)\n",
    "\n",
    "            importance *= tf_idf_factor\n",
    "\n",
    "            # Factor 3: Semantic relevance to the document\n",
    "            if doc_embedding is not None:\n",
    "                try:\n",
    "                    # Get keyphrase embedding\n",
    "                    kp_embedding = self.sentence_model.encode([kp_lower], show_progress_bar=False)[0]\n",
    "                    # Calculate cosine similarity with document\n",
    "                    similarity = np.dot(kp_embedding, doc_embedding) / (\n",
    "                        np.linalg.norm(kp_embedding) * np.linalg.norm(doc_embedding)\n",
    "                    )\n",
    "                    # IMPROVEMENT: More balanced semantic factor (0.6-1.6 range instead of 0.5-1.5)\n",
    "                    semantic_factor = 0.6 + similarity\n",
    "                    importance *= semantic_factor\n",
    "                except Exception:\n",
    "                    # If embedding fails, don't modify importance\n",
    "                    pass\n",
    "\n",
    "            # IMPROVEMENT: Domain-specific importance boost\n",
    "            # Boost importance for domain-specific terms\n",
    "            if any(word in exceptions for word in kp_words):\n",
    "                domain_boost = 1.3  # 30% boost for domain-specific terms\n",
    "                importance *= domain_boost\n",
    "\n",
    "            # Store the calculated importance\n",
    "            keyphrase_importance[keyphrase] = importance\n",
    "\n",
    "        # Second pass: apply filters with context-aware thresholds\n",
    "        for keyphrase, score in keyphrases:\n",
    "            kp_lower = keyphrase.lower()\n",
    "            kp_words = kp_lower.split()\n",
    "\n",
    "            # Get the contextual importance\n",
    "            importance = keyphrase_importance.get(keyphrase, score)\n",
    "\n",
    "            # IMPROVEMENT: More balanced importance factor\n",
    "            # Adjust thresholds based on importance\n",
    "            # More important keyphrases get more lenient filtering\n",
    "            importance_factor = importance / max(score, 0.1)  # Avoid division by zero or very small numbers\n",
    "\n",
    "            # IMPROVEMENT: Domain-specific exception handling\n",
    "            # Skip filtering for domain-specific exceptions with good scores\n",
    "            if any(word in exceptions for word in kp_words) and score > 0.3:\n",
    "                filtered_keyphrases.append((keyphrase, score))\n",
    "                continue\n",
    "\n",
    "            # IMPROVEMENT: Less aggressive exact match filtering\n",
    "            # Skip exact matches with generic terms only if importance is low and not a domain exception\n",
    "            if kp_lower in all_generics and importance_factor < 1.1 and kp_lower not in exceptions:  # Changed from 1.2 to 1.1\n",
    "                filtered_out.append((keyphrase, score, \"exact generic match\"))\n",
    "                continue\n",
    "\n",
    "            # IMPROVEMENT: Less aggressive single-word filtering\n",
    "            # For single-word keyphrases, apply adaptive filtering\n",
    "            if len(kp_words) == 1:\n",
    "                # Single word threshold adjusted by importance - less aggressive\n",
    "                single_word_threshold = 0.32 / importance_factor  # Reduced from 0.35\n",
    "\n",
    "                if kp_lower in all_generics and score < single_word_threshold and kp_lower not in exceptions:\n",
    "                    filtered_out.append((keyphrase, score, \"generic single word\"))\n",
    "                    continue\n",
    "\n",
    "            # IMPROVEMENT: Less aggressive multi-word filtering\n",
    "            # For multi-word phrases, check if they're mostly generic with adaptive threshold\n",
    "            if len(kp_words) > 1:\n",
    "                # Calculate what percentage of words are generic, excluding domain exceptions\n",
    "                generic_word_count = sum(1 for word in kp_words if word in all_generics and word not in exceptions)\n",
    "                generic_percentage = generic_word_count / len(kp_words)\n",
    "\n",
    "                # Adaptive threshold based on importance - less aggressive\n",
    "                generic_threshold = 0.8 if importance_factor < 1.0 else 0.9  # Increased from 0.75/0.85\n",
    "                score_threshold = 0.4 / importance_factor  # Reduced from 0.45\n",
    "\n",
    "                if generic_percentage > generic_threshold and score < score_threshold:\n",
    "                    filtered_out.append((keyphrase, score, f\"{int(generic_percentage*100)}% generic words\"))\n",
    "                    continue\n",
    "\n",
    "            # IMPROVEMENT: Less aggressive frequency filtering\n",
    "            # Check for phrases that are too common in the text with adaptive threshold\n",
    "            kp_frequency = text_lower.count(kp_lower)\n",
    "            if kp_frequency > 0:\n",
    "                frequency_ratio = kp_frequency / (text_word_count / 100)  # Occurrences per 100 words\n",
    "\n",
    "                # Adaptive threshold based on importance - less aggressive\n",
    "                frequency_threshold = 2.5 if importance_factor < 1.0 else 3.5  # Increased from 2.0/3.0\n",
    "                score_threshold = 0.4 / importance_factor  # Reduced from 0.45\n",
    "\n",
    "                if frequency_ratio > frequency_threshold and score < score_threshold:\n",
    "                    filtered_out.append((keyphrase, score, f\"too frequent ({frequency_ratio:.1f}/100 words)\"))\n",
    "                    continue\n",
    "\n",
    "            # IMPROVEMENT: More specific vague patterns\n",
    "            # Check for vague descriptive phrases - these are almost always generic\n",
    "            # regardless of context\n",
    "            vague_patterns = [\n",
    "                r'^(many|various|different|several|some) [a-z]+s?$',\n",
    "                r'^(important|significant|major|key) [a-z]+$',\n",
    "                r'^(new|recent|latest|current) [a-z]+$',\n",
    "                r'^(high|low|large|small) [a-z]+$',\n",
    "                r'^[a-z]+ (issues?|concerns?|matters?|aspects?)$',\n",
    "                # IMPROVEMENT: Additional patterns for truly vague phrases\n",
    "                r'^(certain|specific|particular) [a-z]+s?$',\n",
    "                r'^(overall|general|basic) [a-z]+s?$'\n",
    "            ]\n",
    "\n",
    "            # IMPROVEMENT: Less aggressive vague pattern filtering\n",
    "            # Even for vague patterns, use adaptive threshold\n",
    "            vague_score_threshold = 0.55 / importance_factor  # Reduced from 0.6\n",
    "\n",
    "            if any(re.match(pattern, kp_lower) for pattern in vague_patterns) and score < vague_score_threshold:\n",
    "                filtered_out.append((keyphrase, score, \"vague pattern\"))\n",
    "                continue\n",
    "\n",
    "            # If passed all filters, keep the keyphrase\n",
    "            filtered_keyphrases.append((keyphrase, score))\n",
    "\n",
    "        # IMPROVEMENT: More aggressive recovery mechanism\n",
    "        # Apply Pareto principle to filtering: if we've filtered too many (>70%),\n",
    "        # recover some of the higher-scoring ones to ensure we have enough keyphrases\n",
    "        if filtered_keyphrases and len(filtered_out) > 2.3 * len(filtered_keyphrases):  # Changed from 4x to 2.3x\n",
    "            # Sort filtered out by score\n",
    "            filtered_out.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "            # IMPROVEMENT: Recover more keyphrases\n",
    "            # Recover top 30% of filtered out keyphrases (was 20%)\n",
    "            recovery_count = min(len(filtered_out) // 2, 8)  # Recover up to 8 keyphrases (was 5)\n",
    "\n",
    "            for i in range(recovery_count):\n",
    "                if i < len(filtered_out) and filtered_out[i][1] > 0.2:  # Reduced threshold from 0.25 to 0.2\n",
    "                    keyphrase, score, reason = filtered_out[i]\n",
    "                    filtered_keyphrases.append((keyphrase, score))\n",
    "                    print(f\"Recovered keyphrase due to Pareto principle: {keyphrase} ({score:.2f}) - was filtered for: {reason}\")\n",
    "\n",
    "        # IMPROVEMENT: Additional recovery for domain-specific focus\n",
    "        # If we're in one of our focus domains, ensure we have enough keyphrases\n",
    "        focus_domains = [\"artificial intelligence\", \"cybersecurity\", \"automotive\",\n",
    "                        \"food\", \"environment\", \"real estate\", \"entertainment\"]\n",
    "\n",
    "        if domain in focus_domains and len(filtered_keyphrases) < 0.7 * original_count:\n",
    "            # We've filtered too many in a focus domain - recover more\n",
    "            additional_recovery = int(0.7 * original_count) - len(filtered_keyphrases)\n",
    "            if additional_recovery > 0 and filtered_out:\n",
    "                print(f\"Additional recovery for focus domain '{domain}': recovering {additional_recovery} more keyphrases\")\n",
    "\n",
    "                # Skip keyphrases we've already recovered\n",
    "                already_recovered = set(kp for kp, _ in filtered_keyphrases)\n",
    "\n",
    "                # Find candidates for recovery (not already recovered)\n",
    "                candidates = [(kp, score) for kp, score, _ in filtered_out\n",
    "                             if kp not in already_recovered and score > 0.15]\n",
    "\n",
    "                # Sort by score\n",
    "                candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "                # Recover additional keyphrases\n",
    "                for i in range(min(additional_recovery, len(candidates))):\n",
    "                    keyphrase, score = candidates[i]\n",
    "                    filtered_keyphrases.append((keyphrase, score))\n",
    "                    print(f\"Additional domain-specific recovery: {keyphrase} ({score:.2f})\")\n",
    "\n",
    "        # Re-sort by score\n",
    "        filtered_keyphrases.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Debug output\n",
    "        if filtered_out:\n",
    "            print(f\"\\nGeneric term filtering: {original_count} -> {len(filtered_keyphrases)} keyphrases ({len(filtered_out)} removed)\")\n",
    "            print(\"\\nFiltered out generic terms (examples):\")\n",
    "            for term, score, reason in filtered_out[:5]:\n",
    "                print(f\"- {term} ({score:.2f}): {reason}\")\n",
    "            if len(filtered_out) > 5:\n",
    "                print(f\"...and {len(filtered_out) - 5} more\")\n",
    "\n",
    "        return filtered_keyphrases\n",
    "\n",
    "    def initialize_generic_terms(self):\n",
    "        \"\"\"\n",
    "        Initialize lists of generic terms to filter out by domain using the Pareto principle.\n",
    "        Focus on the 20% of generic terms that appear in 80% of content across each domain.\n",
    "        Improved with more refined lists and focus domain specialization.\n",
    "        \"\"\"\n",
    "        self.generic_terms = {\n",
    "            \"general\": [\n",
    "                # Core reporting terms (extremely common across all news)\n",
    "                \"news\", \"report\", \"article\", \"story\", \"update\", \"information\",\n",
    "                \"according to\", \"said\", \"says\", \"stated\", \"reported\", \"announced\",\n",
    "\n",
    "                # Common temporal references\n",
    "                \"today\", \"yesterday\", \"recent\", \"current\", \"latest\",\n",
    "\n",
    "                # Vague importance indicators\n",
    "                \"important\", \"significant\", \"major\", \"key\", \"critical\",\n",
    "\n",
    "                # Common descriptive fillers\n",
    "                \"various\", \"several\", \"many\", \"some\", \"different\",\n",
    "\n",
    "                # Common structural words\n",
    "                \"example\", \"case\", \"issue\", \"situation\", \"development\",\n",
    "\n",
    "                # Attribution terms\n",
    "                \"source\", \"expert\", \"official\", \"spokesperson\", \"authority\"\n",
    "            ],\n",
    "\n",
    "            \"technology\": [\n",
    "                # Core tech industry terms (too generic to be useful keyphrases)\n",
    "                \"technology\", \"tech\", \"digital\", \"solution\", \"platform\", \"system\",\n",
    "                \"software\", \"hardware\", \"device\", \"application\", \"app\",\n",
    "\n",
    "                # Common tech descriptors\n",
    "                \"new\", \"innovative\", \"advanced\", \"smart\", \"intelligent\", \"modern\",\n",
    "\n",
    "                # User/market terms\n",
    "                \"user\", \"customer\", \"experience\", \"market\", \"industry\",\n",
    "\n",
    "                # Performance terms\n",
    "                \"performance\", \"efficient\", \"fast\", \"powerful\", \"high-performance\"\n",
    "            ],\n",
    "\n",
    "            # IMPROVEMENT: Add specific domain for artificial intelligence\n",
    "            \"artificial intelligence\": [\n",
    "                # Core AI terms (too generic to be useful keyphrases)\n",
    "                \"ai\", \"artificial intelligence\", \"model\", \"algorithm\", \"system\",\n",
    "                \"machine learning\", \"deep learning\", \"neural\", \"network\",\n",
    "\n",
    "                # Common AI descriptors\n",
    "                \"intelligent\", \"automated\", \"smart\", \"predictive\", \"cognitive\",\n",
    "\n",
    "                # Data terms\n",
    "                \"data\", \"dataset\", \"training\", \"learning\", \"inference\",\n",
    "\n",
    "                # Performance terms\n",
    "                \"performance\", \"accuracy\", \"precision\", \"recall\", \"efficiency\"\n",
    "            ],\n",
    "\n",
    "            # IMPROVEMENT: Add specific domain for cybersecurity\n",
    "            \"cybersecurity\": [\n",
    "                # Core security terms (too generic to be useful keyphrases)\n",
    "                \"security\", \"cybersecurity\", \"cyber\", \"protection\", \"defense\",\n",
    "                \"threat\", \"attack\", \"vulnerability\", \"risk\", \"breach\",\n",
    "\n",
    "                # Common security descriptors\n",
    "                \"secure\", \"protected\", \"encrypted\", \"safe\", \"vulnerable\",\n",
    "\n",
    "                # Security actions\n",
    "                \"detect\", \"prevent\", \"mitigate\", \"respond\", \"protect\",\n",
    "\n",
    "                # Security entities\n",
    "                \"hacker\", \"attacker\", \"defender\", \"security team\", \"analyst\"\n",
    "            ],\n",
    "\n",
    "            # IMPROVEMENT: Add specific domain for automotive\n",
    "            \"automotive\": [\n",
    "                # Core automotive terms (too generic to be useful keyphrases)\n",
    "                \"car\", \"vehicle\", \"automotive\", \"auto\", \"automobile\",\n",
    "                \"driver\", \"driving\", \"road\", \"highway\", \"traffic\",\n",
    "\n",
    "                # Vehicle components (generic)\n",
    "                \"engine\", \"motor\", \"wheel\", \"tire\", \"battery\",\n",
    "\n",
    "                # Performance terms\n",
    "                \"performance\", \"speed\", \"efficiency\", \"power\", \"fuel economy\",\n",
    "\n",
    "                # Industry terms\n",
    "                \"manufacturer\", \"industry\", \"market\", \"production\", \"sales\"\n",
    "            ],\n",
    "\n",
    "            # IMPROVEMENT: Add specific domain for food\n",
    "            \"food\": [\n",
    "                # Core food terms (too generic to be useful keyphrases)\n",
    "                \"food\", \"meal\", \"dish\", \"recipe\", \"ingredient\",\n",
    "                \"cooking\", \"cuisine\", \"chef\", \"restaurant\", \"dining\",\n",
    "\n",
    "                # Food descriptors\n",
    "                \"delicious\", \"tasty\", \"flavorful\", \"fresh\", \"healthy\",\n",
    "\n",
    "                # Cooking terms\n",
    "                \"cook\", \"bake\", \"grill\", \"roast\", \"fry\",\n",
    "\n",
    "                # Food categories\n",
    "                \"dessert\", \"appetizer\", \"entree\", \"beverage\", \"snack\"\n",
    "            ],\n",
    "\n",
    "            # IMPROVEMENT: Add specific domain for environment\n",
    "            \"environment\": [\n",
    "                # Core environmental terms (too generic to be useful keyphrases)\n",
    "                \"environment\", \"environmental\", \"climate\", \"ecosystem\", \"sustainable\",\n",
    "                \"green\", \"eco-friendly\", \"conservation\", \"preservation\", \"protection\",\n",
    "\n",
    "                # Climate terms\n",
    "                \"climate change\", \"global warming\", \"carbon\", \"emission\", \"greenhouse\",\n",
    "\n",
    "                # Resource terms\n",
    "                \"energy\", \"renewable\", \"resource\", \"natural resource\", \"biodiversity\",\n",
    "\n",
    "                # Natural elements\n",
    "                \"water\", \"air\", \"forest\", \"ocean\", \"wildlife\",\n",
    "\n",
    "                # Human impact terms\n",
    "                \"pollution\", \"waste\", \"impact\", \"footprint\", \"sustainability\"\n",
    "            ],\n",
    "\n",
    "            # IMPROVEMENT: Add specific domain for real estate\n",
    "            \"real estate\": [\n",
    "                # Core real estate terms (too generic to be useful keyphrases)\n",
    "                \"real estate\", \"property\", \"home\", \"house\", \"apartment\",\n",
    "                \"building\", \"residential\", \"commercial\", \"housing\", \"market\",\n",
    "\n",
    "                # Market terms\n",
    "                \"buyer\", \"seller\", \"agent\", \"broker\", \"listing\",\n",
    "\n",
    "                # Financial terms\n",
    "                \"mortgage\", \"loan\", \"interest rate\", \"down payment\", \"closing cost\",\n",
    "\n",
    "                # Property descriptors\n",
    "                \"spacious\", \"renovated\", \"modern\", \"updated\", \"luxury\"\n",
    "            ],\n",
    "\n",
    "            # IMPROVEMENT: Refined entertainment domain\n",
    "            \"entertainment\": [\n",
    "                # Media types (generic)\n",
    "                \"movie\", \"film\", \"show\", \"series\", \"program\",\n",
    "                \"television\", \"tv\", \"streaming\", \"broadcast\", \"media\",\n",
    "\n",
    "                # Creative roles (generic)\n",
    "                \"actor\", \"actress\", \"director\", \"producer\", \"star\",\n",
    "                \"cast\", \"crew\", \"filmmaker\", \"celebrity\", \"performer\",\n",
    "\n",
    "                # Industry terms (generic)\n",
    "                \"entertainment\", \"Hollywood\", \"studio\", \"network\", \"production\",\n",
    "                \"industry\", \"box office\", \"audience\", \"viewer\", \"fan\",\n",
    "\n",
    "                # Performance terms (generic)\n",
    "                \"performance\", \"role\", \"character\", \"scene\", \"appearance\",\n",
    "\n",
    "                # Reception terms (generic)\n",
    "                \"hit\", \"popular\", \"success\", \"award\", \"critically acclaimed\",\n",
    "                \"blockbuster\", \"bestseller\", \"rating\", \"review\", \"critic\"\n",
    "            ],\n",
    "\n",
    "            \"business\": [\n",
    "                # Core business entities (too generic)\n",
    "                \"business\", \"company\", \"corporation\", \"firm\", \"enterprise\",\n",
    "\n",
    "                # Market terms\n",
    "                \"market\", \"industry\", \"sector\", \"economy\", \"economic\",\n",
    "\n",
    "                # Financial terms\n",
    "                \"financial\", \"profit\", \"revenue\", \"growth\", \"investment\",\n",
    "\n",
    "                # Business activities\n",
    "                \"strategy\", \"plan\", \"approach\", \"decision\", \"management\",\n",
    "\n",
    "                # Business outcomes\n",
    "                \"success\", \"result\", \"performance\", \"achievement\", \"development\"\n",
    "            ],\n",
    "\n",
    "            \"health\": [\n",
    "                # Core health terms\n",
    "                \"health\", \"healthcare\", \"medical\", \"medicine\", \"treatment\",\n",
    "\n",
    "                # Healthcare entities\n",
    "                \"patient\", \"doctor\", \"hospital\", \"clinic\", \"physician\",\n",
    "\n",
    "                # Research terms\n",
    "                \"study\", \"research\", \"trial\", \"finding\", \"result\",\n",
    "\n",
    "                # Condition descriptors\n",
    "                \"condition\", \"symptom\", \"effect\", \"benefit\", \"risk\",\n",
    "\n",
    "                # Care terms\n",
    "                \"care\", \"therapy\", \"procedure\", \"approach\", \"method\"\n",
    "            ],\n",
    "\n",
    "            \"politics\": [\n",
    "                # Government entities\n",
    "                \"government\", \"administration\", \"official\", \"authority\", \"agency\",\n",
    "\n",
    "                # Policy terms\n",
    "                \"policy\", \"legislation\", \"regulation\", \"law\", \"rule\",\n",
    "\n",
    "                # Political actions\n",
    "                \"decision\", \"action\", \"measure\", \"initiative\", \"program\",\n",
    "\n",
    "                # Political descriptors\n",
    "                \"political\", \"public\", \"national\", \"federal\", \"state\",\n",
    "\n",
    "                # International terms\n",
    "                \"international\", \"global\", \"foreign\", \"diplomatic\", \"relations\"\n",
    "            ],\n",
    "\n",
    "            \"sports\": [\n",
    "                # Core sports terms\n",
    "                \"sports\", \"game\", \"match\", \"competition\", \"tournament\",\n",
    "\n",
    "                # Participants\n",
    "                \"team\", \"player\", \"athlete\", \"coach\", \"manager\",\n",
    "\n",
    "                # Performance terms\n",
    "                \"performance\", \"play\", \"win\", \"loss\", \"victory\",\n",
    "\n",
    "                # Metrics\n",
    "                \"score\", \"point\", \"goal\", \"record\", \"statistic\",\n",
    "\n",
    "                # Time periods\n",
    "                \"season\", \"championship\", \"league\", \"series\", \"round\"\n",
    "            ],\n",
    "\n",
    "            \"science\": [\n",
    "                # Research terms\n",
    "                \"research\", \"study\", \"experiment\", \"analysis\", \"investigation\",\n",
    "\n",
    "                # Scientific roles\n",
    "                \"scientist\", \"researcher\", \"professor\", \"expert\", \"specialist\",\n",
    "\n",
    "                # Discovery terms\n",
    "                \"discovery\", \"finding\", \"result\", \"evidence\", \"data\",\n",
    "\n",
    "                # Field descriptors\n",
    "                \"scientific\", \"academic\", \"theoretical\", \"experimental\", \"empirical\",\n",
    "\n",
    "                # Institutional terms\n",
    "                \"university\", \"laboratory\", \"institute\", \"department\", \"center\"\n",
    "            ],\n",
    "\n",
    "            \"world\": [\n",
    "                # International entities\n",
    "                \"country\", \"nation\", \"government\", \"state\", \"region\",\n",
    "\n",
    "                # Global organizations\n",
    "                \"United Nations\", \"international\", \"global\", \"worldwide\", \"organization\",\n",
    "\n",
    "                # Diplomatic terms\n",
    "                \"relations\", \"agreement\", \"treaty\", \"cooperation\", \"alliance\",\n",
    "\n",
    "                # Conflict terms\n",
    "                \"conflict\", \"crisis\", \"tension\", \"dispute\", \"war\",\n",
    "\n",
    "                # Geographic terms\n",
    "                \"border\", \"territory\", \"area\", \"zone\", \"continent\"\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        # IMPROVEMENT: More refined cross-domain generic terms\n",
    "        cross_domain_generics = [\n",
    "            # Common verbs that don't add specificity\n",
    "            \"make\", \"made\", \"making\", \"do\", \"does\", \"doing\", \"done\",\n",
    "            \"get\", \"gets\", \"getting\", \"got\", \"take\", \"takes\", \"taking\",\n",
    "            \"give\", \"gives\", \"giving\", \"gave\", \"put\", \"puts\", \"putting\",\n",
    "            \"use\", \"uses\", \"using\", \"used\", \"provide\", \"provides\", \"providing\",\n",
    "            \"create\", \"creates\", \"creating\", \"created\", \"develop\", \"develops\", \"developing\",\n",
    "\n",
    "            # Common adjectives that don't add specificity\n",
    "            \"good\", \"great\", \"best\", \"better\", \"worse\", \"worst\",\n",
    "            \"big\", \"small\", \"large\", \"little\", \"high\", \"low\",\n",
    "            \"new\", \"old\", \"recent\", \"latest\", \"current\", \"modern\",\n",
    "            \"common\", \"typical\", \"standard\", \"normal\", \"regular\",\n",
    "            \"simple\", \"complex\", \"easy\", \"difficult\", \"hard\",\n",
    "\n",
    "            # Time references\n",
    "            \"time\", \"day\", \"week\", \"month\", \"year\", \"hour\", \"minute\",\n",
    "            \"period\", \"duration\", \"interval\", \"moment\", \"instant\",\n",
    "\n",
    "            # Vague quantities\n",
    "            \"many\", \"much\", \"more\", \"most\", \"some\", \"few\", \"several\",\n",
    "            \"numerous\", \"multiple\", \"various\", \"diverse\", \"different\",\n",
    "\n",
    "            # Common phrases that don't add specificity\n",
    "            \"in fact\", \"as well\", \"in addition\", \"moreover\", \"furthermore\",\n",
    "            \"for example\", \"such as\", \"like\", \"including\", \"especially\"\n",
    "        ]\n",
    "\n",
    "        # IMPROVEMENT: Add domain-specific exceptions to avoid over-filtering\n",
    "        # These are terms that are generic but should NOT be filtered in specific domains\n",
    "        domain_exceptions = {\n",
    "            \"artificial intelligence\": [\"model\", \"algorithm\", \"data\", \"learning\", \"neural\", \"network\"],\n",
    "            \"cybersecurity\": [\"security\", \"threat\", \"attack\", \"vulnerability\", \"breach\", \"protection\"],\n",
    "            \"automotive\": [\"vehicle\", \"car\", \"engine\", \"driver\", \"fuel\", \"electric\"],\n",
    "            \"food\": [\"recipe\", \"ingredient\", \"dish\", \"meal\", \"cooking\", \"flavor\"],\n",
    "            \"environment\": [\"climate\", \"sustainable\", \"energy\", \"carbon\", \"emission\", \"pollution\"],\n",
    "            \"real estate\": [\"property\", \"home\", \"house\", \"market\", \"mortgage\", \"buyer\"],\n",
    "            \"entertainment\": [\"movie\", \"film\", \"actor\", \"director\", \"show\", \"series\"]\n",
    "        }\n",
    "\n",
    "        # Store domain exceptions for use in filtering\n",
    "        self.domain_exceptions = domain_exceptions\n",
    "\n",
    "        # Add cross-domain generics to all domains\n",
    "        for domain in self.generic_terms:\n",
    "            self.generic_terms[domain].extend(cross_domain_generics)\n",
    "            # Remove duplicates while preserving order\n",
    "            self.generic_terms[domain] = list(dict.fromkeys(self.generic_terms[domain]))\n",
    "\n",
    "        print(f\"Initialized generic terms for {len(self.generic_terms)} domains using Pareto principle\")\n",
    "\n",
    "    def get_best_prompt(self, text: str) -> int:\n",
    "        \"\"\"\n",
    "        Find the best prompt template for a given text.\n",
    "\n",
    "        Args:\n",
    "            text: Input text\n",
    "\n",
    "        Returns:\n",
    "            Index of the best prompt template\n",
    "        \"\"\"\n",
    "        # Try all prompts\n",
    "        results = self.try_all_prompts(text)\n",
    "\n",
    "        # Score each prompt based on quality metrics\n",
    "        scores = {}\n",
    "        for idx, keyphrases in results.items():\n",
    "            # Skip if no keyphrases were generated\n",
    "            if not keyphrases:\n",
    "                scores[idx] = 0\n",
    "                continue\n",
    "\n",
    "            # Count keyphrases\n",
    "            num_keyphrases = len(keyphrases)\n",
    "\n",
    "            # Count multi-word keyphrases\n",
    "            multi_word_count = sum(1 for kp in keyphrases if len(kp.split()) > 1)\n",
    "\n",
    "            # Check for n-gram patterns (consecutive words from original text)\n",
    "            # This is a sign of poor extraction - we want to penalize this\n",
    "            ngram_penalty = 0\n",
    "            for i in range(len(keyphrases) - 1):\n",
    "                kp1 = keyphrases[i].split()\n",
    "                kp2 = keyphrases[i+1].split()\n",
    "\n",
    "                # Check if the last words of kp1 match the first words of kp2\n",
    "                if len(kp1) > 0 and len(kp2) > 0:\n",
    "                    if kp1[-1] == kp2[0]:\n",
    "                        ngram_penalty += 0.2  # Penalize for each detected n-gram pattern\n",
    "\n",
    "            # Calculate average keyphrase length\n",
    "            avg_length = sum(len(kp.split()) for kp in keyphrases) / num_keyphrases\n",
    "\n",
    "            # Prefer 2-3 word keyphrases\n",
    "            length_score = 1.0 - abs(avg_length - 2.5) / 2.5\n",
    "\n",
    "            # Calculate multi-word percentage\n",
    "            multi_word_percentage = multi_word_count / num_keyphrases if num_keyphrases > 0 else 0\n",
    "\n",
    "            # Calculate final score\n",
    "            # We want:\n",
    "            # - A reasonable number of keyphrases (5-15 is good)\n",
    "            # - High percentage of multi-word phrases\n",
    "            # - Average length around 2-3 words\n",
    "            # - Low n-gram penalty\n",
    "\n",
    "            num_score = min(num_keyphrases / 10, 1.5)  # Cap at 15 keyphrases\n",
    "\n",
    "            # Penalize too few or too many keyphrases\n",
    "            if num_keyphrases < 3:\n",
    "                num_score *= 0.5\n",
    "            elif num_keyphrases > 20:\n",
    "                num_score *= 0.7\n",
    "\n",
    "            final_score = (\n",
    "                (num_score * 0.3) +\n",
    "                (multi_word_percentage * 0.4) +\n",
    "                (length_score * 0.3) -\n",
    "                ngram_penalty\n",
    "            )\n",
    "\n",
    "            scores[idx] = final_score\n",
    "\n",
    "        # Return the prompt with the highest score\n",
    "        if not scores:\n",
    "            return 0  # Default to first template if no scores\n",
    "\n",
    "        return max(scores.items(), key=lambda x: x[1])[0]\n",
    "\n",
    "\n",
    "    def filter_and_select_by_quality(self, text: str, domain: str, scored_keyphrases: List[Tuple[str, float]], original_domain: str = None) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Filters keyphrases based on quality using a simpler percentile-based approach.\n",
    "        The number of returned keyphrases is dynamic based on these filters.\n",
    "        Optimized to produce more keyphrases while maintaining quality.\n",
    "\n",
    "        Args:\n",
    "            text: Input text\n",
    "            domain: Domain of the text (original domain if provided, otherwise detected domain)\n",
    "            scored_keyphrases: List of (keyphrase, score) tuples\n",
    "\n",
    "        Returns:\n",
    "            List of (keyphrase, score) tuples filtered by quality\n",
    "        \"\"\"\n",
    "        if not scored_keyphrases:\n",
    "            return []\n",
    "\n",
    "        # Calculate text statistics\n",
    "        words = text.split()\n",
    "        word_count = len(words)\n",
    "        unique_words = len(set(w.lower() for w in words))\n",
    "\n",
    "        # Calculate content density (ratio of unique words to total words)\n",
    "        content_density = unique_words / max(word_count, 1)\n",
    "\n",
    "        # Store content density for use in other methods\n",
    "        self.content_density = content_density\n",
    "\n",
    "        # Use original_domain if provided, otherwise use detected domain\n",
    "        domain_to_use = original_domain if original_domain else domain\n",
    "\n",
    "        # Get domain-specific parameters using the get_domain_specific_parameters method\n",
    "        base_threshold, quality_threshold, percentile_from_domain, target_count = self.get_domain_specific_parameters(domain_to_use, word_count)\n",
    "\n",
    "        # Print the domain we're using\n",
    "        print(f\"Using domain '{domain_to_use}' with base threshold: {base_threshold:.3f}, quality threshold: {quality_threshold:.3f}\")\n",
    "\n",
    "        # Store quality threshold for potential use in single-word filtering\n",
    "        self.quality_threshold = quality_threshold\n",
    "\n",
    "        # --- 2. Calculate Percentile-Based Threshold ---\n",
    "        # Get all scores\n",
    "        scores = [score for _, score in scored_keyphrases]\n",
    "\n",
    "        # Get percentile setting for this domain - use much lower percentiles\n",
    "        # Replace the existing percentiles with much lower values\n",
    "        domain_percentiles = {\n",
    "            \"artificial intelligence\": 5,  # Reduced from 10\n",
    "            \"technology\": 5,              # Reduced from 10\n",
    "            \"cybersecurity\": 5,           # Reduced from 10\n",
    "            \"automotive\": 3,              # Reduced from 8\n",
    "            \"food\": 3,                    # Reduced from 8\n",
    "            \"environment\": 3,             # Reduced from 8\n",
    "            \"real estate\": 3,             # Reduced from 8\n",
    "            \"entertainment\": 2,           # Reduced from 5\n",
    "            \"science\": 5,                 # Reduced from 10\n",
    "            \"health\": 3,                  # Reduced from 8\n",
    "            \"business\": 3,                # Reduced from 8\n",
    "            \"politics\": 3,                # Reduced from 8\n",
    "            \"sports\": 2,                  # Reduced from 5\n",
    "            \"world\": 2,                   # Reduced from 5\n",
    "            \"default\": 3\n",
    "        }\n",
    "\n",
    "        # Domain-specific percentiles optimized for 8-12 keyphrases\n",
    "\n",
    "        # IMPORTANT: Use the domain_to_use for percentile lookup\n",
    "        # This ensures we use the correct percentile for the domain\n",
    "        percentile = domain_percentiles.get(domain_to_use, domain_percentiles[\"default\"])\n",
    "\n",
    "        # Use the percentile from domain-specific parameters if available\n",
    "        percentile = percentile_from_domain if percentile_from_domain else percentile\n",
    "\n",
    "        # Store the domain percentiles for future use\n",
    "        if not hasattr(self, '_quality_percentiles'):\n",
    "            self._quality_percentiles = {}\n",
    "\n",
    "        # Update the _quality_percentiles for this domain\n",
    "        self._quality_percentiles[domain_to_use] = percentile\n",
    "\n",
    "        print(f\"Using {percentile}th percentile for domain '{domain_to_use}'\")\n",
    "\n",
    "        # Adjust percentile based on content density - be even more lenient\n",
    "        if content_density > 0.7:  # Very diverse content\n",
    "            percentile = max(5, percentile - 5)  # More lenient (reduced from 10 to 5)\n",
    "        elif content_density < 0.5:  # Less diverse content\n",
    "            percentile = min(20, percentile + 5)  # More strict (reduced from 30 to 20)\n",
    "\n",
    "        # Calculate percentile threshold if we have enough keyphrases\n",
    "        if len(scores) >= 3:\n",
    "            import numpy as np\n",
    "            percentile_threshold = np.percentile(scores, 100 - percentile)  # e.g., 95th percentile from top\n",
    "\n",
    "            # Print percentile info for debugging\n",
    "            print(f\"- Using {percentile}th percentile from top: {percentile_threshold:.3f}\")\n",
    "        else:\n",
    "            # For very few keyphrases, use base threshold\n",
    "            percentile_threshold = 0\n",
    "            print(f\"- Too few keyphrases, using base threshold only\")\n",
    "\n",
    "        # Combine base and percentile thresholds\n",
    "        quality_threshold = max(base_threshold, percentile_threshold)\n",
    "\n",
    "        # Cap the threshold to avoid being too strict - reduced significantly\n",
    "        quality_threshold = min(quality_threshold, 0.35)  # Reduced from 0.45 to 0.35\n",
    "\n",
    "        print(f\"\\nQuality Filtering Stats:\")\n",
    "        print(f\"- Domain: {domain}\")\n",
    "        print(f\"- Text Length: {word_count} words\")\n",
    "        print(f\"- Content Density: {content_density:.2f}\")\n",
    "        print(f\"- Initial Candidates: {len(scored_keyphrases)}\")\n",
    "        print(f\"- Base Threshold: {base_threshold:.3f}\")\n",
    "        print(f\"- Quality Threshold: {quality_threshold:.3f}\")\n",
    "\n",
    "        # --- 3. Apply Quality Filter ---\n",
    "        quality_kps = [(kp, score) for kp, score in scored_keyphrases if score >= quality_threshold]\n",
    "        print(f\"- After Quality Filter: {len(quality_kps)}\")\n",
    "\n",
    "        # --- 4. Fallback if filter removes too many keyphrases ---\n",
    "        # Domain-specific target keyphrase counts for 400-500 word articles\n",
    "        domain_target_counts = {\n",
    "            \"artificial intelligence\": 10,  # Target for AI domain\n",
    "            \"technology\": 10,              # Target for technology\n",
    "            \"cybersecurity\": 10,           # Target for cybersecurity\n",
    "            \"automotive\": 9,               # Target for automotive\n",
    "            \"food\": 9,                     # Target for food\n",
    "            \"environment\": 9,              # Target for environment\n",
    "            \"real estate\": 9,              # Target for real estate\n",
    "            \"entertainment\": 8,            # Target for entertainment\n",
    "            \"default\": 9                   # Default target count\n",
    "        }\n",
    "\n",
    "        # Get base target count for this domain using domain_to_use\n",
    "        base_target = domain_target_counts.get(domain_to_use, domain_target_counts[\"default\"])\n",
    "\n",
    "        # Use the target_count from domain-specific parameters if available\n",
    "        base_target = target_count if target_count else base_target\n",
    "\n",
    "        print(f\"Using target count {base_target} for domain '{domain_to_use}'\")\n",
    "\n",
    "        # Adjust target based on text length\n",
    "        if word_count < 300:  # Shorter text\n",
    "            length_adjustment = -1  # Fewer keyphrases\n",
    "        elif word_count > 600:  # Longer text\n",
    "            length_adjustment = 1   # More keyphrases\n",
    "        else:\n",
    "            length_adjustment = 0   # Standard count for 400-500 words\n",
    "\n",
    "        # Calculate final target (ensure between 8-12 range)\n",
    "        target_keyphrases = min(12, max(8, base_target + length_adjustment))\n",
    "\n",
    "        # Ensure minimum number of keyphrases\n",
    "        min_keyphrases = 8  # Always ensure at least 8 keyphrases\n",
    "\n",
    "        if len(quality_kps) < target_keyphrases and len(scored_keyphrases) >= target_keyphrases:\n",
    "            print(f\"- Quality filter too strict, falling back to top {target_keyphrases} keyphrases\")\n",
    "\n",
    "            # Try a much more lenient threshold (50% of original)\n",
    "            lenient_threshold = quality_threshold * 0.5  # Reduced from 0.6 to 0.5\n",
    "            lenient_kps = [(kp, score) for kp, score in scored_keyphrases if score >= lenient_threshold]\n",
    "\n",
    "            if len(lenient_kps) >= target_keyphrases:\n",
    "                quality_kps = lenient_kps\n",
    "                print(f\"- Using lenient threshold {lenient_threshold:.3f}: {len(quality_kps)} keyphrases\")\n",
    "            else:\n",
    "                # If still too few, just take the top N by score\n",
    "                quality_kps = sorted(scored_keyphrases, key=lambda x: x[1], reverse=True)[:target_keyphrases]\n",
    "                print(f\"- Falling back to top {target_keyphrases} keyphrases by score\")\n",
    "\n",
    "        # Absolute minimum fallback - ensure we always have at least min_keyphrases\n",
    "        if len(quality_kps) < min_keyphrases and len(scored_keyphrases) >= min_keyphrases:\n",
    "            print(f\"- Too few keyphrases ({len(quality_kps)}), ensuring minimum of {min_keyphrases}\")\n",
    "            quality_kps = sorted(scored_keyphrases, key=lambda x: x[1], reverse=True)[:min_keyphrases]\n",
    "\n",
    "        # --- 5. Calculate diversity weight based on content density ---\n",
    "        # More diverse content = higher weight for diversity\n",
    "        diversity_weight = min(0.4, max(0.2, content_density * 0.4))\n",
    "\n",
    "        # --- 6. Select diverse keyphrases if we have more than needed ---\n",
    "        # For 400-500 word articles, we want 8-12 keyphrases\n",
    "        # Hard cap at 12 keyphrases for consistency\n",
    "        max_reasonable_keyphrases = 12\n",
    "\n",
    "        if len(quality_kps) > max_reasonable_keyphrases:\n",
    "            print(f\"- Too many keyphrases ({len(quality_kps)}), selecting {max_reasonable_keyphrases} diverse ones\")\n",
    "            # Use the existing select_diverse_keyphrases method\n",
    "            quality_kps = self.select_diverse_keyphrases(\n",
    "                quality_kps,\n",
    "                max_reasonable_keyphrases,\n",
    "                diversity_weight\n",
    "            )\n",
    "\n",
    "        print(f\"- Final keyphrase count: {len(quality_kps)}\")\n",
    "\n",
    "        # Calculate and display diversity score if we have multiple keyphrases\n",
    "        if len(quality_kps) > 1:\n",
    "            diversity_score = self.calculate_semantic_diversity([kp for kp, _ in quality_kps])\n",
    "            print(f\"- Semantic diversity score: {diversity_score:.2f}\")\n",
    "\n",
    "        return quality_kps\n",
    "\n",
    "    def run_quality_filter_calibration(self, articles: List[str], num_articles: int = 5) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Run a complete quality filter calibration process:\n",
    "        1. Test different percentile thresholds\n",
    "        2. Analyze results\n",
    "        3. Apply recommended settings\n",
    "\n",
    "        Args:\n",
    "            articles: List of articles to use\n",
    "            num_articles: Number of articles to use for calibration\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with calibration results\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"QUALITY FILTER CALIBRATION PROCESS\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        # Step 1: Test different percentile thresholds\n",
    "        print(\"\\nSTEP 1: Testing Different Percentile Thresholds\")\n",
    "        calibration_results = self.tune_quality_thresholds(\n",
    "            articles[:num_articles],\n",
    "            percentiles=[10, 15, 20, 25, 30]\n",
    "        )\n",
    "\n",
    "        # Step 2: Apply recommended settings\n",
    "        print(\"\\nSTEP 2: Applying Recommended Settings\")\n",
    "\n",
    "        # Get the recommended percentiles from the calibration results\n",
    "        best_percentile = None\n",
    "        for percentile in [10, 15, 20, 25, 30]:\n",
    "            if percentile in calibration_results:\n",
    "                avg_count = calibration_results[percentile][\"avg_keyphrase_count\"]\n",
    "                avg_min_score = calibration_results[percentile][\"avg_min_score\"]\n",
    "\n",
    "                # Find the percentile that gives 7-12 keyphrases with acceptable quality\n",
    "                if 7 <= avg_count <= 12 and avg_min_score >= 0.3:\n",
    "                    best_percentile = percentile\n",
    "                    break\n",
    "\n",
    "        if best_percentile is None:\n",
    "            # If no percentile meets both criteria, prioritize count\n",
    "            for percentile in [10, 15, 20, 25, 30]:\n",
    "                if percentile in calibration_results:\n",
    "                    avg_count = calibration_results[percentile][\"avg_keyphrase_count\"]\n",
    "                    if 7 <= avg_count <= 12:\n",
    "                        best_percentile = percentile\n",
    "                        break\n",
    "\n",
    "        if best_percentile is None:\n",
    "            # If still no match, use the lowest percentile\n",
    "            best_percentile = 10\n",
    "\n",
    "        # Create recommended percentiles dictionary\n",
    "        recommended_percentiles = {\n",
    "            \"technology\": best_percentile,\n",
    "            \"science\": best_percentile,\n",
    "            \"health\": best_percentile,\n",
    "            \"business\": best_percentile,\n",
    "            \"politics\": best_percentile,\n",
    "            \"sports\": max(5, best_percentile - 5),  # More lenient for sports\n",
    "            \"entertainment\": max(5, best_percentile - 5),  # More lenient for entertainment\n",
    "            \"world\": best_percentile,\n",
    "            \"default\": best_percentile\n",
    "        }\n",
    "\n",
    "        # Apply recommended percentiles\n",
    "        print(f\"\\nApplying recommended percentiles:\")\n",
    "        for domain, percentile in recommended_percentiles.items():\n",
    "            print(f\"- {domain}: {percentile}\")\n",
    "\n",
    "        self._quality_percentiles = recommended_percentiles\n",
    "\n",
    "        # Step 3: Test with new settings\n",
    "        print(\"\\nSTEP 3: Testing with New Settings\")\n",
    "        test_articles = articles[:3]  # Use first 3 articles for testing\n",
    "\n",
    "        results = []\n",
    "        for i, article in enumerate(test_articles):\n",
    "            print(f\"\\nArticle {i+1}:\")\n",
    "            domain = self.detect_domain(article)\n",
    "            print(f\"Domain: {domain}\")\n",
    "\n",
    "            keyphrases = self.extract_keyphrases_with_scores(article)\n",
    "            print(f\"Generated {len(keyphrases)} keyphrases:\")\n",
    "\n",
    "            for kp, score in keyphrases:\n",
    "                print(f\"- {kp}: {score:.3f}\")\n",
    "\n",
    "            results.append({\n",
    "                \"domain\": domain,\n",
    "                \"count\": len(keyphrases),\n",
    "                \"keyphrases\": keyphrases\n",
    "            })\n",
    "\n",
    "        return {\n",
    "            \"calibration_results\": calibration_results,\n",
    "            \"recommended_percentiles\": recommended_percentiles,\n",
    "            \"test_results\": results\n",
    "        }\n",
    "    def tune_quality_thresholds(self, articles: List[str], percentiles: List[int] = [10, 15, 20, 25, 30]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Systematically evaluate different percentile thresholds for quality filtering.\n",
    "\n",
    "        Args:\n",
    "            articles: List of articles to test\n",
    "            percentiles: List of percentile values to test (lower values = more lenient)\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with evaluation results\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "\n",
    "        # Save original percentile settings\n",
    "        original_percentiles = self._quality_percentiles.copy()\n",
    "\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"TUNING QUALITY THRESHOLDS\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        # Test each percentile\n",
    "        for percentile in percentiles:\n",
    "            print(f\"\\nTesting {percentile}th percentile threshold\")\n",
    "\n",
    "            # Override percentile settings\n",
    "            self._quality_percentiles = {\n",
    "                \"technology\": percentile,\n",
    "                \"science\": percentile,\n",
    "                \"health\": percentile,\n",
    "                \"business\": percentile,\n",
    "                \"politics\": percentile,\n",
    "                \"sports\": percentile,\n",
    "                \"entertainment\": percentile,\n",
    "                \"world\": percentile,\n",
    "                \"default\": percentile\n",
    "            }\n",
    "\n",
    "            # Process each article\n",
    "            article_results = []\n",
    "            domain_counts = {}\n",
    "\n",
    "            for i, article in enumerate(articles):\n",
    "                print(f\"\\nArticle {i+1}/{len(articles)}\")\n",
    "\n",
    "                # Detect domain\n",
    "                domain = self.detect_domain(article)\n",
    "                domain_counts[domain] = domain_counts.get(domain, 0) + 1\n",
    "\n",
    "                # Extract keyphrases\n",
    "                keyphrases = self.extract_keyphrases_with_scores(article)\n",
    "\n",
    "                # Record results\n",
    "                min_score = min([score for _, score in keyphrases]) if keyphrases else 0\n",
    "                max_score = max([score for _, score in keyphrases]) if keyphrases else 0\n",
    "                avg_score = sum([score for _, score in keyphrases]) / len(keyphrases) if keyphrases else 0\n",
    "\n",
    "                # Calculate multi-word percentage\n",
    "                multi_word_count = sum(1 for kp, _ in keyphrases if len(kp.split()) > 1)\n",
    "                multi_word_percentage = multi_word_count / len(keyphrases) if keyphrases else 0\n",
    "\n",
    "                # Calculate average phrase length\n",
    "                avg_length = sum(len(kp.split()) for kp, _ in keyphrases) / len(keyphrases) if keyphrases else 0\n",
    "\n",
    "                article_results.append({\n",
    "                    \"domain\": domain,\n",
    "                    \"count\": len(keyphrases),\n",
    "                    \"min_score\": min_score,\n",
    "                    \"max_score\": max_score,\n",
    "                    \"avg_score\": avg_score,\n",
    "                    \"multi_word_percentage\": multi_word_percentage,\n",
    "                    \"avg_length\": avg_length,\n",
    "                    \"scores\": [score for _, score in keyphrases],\n",
    "                    \"keyphrases\": [kp for kp, _ in keyphrases]\n",
    "                })\n",
    "\n",
    "                # Print article-specific results\n",
    "                print(f\"Domain: {domain}\")\n",
    "                print(f\"Keyphrases: {len(keyphrases)}\")\n",
    "                print(f\"Min score: {min_score:.3f}, Max score: {max_score:.3f}, Avg score: {avg_score:.3f}\")\n",
    "                print(f\"Multi-word: {multi_word_percentage:.1%}, Avg length: {avg_length:.1f} words\")\n",
    "\n",
    "                # Print the lowest scoring keyphrases for inspection\n",
    "                if keyphrases:\n",
    "                    sorted_keyphrases = sorted(keyphrases, key=lambda x: x[1])\n",
    "                    print(\"\\nLowest scoring keyphrases:\")\n",
    "                    for kp, score in sorted_keyphrases[:3]:\n",
    "                        print(f\"- {kp}: {score:.3f}\")\n",
    "\n",
    "            # Calculate statistics\n",
    "            avg_count = sum(r[\"count\"] for r in article_results) / len(article_results)\n",
    "            avg_min_score = sum(r[\"min_score\"] if r[\"scores\"] else 0 for r in article_results) / len(article_results)\n",
    "            avg_max_score = sum(r[\"max_score\"] if r[\"scores\"] else 0 for r in article_results) / len(article_results)\n",
    "            avg_avg_score = sum(r[\"avg_score\"] if r[\"scores\"] else 0 for r in article_results) / len(article_results)\n",
    "            avg_multi_word = sum(r[\"multi_word_percentage\"] for r in article_results) / len(article_results)\n",
    "            avg_length = sum(r[\"avg_length\"] for r in article_results) / len(article_results)\n",
    "\n",
    "            # Calculate domain-specific statistics\n",
    "            domain_stats = {}\n",
    "            for domain in domain_counts.keys():\n",
    "                domain_articles = [r for r in article_results if r[\"domain\"] == domain]\n",
    "                if domain_articles:\n",
    "                    domain_avg_count = sum(r[\"count\"] for r in domain_articles) / len(domain_articles)\n",
    "                    domain_avg_min_score = sum(r[\"min_score\"] if r[\"scores\"] else 0 for r in domain_articles) / len(domain_articles)\n",
    "                    domain_avg_multi_word = sum(r[\"multi_word_percentage\"] for r in domain_articles) / len(domain_articles)\n",
    "                    domain_stats[domain] = {\n",
    "                        \"count\": len(domain_articles),\n",
    "                        \"avg_keyphrase_count\": domain_avg_count,\n",
    "                        \"avg_min_score\": domain_avg_min_score,\n",
    "                        \"avg_multi_word\": domain_avg_multi_word\n",
    "                    }\n",
    "\n",
    "            # Store results\n",
    "            results[percentile] = {\n",
    "                \"avg_keyphrase_count\": avg_count,\n",
    "                \"avg_min_score\": avg_min_score,\n",
    "                \"avg_max_score\": avg_max_score,\n",
    "                \"avg_avg_score\": avg_avg_score,\n",
    "                \"avg_multi_word\": avg_multi_word,\n",
    "                \"avg_length\": avg_length,\n",
    "                \"article_results\": article_results,\n",
    "                \"domain_stats\": domain_stats\n",
    "            }\n",
    "\n",
    "            # Print summary\n",
    "            print(f\"\\nPercentile {percentile} Summary:\")\n",
    "            print(f\"- Average keyphrase count: {avg_count:.1f}\")\n",
    "            print(f\"- Average minimum score: {avg_min_score:.3f}\")\n",
    "            print(f\"- Average maximum score: {avg_max_score:.3f}\")\n",
    "            print(f\"- Average mean score: {avg_avg_score:.3f}\")\n",
    "            print(f\"- Average multi-word percentage: {avg_multi_word:.1%}\")\n",
    "            print(f\"- Average phrase length: {avg_length:.1f} words\")\n",
    "\n",
    "            # Print domain-specific results\n",
    "            print(\"\\nDomain-specific results:\")\n",
    "            for domain, stats in domain_stats.items():\n",
    "                print(f\"- {domain} ({stats['count']} articles): {stats['avg_keyphrase_count']:.1f} keyphrases, \" +\n",
    "                    f\"min score {stats['avg_min_score']:.3f}, multi-word {stats['avg_multi_word']:.1%}\")\n",
    "\n",
    "        # Restore original settings\n",
    "        self._quality_percentiles = original_percentiles\n",
    "\n",
    "        # Print comparison table\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"PERCENTILE COMPARISON\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"{'Percentile':<10} {'Avg Count':<10} {'Min Score':<10} {'Avg Score':<10} {'Multi-word':<10} {'Avg Length':<10}\")\n",
    "        print(\"-\"*60)\n",
    "\n",
    "        for percentile in percentiles:\n",
    "            avg_count = results[percentile][\"avg_keyphrase_count\"]\n",
    "            avg_min_score = results[percentile][\"avg_min_score\"]\n",
    "            avg_avg_score = results[percentile][\"avg_avg_score\"]\n",
    "            avg_multi_word = results[percentile][\"avg_multi_word\"]\n",
    "            avg_length = results[percentile][\"avg_length\"]\n",
    "            print(f\"{percentile:<10} {avg_count:<10.1f} {avg_min_score:<10.3f} {avg_avg_score:<10.3f} {avg_multi_word:<10.1%} {avg_length:<10.1f}\")\n",
    "\n",
    "        # Find optimal percentile based on desired keyphrase count\n",
    "        target_count = 9  # Target average number of keyphrases (increased from 7 to 9)\n",
    "        best_percentile = None\n",
    "        best_diff = float('inf')\n",
    "\n",
    "        for percentile in percentiles:\n",
    "            avg_count = results[percentile][\"avg_keyphrase_count\"]\n",
    "            diff = abs(avg_count - target_count)\n",
    "            if diff < best_diff:\n",
    "                best_diff = diff\n",
    "                best_percentile = percentile\n",
    "\n",
    "        if best_percentile is not None:\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(f\"RECOMMENDED PERCENTILE: {best_percentile}\")\n",
    "            print(\"=\"*80)\n",
    "            print(f\"This percentile produces an average of {results[best_percentile]['avg_keyphrase_count']:.1f} keyphrases\")\n",
    "            print(f\"with an average minimum score of {results[best_percentile]['avg_min_score']:.3f}\")\n",
    "            print(f\"and average multi-word percentage of {results[best_percentile]['avg_multi_word']:.1%}\")\n",
    "\n",
    "            # Quality check - is the minimum score acceptable?\n",
    "            min_score_acceptable = results[best_percentile]['avg_min_score'] >= 0.3\n",
    "            print(f\"\\nQuality check: Average minimum score is {results[best_percentile]['avg_min_score']:.3f}\")\n",
    "            if min_score_acceptable:\n",
    "                print(\"✓ This is above the acceptable threshold of 0.3\")\n",
    "            else:\n",
    "                print(\"⚠ This is below the acceptable threshold of 0.3\")\n",
    "\n",
    "                # Find next best percentile with acceptable min score\n",
    "                for p in sorted(percentiles):\n",
    "                    if p > best_percentile and results[p]['avg_min_score'] >= 0.3:\n",
    "                        print(f\"Consider using {p} instead for better quality (avg count: {results[p]['avg_keyphrase_count']:.1f})\")\n",
    "                        break\n",
    "\n",
    "            # Recommend domain-specific percentiles\n",
    "            print(\"\\nRecommended domain-specific percentiles:\")\n",
    "            domain_percentiles = {}\n",
    "\n",
    "            for domain in domain_counts.keys():\n",
    "                domain_best_percentile = None\n",
    "                domain_best_diff = float('inf')\n",
    "\n",
    "                for percentile in percentiles:\n",
    "                    if domain in results[percentile][\"domain_stats\"]:\n",
    "                        domain_avg_count = results[percentile][\"domain_stats\"][domain][\"avg_keyphrase_count\"]\n",
    "                        domain_avg_min_score = results[percentile][\"domain_stats\"][domain][\"avg_min_score\"]\n",
    "\n",
    "                        # Only consider percentiles that maintain acceptable quality\n",
    "                        if domain_avg_min_score >= 0.3:\n",
    "                            diff = abs(domain_avg_count - target_count)\n",
    "                            if diff < domain_best_diff:\n",
    "                                domain_best_diff = diff\n",
    "                                domain_best_percentile = percentile\n",
    "\n",
    "                # If no percentile meets quality criteria, use the one closest to target count\n",
    "                if domain_best_percentile is None:\n",
    "                    for percentile in percentiles:\n",
    "                        if domain in results[percentile][\"domain_stats\"]:\n",
    "                            domain_avg_count = results[percentile][\"domain_stats\"][domain][\"avg_keyphrase_count\"]\n",
    "                            diff = abs(domain_avg_count - target_count)\n",
    "                            if diff < domain_best_diff:\n",
    "                                domain_best_diff = diff\n",
    "                                domain_best_percentile = percentile\n",
    "\n",
    "                if domain_best_percentile is not None:\n",
    "                    domain_percentiles[domain] = domain_best_percentile\n",
    "                    domain_avg_count = results[domain_best_percentile][\"domain_stats\"][domain][\"avg_keyphrase_count\"]\n",
    "                    domain_avg_min_score = results[domain_best_percentile][\"domain_stats\"][domain][\"avg_min_score\"]\n",
    "                    print(f\"- {domain}: {domain_best_percentile} \" +\n",
    "                        f\"({domain_avg_count:.1f} keyphrases, min score: {domain_avg_min_score:.3f})\")\n",
    "\n",
    "            # Create recommended _quality_percentiles dictionary\n",
    "            recommended_percentiles = {\n",
    "                \"default\": best_percentile\n",
    "            }\n",
    "            recommended_percentiles.update(domain_percentiles)\n",
    "\n",
    "            print(\"\\nRecommended _quality_percentiles setting:\")\n",
    "            print(\"self._quality_percentiles = {\")\n",
    "            for domain, percentile in recommended_percentiles.items():\n",
    "                print(f\"    \\\"{domain}\\\": {percentile},\")\n",
    "            print(\"}\")\n",
    "\n",
    "            # Sample keyphrases from articles with the recommended percentile\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(\"SAMPLE KEYPHRASES WITH RECOMMENDED PERCENTILE\")\n",
    "            print(\"=\"*80)\n",
    "\n",
    "            # Save current percentiles\n",
    "            current_percentiles = self._quality_percentiles.copy()\n",
    "\n",
    "            # Apply recommended percentiles\n",
    "            self._quality_percentiles = recommended_percentiles\n",
    "\n",
    "            # Sample 3 articles\n",
    "            sample_articles = articles[:3]\n",
    "            for i, article in enumerate(sample_articles):\n",
    "                print(f\"\\nArticle {i+1}:\")\n",
    "                domain = self.detect_domain(article)\n",
    "                print(f\"Domain: {domain}\")\n",
    "\n",
    "                keyphrases = self.extract_keyphrases_with_scores(article)\n",
    "                print(f\"Generated {len(keyphrases)} keyphrases:\")\n",
    "\n",
    "                for kp, score in keyphrases:\n",
    "                    print(f\"- {kp}: {score:.3f}\")\n",
    "\n",
    "            # Restore original percentiles\n",
    "            self._quality_percentiles = current_percentiles\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "    def calibrate_domain_detection(self, articles: List[str], confidence_thresholds: List[float] = [0.25, 0.30, 0.35, 0.40, 0.45]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Calibrate the confidence threshold for zero-shot domain detection.\n",
    "\n",
    "        Args:\n",
    "            articles: List of articles to test\n",
    "            confidence_thresholds: List of confidence threshold values to test\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with evaluation results\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"CALIBRATING DOMAIN DETECTION CONFIDENCE THRESHOLD\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        # Test each confidence threshold\n",
    "        for threshold in confidence_thresholds:\n",
    "            print(f\"\\nTesting confidence threshold {threshold:.2f}\")\n",
    "\n",
    "            # Process each article\n",
    "            article_results = []\n",
    "            zsl_success_count = 0\n",
    "            fallback_count = 0\n",
    "\n",
    "            for i, article in enumerate(articles):\n",
    "                print(f\"\\nArticle {i+1}/{len(articles)}\")\n",
    "\n",
    "                # Truncate text if very long for better performance\n",
    "                truncated_text = article[:1500]\n",
    "\n",
    "                # Try zero-shot classification\n",
    "                try:\n",
    "                    # Initialize zero-shot classifier if not already done\n",
    "                    if not hasattr(self, 'zero_shot_classifier'):\n",
    "                        from transformers import pipeline\n",
    "                        print(\"Initializing zero-shot domain classifier...\")\n",
    "                        device = 0 if self.device == \"cuda\" else -1\n",
    "                        self.zero_shot_classifier = pipeline(\n",
    "                            \"zero-shot-classification\",\n",
    "                            model=\"facebook/bart-large-mnli\",\n",
    "                            device=device\n",
    "                        )\n",
    "\n",
    "                    # Define candidate domains\n",
    "                    candidate_domains = [\n",
    "                        \"technology\", \"business\", \"health\", \"politics\", \"sports\",\n",
    "                        \"entertainment\", \"science\", \"environment\", \"world\", \"education\",\n",
    "                        \"food\", \"travel\", \"automotive\", \"real estate\", \"cybersecurity\",\n",
    "                        \"artificial intelligence\", \"space\", \"agriculture\", \"mental health\"\n",
    "                    ]\n",
    "\n",
    "                    # Try specific templates\n",
    "                    best_score = 0\n",
    "                    best_domain = None\n",
    "                    best_result = None\n",
    "\n",
    "                    hypothesis_templates_to_try = [\n",
    "                        \"This news article is about {}.\",\n",
    "                        \"The topic of this document is {}.\",\n",
    "                        \"This text discusses {}.\",\n",
    "                        \"The main subject of this content is {}.\"\n",
    "                    ]\n",
    "\n",
    "                    for template in hypothesis_templates_to_try:\n",
    "                        try:\n",
    "                            result = self.zero_shot_classifier(\n",
    "                                truncated_text,\n",
    "                                candidate_domains,\n",
    "                                multi_label=False,\n",
    "                                hypothesis_template=template\n",
    "                            )\n",
    "                            current_score = result['scores'][0]\n",
    "                            if current_score > best_score:\n",
    "                                best_score = current_score\n",
    "                                best_domain = result['labels'][0]\n",
    "                                best_result = result\n",
    "                        except Exception:\n",
    "                            continue\n",
    "\n",
    "                    # Try default template\n",
    "                    try:\n",
    "                        result_default = self.zero_shot_classifier(\n",
    "                            truncated_text,\n",
    "                            candidate_domains,\n",
    "                            multi_label=False\n",
    "                        )\n",
    "                        default_score = result_default['scores'][0]\n",
    "                        if default_score > best_score:\n",
    "                            best_score = default_score\n",
    "                            best_domain = result_default['labels'][0]\n",
    "                            best_result = result_default\n",
    "                    except Exception:\n",
    "                        pass\n",
    "\n",
    "                    # Process results\n",
    "                    if best_result is None:\n",
    "                        # Zero-shot classification failed\n",
    "                        fallback_domain = self._keyword_based_domain_detection(article)\n",
    "                        article_results.append({\n",
    "                            \"zsl_success\": False,\n",
    "                            \"zsl_score\": 0,\n",
    "                            \"zsl_domain\": None,\n",
    "                            \"fallback_domain\": fallback_domain,\n",
    "                            \"final_domain\": fallback_domain,\n",
    "                            \"used_fallback\": True\n",
    "                        })\n",
    "                        fallback_count += 1\n",
    "                    else:\n",
    "                        # Zero-shot classification succeeded\n",
    "                        zsl_domain = best_domain\n",
    "                        zsl_score = best_score\n",
    "\n",
    "                        # Check if score exceeds threshold\n",
    "                        if zsl_score >= threshold:\n",
    "                            final_domain = zsl_domain\n",
    "                            used_fallback = False\n",
    "                            zsl_success_count += 1\n",
    "                        else:\n",
    "                            # Score below threshold, use fallback\n",
    "                            fallback_domain = self._keyword_based_domain_detection(article)\n",
    "                            final_domain = fallback_domain\n",
    "                            used_fallback = True\n",
    "                            fallback_count += 1\n",
    "\n",
    "                        article_results.append({\n",
    "                            \"zsl_success\": True,\n",
    "                            \"zsl_score\": zsl_score,\n",
    "                            \"zsl_domain\": zsl_domain,\n",
    "                            \"fallback_domain\": fallback_domain if used_fallback else None,\n",
    "                            \"final_domain\": final_domain,\n",
    "                            \"used_fallback\": used_fallback\n",
    "                        })\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in zero-shot domain detection: {str(e)}\")\n",
    "                    fallback_domain = self._keyword_based_domain_detection(article)\n",
    "                    article_results.append({\n",
    "                        \"zsl_success\": False,\n",
    "                        \"zsl_score\": 0,\n",
    "                        \"zsl_domain\": None,\n",
    "                        \"fallback_domain\": fallback_domain,\n",
    "                        \"final_domain\": fallback_domain,\n",
    "                        \"used_fallback\": True\n",
    "                    })\n",
    "                    fallback_count += 1\n",
    "\n",
    "            # Calculate statistics\n",
    "            zsl_success_rate = zsl_success_count / len(articles)\n",
    "            fallback_rate = fallback_count / len(articles)\n",
    "\n",
    "            # Calculate average ZSL score\n",
    "            zsl_scores = [r[\"zsl_score\"] for r in article_results if r[\"zsl_success\"]]\n",
    "            avg_zsl_score = sum(zsl_scores) / len(zsl_scores) if zsl_scores else 0\n",
    "\n",
    "            # Store results\n",
    "            results[threshold] = {\n",
    "                \"zsl_success_rate\": zsl_success_rate,\n",
    "                \"fallback_rate\": fallback_rate,\n",
    "                \"avg_zsl_score\": avg_zsl_score,\n",
    "                \"article_results\": article_results\n",
    "            }\n",
    "\n",
    "            # Print summary\n",
    "            print(f\"\\nConfidence Threshold {threshold:.2f} Summary:\")\n",
    "            print(f\"- ZSL Success Rate: {zsl_success_rate:.1%}\")\n",
    "            print(f\"- Fallback Rate: {fallback_rate:.1%}\")\n",
    "            print(f\"- Average ZSL Score: {avg_zsl_score:.3f}\")\n",
    "\n",
    "        # Print comparison table\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"CONFIDENCE THRESHOLD COMPARISON\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"{'Threshold':<10} {'ZSL Rate':<10} {'Fallback':<10} {'Avg Score':<10}\")\n",
    "        print(\"-\"*40)\n",
    "\n",
    "        for threshold in confidence_thresholds:\n",
    "            zsl_rate = results[threshold][\"zsl_success_rate\"]\n",
    "            fallback_rate = results[threshold][\"fallback_rate\"]\n",
    "            avg_score = results[threshold][\"avg_zsl_score\"]\n",
    "            print(f\"{threshold:<10.2f} {zsl_rate:<10.1%} {fallback_rate:<10.1%} {avg_score:<10.3f}\")\n",
    "\n",
    "        # Find optimal threshold based on ZSL success rate\n",
    "        target_fallback_rate = 0.2  # Target fallback rate (20%)\n",
    "        best_threshold = None\n",
    "        best_diff = float('inf')\n",
    "\n",
    "        for threshold in confidence_thresholds:\n",
    "            fallback_rate = results[threshold][\"fallback_rate\"]\n",
    "            diff = abs(fallback_rate - target_fallback_rate)\n",
    "            if diff < best_diff:\n",
    "                best_diff = diff\n",
    "                best_threshold = threshold\n",
    "\n",
    "        if best_threshold is not None:\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(f\"RECOMMENDED CONFIDENCE THRESHOLD: {best_threshold:.2f}\")\n",
    "            print(\"=\"*80)\n",
    "            print(f\"This threshold results in a ZSL success rate of {results[best_threshold]['zsl_success_rate']:.1%}\")\n",
    "            print(f\"and a fallback rate of {results[best_threshold]['fallback_rate']:.1%}\")\n",
    "            print(f\"with an average ZSL score of {results[best_threshold]['avg_zsl_score']:.3f}\")\n",
    "\n",
    "            print(\"\\nTo apply this threshold, update the confidence_threshold variable in the detect_domain method:\")\n",
    "            print(f\"confidence_threshold = {best_threshold:.2f}  # Changed from 0.4\")\n",
    "\n",
    "        return results\n",
    "\n",
    "    def create_labeled_dataset(self, articles: List[str], manual_review: bool = False) -> List[Tuple[str, str]]:\n",
    "        \"\"\"\n",
    "        Create a labeled dataset for training the domain classifier.\n",
    "        Uses zero-shot learning with high confidence threshold to create initial labels,\n",
    "        then optionally allows for manual review.\n",
    "\n",
    "        Args:\n",
    "            articles: List of articles to label\n",
    "            manual_review: Whether to allow manual review of labels\n",
    "\n",
    "        Returns:\n",
    "            List of (article_text, domain) tuples\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"CREATING LABELED DATASET FOR DOMAIN CLASSIFIER\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        labeled_data = []\n",
    "        high_confidence_count = 0\n",
    "\n",
    "        # Process each article\n",
    "        for i, article in enumerate(articles):\n",
    "            print(f\"\\nArticle {i+1}/{len(articles)}\")\n",
    "\n",
    "            # Use zero-shot with high confidence threshold\n",
    "            try:\n",
    "                # Initialize zero-shot classifier if not already done\n",
    "                if not hasattr(self, 'zero_shot_classifier'):\n",
    "                    from transformers import pipeline\n",
    "                    print(\"Initializing zero-shot domain classifier...\")\n",
    "                    device = 0 if self.device == \"cuda\" else -1\n",
    "                    self.zero_shot_classifier = pipeline(\n",
    "                        \"zero-shot-classification\",\n",
    "                        model=\"facebook/bart-large-mnli\",\n",
    "                        device=device\n",
    "                    )\n",
    "\n",
    "                # Truncate text if very long\n",
    "                truncated_text = article[:1500]\n",
    "\n",
    "                # Define candidate domains\n",
    "                candidate_domains = [\n",
    "                    \"technology\", \"business\", \"health\", \"politics\", \"sports\",\n",
    "                    \"entertainment\", \"science\", \"environment\", \"world\", \"education\",\n",
    "                    \"food\", \"travel\", \"automotive\", \"real estate\", \"cybersecurity\",\n",
    "                    \"artificial intelligence\", \"space\", \"agriculture\", \"mental health\"\n",
    "                ]\n",
    "\n",
    "                # Try multiple templates\n",
    "                best_score = 0\n",
    "                best_domain = None\n",
    "\n",
    "                hypothesis_templates_to_try = [\n",
    "                    \"This news article is about {}.\",\n",
    "                    \"The topic of this document is {}.\",\n",
    "                    \"This text discusses {}.\",\n",
    "                    \"The main subject of this content is {}.\"\n",
    "                ]\n",
    "\n",
    "                for template in hypothesis_templates_to_try:\n",
    "                    try:\n",
    "                        result = self.zero_shot_classifier(\n",
    "                            truncated_text,\n",
    "                            candidate_domains,\n",
    "                            multi_label=False,\n",
    "                            hypothesis_template=template\n",
    "                        )\n",
    "                        current_score = result['scores'][0]\n",
    "                        if current_score > best_score:\n",
    "                            best_score = current_score\n",
    "                            best_domain = result['labels'][0]\n",
    "                    except Exception:\n",
    "                        continue\n",
    "\n",
    "                # Try default template\n",
    "                try:\n",
    "                    result_default = self.zero_shot_classifier(\n",
    "                        truncated_text,\n",
    "                        candidate_domains,\n",
    "                        multi_label=False\n",
    "                    )\n",
    "                    default_score = result_default['scores'][0]\n",
    "                    if default_score > best_score:\n",
    "                        best_score = default_score\n",
    "                        best_domain = result_default['labels'][0]\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "                # Map specific domains to broader categories if needed\n",
    "                domain_mapping = {\n",
    "                    \"artificial intelligence\": \"technology\",\n",
    "                    \"cybersecurity\": \"technology\",\n",
    "                    \"space\": \"science\",\n",
    "                    \"agriculture\": \"science\",\n",
    "                    \"mental health\": \"health\",\n",
    "                    \"automotive\": \"technology\",\n",
    "                    \"real estate\": \"business\",\n",
    "                    \"food\": \"lifestyle\"\n",
    "                }\n",
    "\n",
    "                if best_domain in domain_mapping:\n",
    "                    best_domain = domain_mapping[best_domain]\n",
    "\n",
    "                # Check confidence\n",
    "                high_confidence_threshold = 0.6  # High threshold for automatic labeling\n",
    "\n",
    "                if best_score >= high_confidence_threshold:\n",
    "                    print(f\"High confidence label: {best_domain} (Score: {best_score:.4f})\")\n",
    "                    domain = best_domain\n",
    "                    high_confidence_count += 1\n",
    "                else:\n",
    "                    print(f\"Low confidence label: {best_domain} (Score: {best_score:.4f})\")\n",
    "                    # Use keyword-based fallback\n",
    "                    keyword_domain = self._keyword_based_domain_detection(article)\n",
    "\n",
    "                    if manual_review:\n",
    "                        # Print excerpt for manual review\n",
    "                        excerpt = article[:300] + \"...\" if len(article) > 300 else article\n",
    "                        print(\"\\nArticle excerpt:\")\n",
    "                        print(\"-\" * 40)\n",
    "                        print(excerpt)\n",
    "                        print(\"-\" * 40)\n",
    "                        print(f\"ZSL domain: {best_domain} (Score: {best_score:.4f})\")\n",
    "                        print(f\"Keyword domain: {keyword_domain}\")\n",
    "\n",
    "                        # Ask for manual label\n",
    "                        domain_options = \", \".join(sorted(set(candidate_domains) - {\"artificial intelligence\", \"cybersecurity\", \"space\", \"agriculture\", \"mental health\", \"automotive\", \"real estate\"}))\n",
    "                        manual_domain = input(f\"Enter domain ({domain_options}), or press Enter to accept ZSL domain: \")\n",
    "\n",
    "                        if manual_domain.strip():\n",
    "                            domain = manual_domain.strip()\n",
    "                        else:\n",
    "                            domain = best_domain\n",
    "                    else:\n",
    "                        # Without manual review, use ZSL domain if score is at least 0.4\n",
    "                        if best_score >= 0.4:\n",
    "                            domain = best_domain\n",
    "                        else:\n",
    "                            domain = keyword_domain\n",
    "\n",
    "                # Add to labeled data\n",
    "                labeled_data.append((article, domain))\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing article: {str(e)}\")\n",
    "                # Skip this article\n",
    "                continue\n",
    "\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"LABELED DATASET CREATED: {len(labeled_data)} articles\")\n",
    "        print(f\"High confidence labels: {high_confidence_count} ({high_confidence_count/len(labeled_data):.1%})\")\n",
    "\n",
    "        # Print domain distribution\n",
    "        domain_counts = {}\n",
    "        for _, domain in labeled_data:\n",
    "            domain_counts[domain] = domain_counts.get(domain, 0) + 1\n",
    "\n",
    "        print(\"\\nDomain distribution:\")\n",
    "        for domain, count in sorted(domain_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "            print(f\"- {domain}: {count} articles ({count/len(labeled_data):.1%})\")\n",
    "\n",
    "        return labeled_data\n",
    "\n",
    "    def improve_domain_detection(self, articles: List[str], num_articles: int = 10) -> Dict[str, Any]:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"DOMAIN DETECTION IMPROVEMENT PROCESS\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        # Step 1: Calibrate confidence threshold\n",
    "        print(\"\\nSTEP 1: Calibrating Confidence Threshold\")\n",
    "        calibration_results = self.calibrate_domain_detection(\n",
    "            articles[:num_articles],\n",
    "            confidence_thresholds=[0.25, 0.30, 0.35, 0.40, 0.45]\n",
    "        )\n",
    "\n",
    "        # Step 2: Create labeled dataset\n",
    "        print(\"\\nSTEP 2: Creating Labeled Dataset\")\n",
    "        labeled_data = self.create_labeled_dataset(articles)\n",
    "\n",
    "        # Step 3: Train fallback classifier\n",
    "        print(\"\\nSTEP 3: Training Fallback Classifier\")\n",
    "        self.train_domain_classifier(labeled_data)\n",
    "\n",
    "        # Test improved domain detection\n",
    "        print(\"\\nTesting Improved Domain Detection\")\n",
    "        test_articles = articles[:5]  # Use first 5 articles for testing\n",
    "\n",
    "        results = []\n",
    "        for i, article in enumerate(test_articles):\n",
    "            print(f\"\\nArticle {i+1}:\")\n",
    "            domain = self.detect_domain(article)\n",
    "            print(f\"Detected domain: {domain}\")\n",
    "            results.append(domain)\n",
    "\n",
    "        return {\n",
    "            \"calibration_results\": calibration_results,\n",
    "            \"labeled_data_size\": len(labeled_data),\n",
    "            \"has_classifier\": self.has_domain_classifier,\n",
    "            \"test_results\": results\n",
    "        }\n",
    "\n",
    "    def apply_optimized_parameters(self, params: Dict[str, Any]) -> None:\n",
    "        \"\"\"\n",
    "        Apply optimized parameters to the extractor.\n",
    "\n",
    "        Args:\n",
    "            params: Dictionary of parameters to apply\n",
    "        \"\"\"\n",
    "        print(\"\\nApplying optimized parameters:\")\n",
    "\n",
    "        for param, value in params.items():\n",
    "            if hasattr(self, param):\n",
    "                print(f\"  Setting {param} = {value}\")\n",
    "                setattr(self, param, value)\n",
    "            else:\n",
    "                print(f\"  Warning: Parameter '{param}' not found in extractor\")\n",
    "\n",
    "        # Handle special case for beam groups\n",
    "        if \"num_beam_groups\" in params and params[\"num_beam_groups\"] > 1:\n",
    "            self.num_beam_groups = params[\"num_beam_groups\"]\n",
    "            if \"diversity_penalty\" in params:\n",
    "                self.diversity_penalty = params[\"diversity_penalty\"]\n",
    "            else:\n",
    "                self.diversity_penalty = 1.0  # Default value\n",
    "            print(f\"  Setting num_beam_groups = {self.num_beam_groups}\")\n",
    "            print(f\"  Setting diversity_penalty = {self.diversity_penalty}\")\n",
    "        elif hasattr(self, 'num_beam_groups'):\n",
    "            # Remove beam group attributes if not needed\n",
    "            delattr(self, 'num_beam_groups')\n",
    "            if hasattr(self, 'diversity_penalty'):\n",
    "                delattr(self, 'diversity_penalty')\n",
    "\n",
    "        print(\"Parameters applied successfully\")\n",
    "\n",
    "    def run_phase2_optimization(self, articles: List[str], num_articles: int = 5) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Run a complete Phase 2 optimization process:\n",
    "        1. Benchmark generation parameters\n",
    "        2. Calibrate quality filter thresholds\n",
    "        3. Calibrate domain detection confidence\n",
    "\n",
    "        Args:\n",
    "            articles: List of articles to test\n",
    "            num_articles: Number of articles to use for testing\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with optimization results\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"PHASE 2 OPTIMIZATION\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        # Step 1: Benchmark generation parameters\n",
    "        print(\"\\nSTEP 1: Benchmarking Generation Parameters\")\n",
    "        generation_results = self.optimize_generation_across_articles(articles, num_articles=num_articles)\n",
    "\n",
    "        # Apply best parameters\n",
    "        if generation_results[\"best_params\"]:\n",
    "            self.apply_optimized_parameters(generation_results[\"best_params\"])\n",
    "\n",
    "        # Step 2: Calibrate quality filter thresholds\n",
    "        print(\"\\nSTEP 2: Calibrating Quality Filter Thresholds\")\n",
    "        threshold_results = self.tune_quality_thresholds(articles[:num_articles])\n",
    "\n",
    "        # Step 3: Calibrate domain detection confidence\n",
    "        print(\"\\nSTEP 3: Calibrating Domain Detection Confidence\")\n",
    "        confidence_results = self.calibrate_domain_detection(articles[:num_articles])\n",
    "\n",
    "        # Restore original parameters\n",
    "        print(\"\\nOptimization complete. Use the recommended settings to update your code.\")\n",
    "\n",
    "        return {\n",
    "            \"generation_results\": generation_results,\n",
    "            \"threshold_results\": threshold_results,\n",
    "            \"confidence_results\": confidence_results\n",
    "        }\n",
    "\n",
    "    def find_best_params(self, text: str) -> Tuple[Dict[str, Any], List[str]]:\n",
    "        \"\"\"\n",
    "        Find the best generation parameters for a specific text.\n",
    "\n",
    "        Args:\n",
    "            text: Input text\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (best_params, keyphrases)\n",
    "        \"\"\"\n",
    "        # Test different parameter combinations\n",
    "        param_results = self.test_generation_params(text)\n",
    "\n",
    "        # Evaluate each result\n",
    "        best_score = -1\n",
    "        best_params = None\n",
    "        best_keyphrases = []\n",
    "\n",
    "        for param_key, keyphrases in param_results.items():\n",
    "            # Skip if no keyphrases were generated\n",
    "            if not keyphrases:\n",
    "                continue\n",
    "\n",
    "            # Count keyphrases\n",
    "            num_keyphrases = len(keyphrases)\n",
    "\n",
    "            # Count multi-word keyphrases\n",
    "            multi_word_count = sum(1 for kp in keyphrases if len(kp.split()) > 1)\n",
    "\n",
    "            # Check for n-gram patterns (consecutive words from original text)\n",
    "            ngram_penalty = 0\n",
    "            for i in range(len(keyphrases) - 1):\n",
    "                kp1 = keyphrases[i].split()\n",
    "                kp2 = keyphrases[i+1].split()\n",
    "\n",
    "                # Check if the last words of kp1 match the first words of kp2\n",
    "                if len(kp1) > 0 and len(kp2) > 0:\n",
    "                    if kp1[-1] == kp2[0]:\n",
    "                        ngram_penalty += 0.2\n",
    "\n",
    "            # Calculate average keyphrase length\n",
    "            avg_length = sum(len(kp.split()) for kp in keyphrases) / num_keyphrases\n",
    "\n",
    "            # Prefer 2-3 word keyphrases\n",
    "            length_score = 1.0 - abs(avg_length - 2.5) / 2.5\n",
    "\n",
    "            # Calculate multi-word percentage\n",
    "            multi_word_percentage = multi_word_count / num_keyphrases if num_keyphrases > 0 else 0\n",
    "\n",
    "            # Calculate final score\n",
    "            num_score = min(num_keyphrases / 10, 1.5)\n",
    "\n",
    "            # Penalize too few or too many keyphrases\n",
    "            if num_keyphrases < 3:\n",
    "                num_score *= 0.5\n",
    "            elif num_keyphrases > 20:\n",
    "                num_score *= 0.7\n",
    "\n",
    "            final_score = (\n",
    "                (num_score * 0.3) +\n",
    "                (multi_word_percentage * 0.4) +\n",
    "                (length_score * 0.3) -\n",
    "                ngram_penalty\n",
    "            )\n",
    "\n",
    "            # Update best parameters if this score is higher\n",
    "            if final_score > best_score:\n",
    "                best_score = final_score\n",
    "                best_params = param_key\n",
    "                best_keyphrases = keyphrases\n",
    "\n",
    "        # Parse the best parameter key\n",
    "        if best_params:\n",
    "            param_dict = {}\n",
    "            for param in best_params.split(', '):\n",
    "                key, value = param.split('=')\n",
    "                if key == 'temp':\n",
    "                    param_dict['temperature'] = float(value)\n",
    "                elif key == 'top_p':\n",
    "                    param_dict['top_p'] = float(value)\n",
    "                elif key == 'len_pen':\n",
    "                    param_dict['length_penalty'] = float(value)\n",
    "                elif key == 'prompt':\n",
    "                    param_dict['prompt_template_idx'] = int(value)\n",
    "        else:\n",
    "            # Default parameters if no good combination found\n",
    "            param_dict = {\n",
    "                'temperature': 0.7,\n",
    "                'top_p': 0.92,\n",
    "                'length_penalty': 0.8,\n",
    "                'prompt_template_idx': 0\n",
    "            }\n",
    "\n",
    "        return param_dict, best_keyphrases\n",
    "\n",
    "    def extract_title_and_lead(self, text: str) -> Tuple[str, str, str]:\n",
    "        \"\"\"\n",
    "        Extract the title, lead paragraph, and first few paragraphs from a text.\n",
    "        For news articles, the title is often the first line, the lead paragraph is the first paragraph,\n",
    "        and the first few paragraphs contain the most important information.\n",
    "\n",
    "        Args:\n",
    "            text: Input text\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (title, lead_paragraph, first_paragraphs)\n",
    "        \"\"\"\n",
    "        # Split text into lines\n",
    "        lines = text.split('\\n')\n",
    "\n",
    "        # Extract title (first non-empty line)\n",
    "        title = \"\"\n",
    "        for line in lines:\n",
    "            if line.strip():\n",
    "                title = line.strip()\n",
    "                break\n",
    "\n",
    "        # Extract paragraphs\n",
    "        paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
    "\n",
    "        # Extract lead paragraph (first paragraph after title)\n",
    "        lead_paragraph = \"\"\n",
    "        if len(paragraphs) > 1 and title in paragraphs[0]:\n",
    "            # If title is in the first paragraph, use the second paragraph\n",
    "            lead_paragraph = paragraphs[1]\n",
    "        elif paragraphs:\n",
    "            # Otherwise use the first paragraph\n",
    "            lead_paragraph = paragraphs[0]\n",
    "            # If the lead paragraph is just the title, use the next paragraph if available\n",
    "            if lead_paragraph == title and len(paragraphs) > 1:\n",
    "                lead_paragraph = paragraphs[1]\n",
    "\n",
    "        # Extract first few paragraphs (up to 3)\n",
    "        first_paragraphs = \"\"\n",
    "        if len(paragraphs) > 1:\n",
    "            # Join the first 2-3 paragraphs (excluding the title paragraph if it's separate)\n",
    "            if title in paragraphs[0] and paragraphs[0] == title:\n",
    "                # Title is a separate paragraph\n",
    "                first_paragraphs = \" \".join(paragraphs[1:min(4, len(paragraphs))])\n",
    "            else:\n",
    "                # Title is part of the first paragraph or not a separate paragraph\n",
    "                first_paragraphs = \" \".join(paragraphs[:min(3, len(paragraphs))])\n",
    "        else:\n",
    "            # Only one paragraph\n",
    "            first_paragraphs = lead_paragraph\n",
    "\n",
    "        return title, lead_paragraph, first_paragraphs\n",
    "\n",
    "    def score_keyphrases_by_relevance(self, keyphrases: List[str], text: str) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Score keyphrases by their semantic relevance to the original text.\n",
    "        Uses a combination of document-level and sentence-level similarity for more accurate scoring.\n",
    "        Also boosts keyphrases that appear in the title or lead paragraph.\n",
    "\n",
    "        Args:\n",
    "            keyphrases: List of keyphrases\n",
    "            text: Original text\n",
    "\n",
    "        Returns:\n",
    "            List of (keyphrase, score) tuples\n",
    "        \"\"\"\n",
    "        if not keyphrases:\n",
    "            return []\n",
    "\n",
    "        # Get document-level embedding\n",
    "        doc_embedding = self.sentence_model.encode([text], show_progress_bar=False)[0]\n",
    "\n",
    "        # Extract title, lead paragraph, and first paragraphs for potential importance boosting\n",
    "        # Store these for use in future enhancements\n",
    "        self.title, self.lead_paragraph, self.first_paragraphs = self.extract_title_and_lead(text)\n",
    "\n",
    "    # Get keyphrase embeddings\n",
    "        keyphrase_embeddings = self.sentence_model.encode(keyphrases, show_progress_bar=False)\n",
    "\n",
    "        # Extract named entities for boosting if NER is enabled\n",
    "        named_entities = set()\n",
    "        if self.use_ner:\n",
    "            # Fix: extract_named_entities now returns a list of strings, not tuples\n",
    "            named_entities = set(self.extract_named_entities(text))\n",
    "\n",
    "        # Split text into sentences for contextual similarity\n",
    "        sentences = self._split_into_sentences(text)\n",
    "\n",
    "        # Get sentence embeddings (only compute once)\n",
    "        sentence_embeddings = self.sentence_model.encode(sentences, show_progress_bar=False)\n",
    "        # Calculate similarities\n",
    "        scored_keyphrases = []\n",
    "        for i, kp in enumerate(keyphrases):\n",
    "            kp_embedding = keyphrase_embeddings[i]\n",
    "            kp_lower = kp.lower()\n",
    "\n",
    "            # 1. Calculate document-level similarity\n",
    "            doc_similarity = cosine_similarity(\n",
    "                doc_embedding.reshape(1, -1),\n",
    "                kp_embedding.reshape(1, -1)\n",
    "            )[0][0]\n",
    "\n",
    "            # 2. Calculate contextual sentence-level similarity\n",
    "            # Find sentences that contain this keyphrase\n",
    "            containing_sentences = []\n",
    "            containing_sentence_indices = []\n",
    "\n",
    "            for j, sentence in enumerate(sentences):\n",
    "                # Check if keyphrase appears in sentence (case-insensitive)\n",
    "                if kp_lower in sentence.lower():\n",
    "                    containing_sentences.append(sentence)\n",
    "                    containing_sentence_indices.append(j)\n",
    "\n",
    "            # Calculate sentence-level similarity\n",
    "            sentence_similarity = 0.0\n",
    "            if containing_sentences:\n",
    "                # Calculate similarity with each containing sentence\n",
    "                sentence_similarities = []\n",
    "                for idx in containing_sentence_indices:\n",
    "                    sent_embedding = sentence_embeddings[idx]\n",
    "                    sim = cosine_similarity(\n",
    "                        sent_embedding.reshape(1, -1),\n",
    "                        kp_embedding.reshape(1, -1)\n",
    "                    )[0][0]\n",
    "                    sentence_similarities.append(sim)\n",
    "\n",
    "                # Use the maximum similarity with any containing sentence\n",
    "                sentence_similarity = max(sentence_similarities)\n",
    "\n",
    "            # 3. Apply contextual weighting\n",
    "            # If keyphrase appears in specific sentences, weight sentence similarity higher\n",
    "            # Otherwise, rely more on document similarity\n",
    "            if containing_sentences:\n",
    "                # Combine document and sentence similarity with more weight on sentence similarity\n",
    "                # since we found contextual matches\n",
    "                combined_similarity = 0.4 * doc_similarity + 0.6 * sentence_similarity\n",
    "            else:\n",
    "                # No specific sentence context found, rely more on document similarity\n",
    "                # but penalize slightly for not appearing explicitly\n",
    "                combined_similarity = 0.9 * doc_similarity\n",
    "\n",
    "            # 4. Apply contextual importance scoring\n",
    "            final_score = combined_similarity\n",
    "\n",
    "            # Apply boost for named entities (10% boost, reduced from 15%)\n",
    "            if kp in named_entities:\n",
    "                final_score = min(final_score * 1.1, 1.0)\n",
    "\n",
    "            # Calculate contextual importance using our new method\n",
    "            contextual_importance = self.calculate_contextual_importance(kp, text)\n",
    "\n",
    "            # Apply contextual importance as a multiplier to the final score\n",
    "            # The contextual_importance score ranges from 0.0-2.0, so this will\n",
    "            # either boost or reduce the score based on the keyphrase's importance\n",
    "            final_score = min(final_score * contextual_importance, 1.0)\n",
    "\n",
    "            # 5. Apply length-based adjustments\n",
    "            word_count = len(kp.split())\n",
    "            if word_count > 1:\n",
    "                # Small boost for multi-word phrases (2% per word beyond the first, up to 6%)\n",
    "                length_boost = min(0.06, (word_count - 1) * 0.02)\n",
    "                final_score = min(final_score * (1 + length_boost), 1.0)\n",
    "\n",
    "            # Add to results\n",
    "            scored_keyphrases.append((kp, final_score))\n",
    "\n",
    "        # Sort by final score\n",
    "        scored_keyphrases.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        return scored_keyphrases\n",
    "\n",
    "    def _split_into_sentences(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Split text into sentences using NLTK for contextual similarity calculation.\n",
    "\n",
    "        Args:\n",
    "            text: Input text\n",
    "\n",
    "        Returns:\n",
    "            List of sentences\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Use NLTK's recommended sentence tokenizer\n",
    "            from nltk.tokenize import sent_tokenize\n",
    "            sentences = sent_tokenize(text)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: NLTK sent_tokenize failed: {e}. Falling back to basic split.\")\n",
    "            # Basic fallback for safety\n",
    "            sentences = re.split(r'(?<=\\.|\\?|\\!)\\s+', text)\n",
    "\n",
    "        # Filter out empty sentences and strip whitespace\n",
    "        sentences = [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "        # Ensure we have at least one sentence\n",
    "        if not sentences and text.strip():\n",
    "            sentences = [text.strip()]\n",
    "\n",
    "        return sentences\n",
    "\n",
    "    def enhance_semantic_coherence(self, keyphrases: List[Tuple[str, float]], text: str) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Enhance semantic coherence by ensuring keyphrases form a coherent set that covers the main topics.\n",
    "\n",
    "        Args:\n",
    "            keyphrases: List of (keyphrase, score) tuples\n",
    "            text: Original text\n",
    "\n",
    "        Returns:\n",
    "            Enhanced list of (keyphrase, score) tuples with adjusted scores\n",
    "        \"\"\"\n",
    "        if not keyphrases or len(keyphrases) < 2:\n",
    "            return keyphrases\n",
    "\n",
    "        # Extract keyphrases and scores\n",
    "        kps = [kp for kp, _ in keyphrases]\n",
    "        scores = [score for _, score in keyphrases]\n",
    "\n",
    "        # Get embeddings for keyphrases and document\n",
    "        kp_embeddings = self.sentence_model.encode(kps, show_progress_bar=False)\n",
    "        doc_embedding = self.sentence_model.encode([text], show_progress_bar=False)[0]\n",
    "\n",
    "        # Calculate pairwise similarities between keyphrases\n",
    "        similarity_matrix = cosine_similarity(kp_embeddings)\n",
    "\n",
    "        # Calculate coherence score for each keyphrase\n",
    "        coherence_scores = []\n",
    "        for i in range(len(kps)):\n",
    "            # Average similarity to other keyphrases (excluding self-similarity)\n",
    "            similarities = [similarity_matrix[i][j] for j in range(len(kps)) if i != j]\n",
    "            avg_similarity = sum(similarities) / len(similarities) if similarities else 0\n",
    "\n",
    "            # Document relevance\n",
    "            doc_relevance = cosine_similarity([kp_embeddings[i]], [doc_embedding])[0][0]\n",
    "\n",
    "            # Combine for coherence score (balance between relevance and similarity to other keyphrases)\n",
    "            coherence_score = 0.7 * doc_relevance + 0.3 * avg_similarity\n",
    "            coherence_scores.append(coherence_score)\n",
    "\n",
    "        # Adjust original scores based on coherence\n",
    "        adjusted_scores = [0.7 * scores[i] + 0.3 * coherence_scores[i] for i in range(len(scores))]\n",
    "\n",
    "        # Create new keyphrase list with adjusted scores\n",
    "        enhanced_keyphrases = [(kps[i], adjusted_scores[i]) for i in range(len(kps))]\n",
    "\n",
    "        # Sort by adjusted score\n",
    "        enhanced_keyphrases.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        return enhanced_keyphrases\n",
    "\n",
    "    def calculate_contextual_importance(self, keyphrase: str, text: str) -> float:\n",
    "        \"\"\"\n",
    "        Calculate the contextual importance of a keyphrase based on its relationship\n",
    "        to the document structure and semantic context.\n",
    "\n",
    "        Args:\n",
    "            keyphrase: The keyphrase to evaluate\n",
    "            text: Original text\n",
    "\n",
    "        Returns:\n",
    "            Contextual importance score (0.0-2.0)\n",
    "        \"\"\"\n",
    "        # Extract title and sections\n",
    "        title, lead_paragraph, first_paragraphs = self.extract_title_and_lead(text)\n",
    "\n",
    "        # Calculate base importance from structural position\n",
    "        importance = 1.0\n",
    "\n",
    "        # Title match is extremely important\n",
    "        if keyphrase.lower() in title.lower():\n",
    "            importance *= 2.0\n",
    "        # Lead paragraph match is very important\n",
    "        elif keyphrase.lower() in lead_paragraph.lower():\n",
    "            importance *= 1.5\n",
    "        # First paragraphs match is important\n",
    "        elif keyphrase.lower() in first_paragraphs.lower():\n",
    "            importance *= 1.3\n",
    "\n",
    "        # Calculate semantic centrality\n",
    "        doc_embedding = self.sentence_model.encode([text], show_progress_bar=False)[0]\n",
    "        kp_embedding = self.sentence_model.encode([keyphrase], show_progress_bar=False)[0]\n",
    "        semantic_centrality = cosine_similarity([kp_embedding], [doc_embedding])[0][0]\n",
    "\n",
    "        # Adjust importance based on semantic centrality\n",
    "        importance *= (0.5 + 0.5 * semantic_centrality)\n",
    "\n",
    "        # Calculate frequency-based importance\n",
    "        text_lower = text.lower()\n",
    "        kp_lower = keyphrase.lower()\n",
    "\n",
    "        # Count occurrences with word boundary consideration\n",
    "        count = 0\n",
    "        for pattern in [f\" {kp_lower} \", f\"{kp_lower} \", f\" {kp_lower}\", f\"{kp_lower}\"]:\n",
    "            count += text_lower.count(pattern)\n",
    "\n",
    "        # Normalize by text length (per 1000 words)\n",
    "        text_length = len(text.split())\n",
    "        normalized_frequency = count / (text_length / 1000)\n",
    "\n",
    "        # Adjust importance based on frequency (with diminishing returns)\n",
    "        if normalized_frequency > 0:\n",
    "            frequency_factor = min(1.5, 1.0 + 0.1 * math.log(1 + normalized_frequency))\n",
    "            importance *= frequency_factor\n",
    "\n",
    "        # Cap importance\n",
    "        importance = min(2.0, importance)\n",
    "\n",
    "        return importance\n",
    "\n",
    "    def boost_domain_specific_concepts(self, keyphrases: List[Tuple[str, float]], domain: str) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Boost scores of keyphrases that represent important domain-specific concepts.\n",
    "        Uses a dynamic approach leveraging existing domain keyword lists and semantic similarity.\n",
    "\n",
    "        Args:\n",
    "            keyphrases: List of (keyphrase, score) tuples\n",
    "            domain: Detected domain\n",
    "\n",
    "        Returns:\n",
    "            List of (keyphrase, score) tuples with boosted scores for domain-specific concepts\n",
    "        \"\"\"\n",
    "        if not keyphrases:\n",
    "            return keyphrases\n",
    "\n",
    "        # Normalize domain name\n",
    "        normalized_domain = domain.lower()\n",
    "\n",
    "        # Map specific domains to broader categories if needed\n",
    "        domain_mapping = {\n",
    "            \"artificial intelligence\": \"technology\",\n",
    "            \"cybersecurity\": \"technology\",\n",
    "            \"automotive\": \"technology\",\n",
    "            \"real estate\": \"business\",\n",
    "            \"food\": \"food\",  # Already has its own category\n",
    "            \"environment\": \"environment\",  # Already has its own category\n",
    "            \"entertainment\": \"entertainment\"  # Already has its own category\n",
    "        }\n",
    "\n",
    "        # Get the appropriate domain for keyword lookup\n",
    "        lookup_domain = domain_mapping.get(normalized_domain, normalized_domain)\n",
    "\n",
    "        # Get domain-specific keywords\n",
    "        # First try to get from the domain_keywords dictionary\n",
    "        domain_keywords_list = []\n",
    "        if hasattr(self, 'domain_keywords') and lookup_domain in self.domain_keywords:\n",
    "            domain_keywords_list = self.domain_keywords.get(lookup_domain, [])\n",
    "        # If not found or empty, try to get from the global domain keyword lists\n",
    "        elif not domain_keywords_list:\n",
    "            # Try to access the global domain keyword lists\n",
    "            try:\n",
    "                if lookup_domain == \"technology\" and \"TECHNOLOGY_KEYWORDS\" in globals():\n",
    "                    domain_keywords_list = globals()[\"TECHNOLOGY_KEYWORDS\"]\n",
    "                elif lookup_domain == \"business\" and \"BUSINESS_KEYWORDS\" in globals():\n",
    "                    domain_keywords_list = globals()[\"BUSINESS_KEYWORDS\"]\n",
    "                elif lookup_domain == \"entertainment\" and \"ENTERTAINMENT_KEYWORDS\" in globals():\n",
    "                    domain_keywords_list = globals()[\"ENTERTAINMENT_KEYWORDS\"]\n",
    "                elif lookup_domain == \"environment\" and \"ENVIRONMENT_KEYWORDS\" in globals():\n",
    "                    domain_keywords_list = globals()[\"ENVIRONMENT_KEYWORDS\"]\n",
    "                elif lookup_domain == \"food\" and \"FOOD_KEYWORDS\" in globals():\n",
    "                    domain_keywords_list = globals()[\"FOOD_KEYWORDS\"]\n",
    "                # Add more domains as needed\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error accessing global domain keywords: {str(e)}\")\n",
    "\n",
    "        # If still no keywords, use a small default set for the specific domains we care about\n",
    "        if not domain_keywords_list:\n",
    "            default_domain_keywords = {\n",
    "                \"artificial intelligence\": [\n",
    "                    \"ai\", \"artificial intelligence\", \"machine learning\", \"deep learning\", \"neural network\",\n",
    "                    \"algorithm\", \"data\", \"model\", \"training\", \"prediction\", \"classification\", \"computer vision\",\n",
    "                    \"natural language processing\", \"nlp\", \"transformer\", \"generative ai\", \"large language model\"\n",
    "                ],\n",
    "                \"cybersecurity\": [\n",
    "                    \"cybersecurity\", \"security\", \"hack\", \"breach\", \"vulnerability\", \"threat\", \"malware\",\n",
    "                    \"ransomware\", \"phishing\", \"encryption\", \"firewall\", \"authentication\", \"zero-day\",\n",
    "                    \"exploit\", \"attack\", \"defense\", \"protection\", \"data breach\", \"cyber attack\"\n",
    "                ],\n",
    "                \"automotive\": [\n",
    "                    \"automotive\", \"car\", \"vehicle\", \"electric vehicle\", \"ev\", \"autonomous\", \"self-driving\",\n",
    "                    \"battery\", \"charging\", \"engine\", \"motor\", \"transmission\", \"fuel\", \"emissions\",\n",
    "                    \"safety\", \"driver\", \"passenger\", \"manufacturer\", \"model\", \"brand\"\n",
    "                ],\n",
    "                \"food\": [\n",
    "                    \"food\", \"cuisine\", \"dish\", \"meal\", \"recipe\", \"ingredient\", \"cooking\", \"chef\",\n",
    "                    \"restaurant\", \"taste\", \"flavor\", \"culinary\", \"nutrition\", \"diet\", \"organic\",\n",
    "                    \"sustainable\", \"farm\", \"agriculture\", \"produce\", \"meat\", \"vegetable\", \"fruit\"\n",
    "                ],\n",
    "                \"environment\": [\n",
    "                    \"environment\", \"climate\", \"climate change\", \"global warming\", \"carbon\", \"emission\",\n",
    "                    \"pollution\", \"renewable\", \"sustainable\", \"conservation\", \"ecosystem\", \"biodiversity\",\n",
    "                    \"species\", \"wildlife\", \"forest\", \"ocean\", \"water\", \"energy\", \"green\", \"recycling\"\n",
    "                ],\n",
    "                \"real estate\": [\n",
    "                    \"real estate\", \"property\", \"housing\", \"home\", \"house\", \"apartment\", \"condo\",\n",
    "                    \"commercial\", \"residential\", \"mortgage\", \"loan\", \"interest rate\", \"market\",\n",
    "                    \"buyer\", \"seller\", \"agent\", \"broker\", \"development\", \"construction\", \"investment\"\n",
    "                ],\n",
    "                \"entertainment\": [\n",
    "                    \"entertainment\", \"movie\", \"film\", \"television\", \"tv\", \"show\", \"series\", \"streaming\",\n",
    "                    \"actor\", \"actress\", \"director\", \"producer\", \"celebrity\", \"music\", \"song\", \"artist\",\n",
    "                    \"band\", \"concert\", \"performance\", \"box office\", \"audience\", \"viewer\", \"subscriber\"\n",
    "                ]\n",
    "            }\n",
    "            domain_keywords_list = default_domain_keywords.get(normalized_domain, [])\n",
    "\n",
    "        # If we still don't have any keywords, return the original keyphrases\n",
    "        if not domain_keywords_list:\n",
    "            return keyphrases\n",
    "\n",
    "        # Create a dictionary for faster lookup\n",
    "        domain_keywords_set = set(kw.lower() for kw in domain_keywords_list)\n",
    "\n",
    "        # Calculate importance tiers for domain keywords\n",
    "        # We'll use semantic similarity to determine how closely related each keyphrase is to domain concepts\n",
    "        if hasattr(self, 'sentence_model'):\n",
    "            try:\n",
    "                # Get embeddings for keyphrases\n",
    "                kp_texts = [kp for kp, _ in keyphrases]\n",
    "                kp_embeddings = self.sentence_model.encode(kp_texts, show_progress_bar=False)\n",
    "\n",
    "                # Get embeddings for domain keywords (use a sample if there are too many)\n",
    "                max_domain_keywords = 100  # Limit to avoid memory issues\n",
    "                domain_sample = domain_keywords_list[:max_domain_keywords] if len(domain_keywords_list) > max_domain_keywords else domain_keywords_list\n",
    "                domain_embeddings = self.sentence_model.encode(domain_sample, show_progress_bar=False)\n",
    "\n",
    "                # Calculate average domain embedding\n",
    "                avg_domain_embedding = np.mean(domain_embeddings, axis=0)\n",
    "\n",
    "                # Calculate similarity of each keyphrase to the domain\n",
    "                domain_similarities = cosine_similarity(kp_embeddings, [avg_domain_embedding])\n",
    "\n",
    "                # Create boosted keyphrases with semantic similarity boost\n",
    "                boosted_keyphrases = []\n",
    "                for i, (kp, score) in enumerate(keyphrases):\n",
    "                    kp_lower = kp.lower()\n",
    "\n",
    "                    # Base boost factor\n",
    "                    boost_factor = 1.0\n",
    "\n",
    "                    # Check for exact matches with domain keywords\n",
    "                    if kp_lower in domain_keywords_set:\n",
    "                        boost_factor = 1.3  # 30% boost for exact matches\n",
    "                    # Check for partial matches (if keyphrase contains a domain keyword)\n",
    "                    elif any(keyword.lower() in kp_lower for keyword in domain_keywords_list):\n",
    "                        boost_factor = 1.2  # 20% boost for partial matches\n",
    "\n",
    "                    # Apply semantic similarity boost (0-10% additional boost based on similarity)\n",
    "                    semantic_boost = 0.1 * domain_similarities[i][0]  # 0-10% boost based on similarity\n",
    "                    boost_factor += semantic_boost\n",
    "\n",
    "                    # Apply boost and cap at 1.0\n",
    "                    boosted_score = min(score * boost_factor, 1.0)\n",
    "                    boosted_keyphrases.append((kp, boosted_score))\n",
    "\n",
    "                # Sort by boosted score\n",
    "                boosted_keyphrases.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "                return boosted_keyphrases\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error in semantic domain boosting: {str(e)}\")\n",
    "                # Fall back to simple exact/partial matching\n",
    "\n",
    "        # Simple fallback approach using exact and partial matching\n",
    "        boosted_keyphrases = []\n",
    "        for kp, score in keyphrases:\n",
    "            kp_lower = kp.lower()\n",
    "\n",
    "            # Check for exact matches\n",
    "            if kp_lower in domain_keywords_set:\n",
    "                boosted_score = min(score * 1.3, 1.0)  # 30% boost for exact matches\n",
    "                boosted_keyphrases.append((kp, boosted_score))\n",
    "            # Check for partial matches (if keyphrase contains a domain keyword)\n",
    "            elif any(keyword.lower() in kp_lower for keyword in domain_keywords_list):\n",
    "                boosted_score = min(score * 1.2, 1.0)  # 20% boost for partial matches\n",
    "                boosted_keyphrases.append((kp, boosted_score))\n",
    "            # No match, keep original score\n",
    "            else:\n",
    "                boosted_keyphrases.append((kp, score))\n",
    "\n",
    "        # Sort by boosted score\n",
    "        boosted_keyphrases.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        return boosted_keyphrases\n",
    "\n",
    "    def remove_redundant_keyphrases(self, scored_keyphrases: List[Tuple[str, float]], base_threshold: float = 0.70, domain: str = \"general\") -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Remove semantically redundant keyphrases and substring duplicates using tiered thresholds.\n",
    "        Improved to be less aggressive while still maintaining quality.\n",
    "\n",
    "        Uses different thresholds based on:\n",
    "        - Keyphrase length (stricter for shorter phrases)\n",
    "        - Domain (technical domains can tolerate more specificity)\n",
    "        - Position in the ranked list (stricter for higher-ranked phrases)\n",
    "        - Semantic relationship between keyphrases\n",
    "\n",
    "        Args:\n",
    "            scored_keyphrases: List of (keyphrase, score) tuples\n",
    "            threshold: Base similarity threshold for considering keyphrases as redundant\n",
    "            domain: The detected domain of the text\n",
    "\n",
    "        Returns:\n",
    "            Filtered list of (keyphrase, score) tuples\n",
    "        \"\"\"\n",
    "        if not scored_keyphrases:\n",
    "            return []\n",
    "\n",
    "        # Sort keyphrases by score in descending order\n",
    "        sorted_keyphrases = sorted(scored_keyphrases, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Get keyphrases and their embeddings\n",
    "        keyphrases = [kp for kp, _ in sorted_keyphrases]\n",
    "        scores = [score for _, score in sorted_keyphrases]\n",
    "\n",
    "        # Get embeddings\n",
    "        embeddings = self.sentence_model.encode(keyphrases, show_progress_bar=False)\n",
    "\n",
    "        # Calculate similarity matrix\n",
    "        similarity_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "        # Initialize list to track which keyphrases to keep\n",
    "        to_keep = [True] * len(keyphrases)\n",
    "\n",
    "        # Track which keyphrases were removed and why\n",
    "        substring_removals = []\n",
    "        semantic_removals = []\n",
    "        similarity_debug = []  # For debugging high similarities\n",
    "\n",
    "        # First pass: Check for substring redundancy\n",
    "        for i in range(len(keyphrases)):\n",
    "            if not to_keep[i]:\n",
    "                continue\n",
    "\n",
    "            kp_i = keyphrases[i].lower()\n",
    "            kp_i_words = set(kp_i.split())\n",
    "\n",
    "            # Create normalized versions for better matching\n",
    "            kp_i_normalized = self._normalize_keyphrase(kp_i)\n",
    "\n",
    "            # Extract potential acronym from kp_i\n",
    "            i_acronym = self._extract_acronym(kp_i)\n",
    "\n",
    "            for j in range(i + 1, len(keyphrases)):\n",
    "                if not to_keep[j]:\n",
    "                    continue\n",
    "\n",
    "                kp_j = keyphrases[j].lower()\n",
    "                kp_j_words = set(kp_j.split())\n",
    "\n",
    "                # Create normalized versions for better matching\n",
    "                kp_j_normalized = self._normalize_keyphrase(kp_j)\n",
    "\n",
    "                # Extract potential acronym from kp_j\n",
    "                j_acronym = self._extract_acronym(kp_j)\n",
    "\n",
    "                # Check for exact substring relationship\n",
    "                is_i_in_j = kp_i in kp_j and kp_i != kp_j\n",
    "                is_j_in_i = kp_j in kp_i and kp_i != kp_j\n",
    "\n",
    "                # Check for normalized substring relationship if no exact match\n",
    "                if not (is_i_in_j or is_j_in_i):\n",
    "                    is_i_in_j = kp_i_normalized in kp_j_normalized and kp_i_normalized != kp_j_normalized\n",
    "                    is_j_in_i = kp_j_normalized in kp_i_normalized and kp_i_normalized != kp_j_normalized\n",
    "\n",
    "                # Check for plural variants\n",
    "                is_plural_variant = False\n",
    "                if not (is_i_in_j or is_j_in_i):\n",
    "                    # Check common plural forms\n",
    "                    if kp_i + \"s\" == kp_j or kp_i + \"es\" == kp_j:\n",
    "                        is_plural_variant = True\n",
    "                        is_i_in_j = True\n",
    "                    elif kp_j + \"s\" == kp_i or kp_j + \"es\" == kp_i:\n",
    "                        is_plural_variant = True\n",
    "                        is_j_in_i = True\n",
    "                    # Check for irregular plurals (if words are very similar)\n",
    "                    elif (len(kp_i) > 3 and len(kp_j) > 3 and\n",
    "                        kp_i[:-1] == kp_j[:-1] and\n",
    "                        similarity_matrix[i, j] > 0.9):\n",
    "                        is_plural_variant = True\n",
    "                        # Prefer the longer version as it's likely more specific\n",
    "                        if len(kp_i) > len(kp_j):\n",
    "                            is_j_in_i = True\n",
    "                        else:\n",
    "                            is_i_in_j = True\n",
    "\n",
    "                # Check for possessive variants\n",
    "                is_possessive_variant = False\n",
    "                if not (is_i_in_j or is_j_in_i or is_plural_variant):\n",
    "                    if kp_i + \"'s\" == kp_j or kp_i + \"s'\" == kp_j:\n",
    "                        is_possessive_variant = True\n",
    "                        is_i_in_j = True\n",
    "                    elif kp_j + \"'s\" == kp_i or kp_j + \"s'\" == kp_i:\n",
    "                        is_possessive_variant = True\n",
    "                        is_j_in_i = True\n",
    "\n",
    "                # Check for hyphenated variants\n",
    "                is_hyphenated_variant = False\n",
    "                if not (is_i_in_j or is_j_in_i or is_plural_variant or is_possessive_variant):\n",
    "                    # Replace hyphens with spaces and check again\n",
    "                    kp_i_no_hyphen = kp_i.replace('-', ' ')\n",
    "                    kp_j_no_hyphen = kp_j.replace('-', ' ')\n",
    "\n",
    "                    if kp_i_no_hyphen == kp_j or kp_i == kp_j_no_hyphen:\n",
    "                        is_hyphenated_variant = True\n",
    "                        # Prefer the hyphenated version (usually more specific)\n",
    "                        if '-' in kp_i:\n",
    "                            is_j_in_i = True\n",
    "                        else:\n",
    "                            is_i_in_j = True\n",
    "\n",
    "                # Check for acronym/full form pairs\n",
    "                is_acronym_pair = False\n",
    "                if not (is_i_in_j or is_j_in_i or is_plural_variant or is_possessive_variant or is_hyphenated_variant):\n",
    "                    # Check if i is an acronym of j\n",
    "                    if i_acronym and i_acronym == kp_j:\n",
    "                        is_acronym_pair = True\n",
    "                        is_j_in_i = True  # Prefer the full form over the acronym\n",
    "                    # Check if j is an acronym of i\n",
    "                    elif j_acronym and j_acronym == kp_i:\n",
    "                        is_acronym_pair = True\n",
    "                        is_i_in_j = True  # Prefer the full form over the acronym\n",
    "\n",
    "                # Check for article/determiner prefix (the, a, an)\n",
    "                has_article_difference = False\n",
    "                if not (is_i_in_j or is_j_in_i or is_plural_variant or is_possessive_variant or is_hyphenated_variant or is_acronym_pair):\n",
    "                    # Check if one has \"the\", \"a\", or \"an\" prefix\n",
    "                    articles = [\"the \", \"a \", \"an \"]\n",
    "                    for article in articles:\n",
    "                        if kp_i.startswith(article) and kp_i[len(article):] == kp_j:\n",
    "                            is_j_in_i = True\n",
    "                            has_article_difference = True\n",
    "                            break\n",
    "                        elif kp_j.startswith(article) and kp_j[len(article):] == kp_i:\n",
    "                            is_i_in_j = True\n",
    "                            has_article_difference = True\n",
    "                            break\n",
    "\n",
    "                # If there's a substring relationship, decide which one to keep\n",
    "                if is_i_in_j or is_j_in_i or is_plural_variant or is_possessive_variant or is_hyphenated_variant or is_acronym_pair:\n",
    "                    # Calculate word overlap ratio - for future use if needed\n",
    "                    # Currently not using these variables directly, but keeping the calculation\n",
    "                    # for potential future enhancements\n",
    "                    if len(kp_i_words) > 0 and len(kp_j_words) > 0:\n",
    "                        # Convert to sets for intersection operation\n",
    "                        kp_i_word_set = set(kp_i_words)\n",
    "                        kp_j_word_set = set(kp_j_words)\n",
    "                        # These overlap ratios could be used for more sophisticated filtering\n",
    "                        _ = len(kp_i_word_set.intersection(kp_j_word_set)) / len(kp_i_words)  # overlap_i_to_j\n",
    "                        _ = len(kp_i_word_set.intersection(kp_j_word_set)) / len(kp_j_words)  # overlap_j_to_i\n",
    "\n",
    "                    # IMPROVEMENT: Score difference threshold is now adaptive based on domain\n",
    "                    score_diff_threshold = 1.05  # Default: 5% difference\n",
    "                    if domain in [\"artificial intelligence\", \"technology\", \"science\", \"cybersecurity\"]:\n",
    "                        score_diff_threshold = 1.03  # Technical domains: 3% difference\n",
    "                    elif domain in [\"entertainment\", \"sports\"]:\n",
    "                        score_diff_threshold = 1.08  # General domains: 8% difference\n",
    "\n",
    "                    # IMPROVEMENT: More balanced decision logic for substring relationships\n",
    "                    if is_i_in_j:  # i is substring of j\n",
    "                        # If j is significantly longer and has a good score, keep j\n",
    "                        if (len(kp_j) > len(kp_i) * 1.5 and scores[j] > scores[i] * 0.85) or has_article_difference:\n",
    "                            to_keep[i] = False\n",
    "                            substring_removals.append((i, j, \"substring\"))\n",
    "                        else:\n",
    "                            # Otherwise, prefer the shorter one with higher score\n",
    "                            # IMPROVEMENT: More balanced score comparison\n",
    "                            if scores[i] > scores[j] * score_diff_threshold:\n",
    "                                to_keep[j] = False\n",
    "                                substring_removals.append((j, i, \"substring\"))\n",
    "                            else:\n",
    "                                # IMPROVEMENT: Keep both if scores are very close and in technical domain\n",
    "                                if domain in [\"artificial intelligence\", \"technology\", \"science\", \"cybersecurity\"] and scores[i] > 0.5 and scores[j] > 0.5:\n",
    "                                    # Keep both\n",
    "                                    pass\n",
    "                                else:\n",
    "                                    to_keep[i] = False\n",
    "                                    substring_removals.append((i, j, \"substring\"))\n",
    "                    else:  # j is substring of i\n",
    "                        # If i is significantly longer and has a good score, keep i\n",
    "                        if (len(kp_i) > len(kp_j) * 1.5 and scores[i] > scores[j] * 0.85) or has_article_difference:\n",
    "                            to_keep[j] = False\n",
    "                            substring_removals.append((j, i, \"substring\"))\n",
    "                        else:\n",
    "                            # Otherwise, prefer the shorter one with higher score\n",
    "                            # IMPROVEMENT: More balanced score comparison\n",
    "                            if scores[j] > scores[i] * score_diff_threshold:\n",
    "                                to_keep[i] = False\n",
    "                                substring_removals.append((i, j, \"substring\"))\n",
    "                            else:\n",
    "                                # IMPROVEMENT: Keep both if scores are very close and in technical domain\n",
    "                                if domain in [\"artificial intelligence\", \"technology\", \"science\", \"cybersecurity\"] and scores[i] > 0.5 and scores[j] > 0.5:\n",
    "                                    # Keep both\n",
    "                                    pass\n",
    "                                else:\n",
    "                                    to_keep[j] = False\n",
    "                                    substring_removals.append((j, i, \"substring\"))\n",
    "\n",
    "        # Second pass: Check for semantic similarity (only for pairs not already handled by substring check)\n",
    "        for i in range(len(keyphrases)):\n",
    "            if not to_keep[i]:\n",
    "                continue\n",
    "\n",
    "            kp_i = keyphrases[i].lower()\n",
    "            kp_i_words = kp_i.split()\n",
    "\n",
    "            # IMPROVEMENT: Reduced position-based threshold adjustment\n",
    "            position_factor = max(0.0, min(0.03, 0.03 * (1.0 - i / max(10, len(keyphrases)))))\n",
    "\n",
    "            # Calculate length-based threshold adjustment\n",
    "            # IMPROVEMENT: Less strict with shorter keyphrases\n",
    "            length_i = len(kp_i_words)\n",
    "            length_factor = 0.0\n",
    "            if length_i == 1:\n",
    "                length_factor = 0.05  # Less strict for single words (was 0.08)\n",
    "            elif length_i == 2:\n",
    "                length_factor = 0.02  # Less strict for bigrams (was 0.04)\n",
    "\n",
    "            # Domain-based threshold adjustment\n",
    "            # IMPROVEMENT: More balanced domain factors\n",
    "            domain_factor = 0.0\n",
    "            if domain in [\"technology\", \"science\", \"health\", \"artificial intelligence\", \"cybersecurity\", \"automotive\"]:\n",
    "                domain_factor = -0.03  # Less strict (allow more similar terms in technical domains)\n",
    "            elif domain in [\"entertainment\", \"sports\"]:\n",
    "                domain_factor = 0.01   # Less strict than before (was 0.02)\n",
    "\n",
    "            # IMPROVEMENT: Score-based threshold adjustment\n",
    "            # Higher-scoring keyphrases get more protection from filtering\n",
    "            score_factor = 0.0\n",
    "            if scores[i] > 0.7:\n",
    "                score_factor = 0.03  # Higher threshold (less likely to be filtered)\n",
    "            elif scores[i] > 0.5:\n",
    "                score_factor = 0.01  # Slightly higher threshold\n",
    "\n",
    "            # Calculate final adjusted threshold for this keyphrase\n",
    "            # IMPROVEMENT: Use the base_threshold parameter instead of hardcoded value\n",
    "            adjusted_threshold = base_threshold - position_factor - length_factor + domain_factor - score_factor\n",
    "\n",
    "            # IMPROVEMENT: More reasonable bounds (0.62-0.82 instead of 0.65-0.85)\n",
    "            # Use the base_threshold parameter to adjust the bounds\n",
    "            lower_bound = max(0.62, base_threshold - 0.08)\n",
    "            upper_bound = min(0.82, base_threshold + 0.12)\n",
    "            adjusted_threshold = max(lower_bound, min(upper_bound, adjusted_threshold))\n",
    "\n",
    "            for j in range(i + 1, len(keyphrases)):\n",
    "                if not to_keep[j]:\n",
    "                    continue\n",
    "\n",
    "                kp_j = keyphrases[j].lower()\n",
    "                kp_j_words = kp_j.split()\n",
    "\n",
    "                # Calculate similarity\n",
    "                similarity = similarity_matrix[i, j]\n",
    "\n",
    "                # For debugging\n",
    "                if similarity > adjusted_threshold - 0.1:  # Only log high similarities to avoid clutter\n",
    "                    similarity_debug.append((i, j, kp_i, kp_j, similarity, adjusted_threshold))\n",
    "\n",
    "                # Apply the adjusted threshold\n",
    "                if similarity > adjusted_threshold:\n",
    "                    # IMPROVEMENT: Special case for technical terms expanded\n",
    "                    # If both are technical terms in technical domains, be more lenient\n",
    "                    if domain in [\"technology\", \"science\", \"artificial intelligence\", \"cybersecurity\", \"automotive\"]:\n",
    "                        # Keep both if they're technical terms with good scores\n",
    "                        if (scores[i] > 0.55 and scores[j] > 0.55 and\n",
    "                            similarity < 0.85 and  # Not extremely similar\n",
    "                            (length_i > 1 or len(kp_j_words) > 1)):  # At least one is multi-word\n",
    "                            continue\n",
    "\n",
    "                    # IMPROVEMENT: Special case for single words expanded\n",
    "                    # If both are single words and very similar but not identical,\n",
    "                    # keep both if they have high scores\n",
    "                    if (length_i == 1 and len(kp_j_words) == 1 and\n",
    "                        kp_i != kp_j and\n",
    "                        scores[i] > 0.5 and scores[j] > 0.5 and\n",
    "                        similarity < 0.85):  # Not extremely similar\n",
    "                        continue\n",
    "\n",
    "                    # IMPROVEMENT: More balanced score comparison\n",
    "                    # Keep both if scores are very close and similarity is not too high\n",
    "                    if abs(scores[i] - scores[j]) < 0.05 and similarity < 0.78:\n",
    "                        continue\n",
    "\n",
    "                    # Keep the higher-scored keyphrase\n",
    "                    if scores[i] >= scores[j]:\n",
    "                        to_keep[j] = False\n",
    "                        semantic_removals.append((j, i, similarity, adjusted_threshold))\n",
    "                    else:\n",
    "                        to_keep[i] = False\n",
    "                        semantic_removals.append((i, j, similarity, adjusted_threshold))\n",
    "                        break  # Break since i is no longer kept\n",
    "\n",
    "        # IMPROVEMENT: Recovery mechanism to ensure we don't lose too many keyphrases\n",
    "        # If we've filtered out more than 40% of keyphrases, recover some high-scoring ones\n",
    "        filtered_count = sum(1 for k in to_keep if k)\n",
    "        original_count = len(keyphrases)\n",
    "\n",
    "        if filtered_count < original_count * 0.6:  # We've filtered more than 40%\n",
    "            # Sort keyphrases by score\n",
    "            candidates_for_recovery = [(i, scores[i]) for i in range(len(keyphrases)) if not to_keep[i]]\n",
    "            candidates_for_recovery.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "            # Calculate how many to recover\n",
    "            target_recovery = int(original_count * 0.7) - filtered_count  # Aim to keep at least 70%\n",
    "\n",
    "            # Recover high-scoring keyphrases\n",
    "            for i in range(min(target_recovery, len(candidates_for_recovery))):\n",
    "                idx, _ = candidates_for_recovery[i]\n",
    "                to_keep[idx] = True\n",
    "                print(f\"Recovered keyphrase: {keyphrases[idx]} (score: {scores[idx]:.4f})\")\n",
    "\n",
    "        # Create filtered list\n",
    "        filtered_keyphrases = []\n",
    "        for i in range(len(keyphrases)):\n",
    "            if to_keep[i]:\n",
    "                filtered_keyphrases.append((keyphrases[i], scores[i]))\n",
    "\n",
    "        # Print debug info if requested\n",
    "        if hasattr(self, 'debug_redundancy') and self.debug_redundancy:\n",
    "            print(f\"\\nRedundancy filtering: {len(sorted_keyphrases)} -> {len(filtered_keyphrases)} keyphrases\")\n",
    "            print(f\"Substring removals: {len(substring_removals)}\")\n",
    "            print(f\"Semantic similarity removals: {len(semantic_removals)}\")\n",
    "\n",
    "            if substring_removals:\n",
    "                print(\"\\nSubstring removal examples:\")\n",
    "                for i, j, _ in substring_removals[:3]:  # Show first 3 examples\n",
    "                    print(f\"  Removed '{keyphrases[i]}' (score: {scores[i]:.4f}) in favor of '{keyphrases[j]}' (score: {scores[j]:.4f})\")\n",
    "\n",
    "            if semantic_removals:\n",
    "                print(\"\\nSemantic removal examples:\")\n",
    "                for i, j, sim, adj_threshold in semantic_removals[:3]:  # Show first 3 examples\n",
    "                    print(f\"  Removed '{keyphrases[i]}' (score: {scores[i]:.4f}) in favor of '{keyphrases[j]}' (score: {scores[j]:.4f})\")\n",
    "                    print(f\"  Similarity: {sim:.4f}, Adjusted threshold: {adj_threshold:.4f}\")\n",
    "\n",
    "        return filtered_keyphrases\n",
    "\n",
    "    def _normalize_keyphrase(self, keyphrase: str) -> str:\n",
    "        \"\"\"\n",
    "        Normalize a keyphrase for better matching by removing common variations.\n",
    "\n",
    "        Args:\n",
    "            keyphrase: The keyphrase to normalize\n",
    "\n",
    "        Returns:\n",
    "            Normalized keyphrase\n",
    "        \"\"\"\n",
    "        # Convert to lowercase\n",
    "        normalized = keyphrase.lower()\n",
    "\n",
    "        # Remove possessive forms\n",
    "        normalized = re.sub(r\"'s$\", \"\", normalized)\n",
    "        normalized = re.sub(r\"s'$\", \"s\", normalized)\n",
    "\n",
    "        # Replace hyphens with spaces\n",
    "        normalized = normalized.replace(\"-\", \" \")\n",
    "\n",
    "        # Remove articles at the beginning\n",
    "        normalized = re.sub(r\"^(the|a|an) \", \"\", normalized)\n",
    "\n",
    "        # Remove extra whitespace\n",
    "        normalized = re.sub(r\"\\s+\", \" \", normalized).strip()\n",
    "\n",
    "        return normalized\n",
    "\n",
    "    def _extract_acronym(self, phrase: str) -> str:\n",
    "        \"\"\"\n",
    "        Extract a potential acronym from a multi-word phrase.\n",
    "\n",
    "        Args:\n",
    "            phrase: The phrase to extract an acronym from\n",
    "\n",
    "        Returns:\n",
    "            Potential acronym or empty string if not applicable\n",
    "        \"\"\"\n",
    "        words = phrase.split()\n",
    "\n",
    "        # Only create acronyms for phrases with 2+ words\n",
    "        if len(words) < 2:\n",
    "            return \"\"\n",
    "\n",
    "        # Skip phrases that are already likely acronyms (all caps, 2-5 letters)\n",
    "        if len(phrase) <= 5 and phrase.isupper():\n",
    "            return \"\"\n",
    "\n",
    "        # Create acronym from first letter of each word\n",
    "        acronym = \"\".join(word[0] for word in words if word and not word.lower() in ['a', 'an', 'the', 'and', 'or', 'of', 'for', 'in', 'on', 'by', 'to'])\n",
    "\n",
    "        # Only return if acronym has at least 2 characters\n",
    "        if len(acronym) >= 2:\n",
    "            return acronym.upper()\n",
    "\n",
    "        return \"\"\n",
    "\n",
    "    def test_generation_params(self, text: str, debug_output: bool = False) -> Dict[str, List[str]]:\n",
    "        \"\"\"\n",
    "        Test different generation parameters to find the best combination.\n",
    "        Systematically explores sampling vs. beam search vs. diverse beam search.\n",
    "\n",
    "        Args:\n",
    "            text: Input text\n",
    "            debug_output: Whether to print debug information about raw model output\n",
    "\n",
    "        Returns:\n",
    "            Dictionary mapping parameter combinations to keyphrases\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "\n",
    "        # Save original parameters\n",
    "        original_params = {\n",
    "            'temperature': self.temperature,\n",
    "            'top_p': self.top_p,\n",
    "            'top_k': self.top_k,\n",
    "            'length_penalty': self.length_penalty,\n",
    "            'max_new_tokens': self.max_new_tokens,\n",
    "            'prompt_template_idx': self.prompt_template_idx,\n",
    "            'num_beams': self.num_beams,\n",
    "            'use_sampling': getattr(self, 'use_sampling', True),\n",
    "            'num_beam_groups': getattr(self, 'num_beam_groups', 1),\n",
    "            'diversity_penalty': getattr(self, 'diversity_penalty', 0.0),\n",
    "            'repetition_penalty': self.repetition_penalty\n",
    "        }\n",
    "\n",
    "        # Truncate text for faster testing if very long\n",
    "        test_text = text\n",
    "        if len(text.split()) > 500:\n",
    "            # Use first 500 words for faster testing\n",
    "            test_text = ' '.join(text.split()[:500])\n",
    "\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"TESTING GENERATION PARAMETERS\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        # --- 1. Test pure beam search (no sampling) ---\n",
    "        print(\"\\nTesting Pure Beam Search:\")\n",
    "        print(\"-\"*40)\n",
    "\n",
    "        beam_search_configs = [\n",
    "            {\"num_beams\": 5, \"length_penalty\": 0.8},\n",
    "            {\"num_beams\": 8, \"length_penalty\": 0.8},\n",
    "            {\"num_beams\": 8, \"length_penalty\": 1.0},\n",
    "            {\"num_beams\": 10, \"length_penalty\": 0.8},\n",
    "            {\"num_beams\": 12, \"length_penalty\": 1.0},\n",
    "        ]\n",
    "\n",
    "        for config in beam_search_configs:\n",
    "            # Set parameters\n",
    "            self.num_beams = config[\"num_beams\"]\n",
    "            self.length_penalty = config[\"length_penalty\"]\n",
    "            self.use_sampling = False\n",
    "\n",
    "            # Remove beam group attributes if they exist\n",
    "            if hasattr(self, 'num_beam_groups'):\n",
    "                delattr(self, 'num_beam_groups')\n",
    "            if hasattr(self, 'diversity_penalty'):\n",
    "                delattr(self, 'diversity_penalty')\n",
    "\n",
    "            # Test with different prompt templates\n",
    "            for prompt_idx in range(min(3, len(self.PROMPT_TEMPLATES))):\n",
    "                self.prompt_template_idx = prompt_idx\n",
    "\n",
    "                # Generate keyphrases\n",
    "                keyphrases = self.generate_keyphrases(test_text, debug_output=debug_output)\n",
    "\n",
    "                # Store results\n",
    "                param_key = f\"beam_search, beams={config['num_beams']}, len_pen={config['length_penalty']}, prompt={prompt_idx}\"\n",
    "                results[param_key] = keyphrases\n",
    "\n",
    "                # Print parameter combination and number of keyphrases\n",
    "                print(f\"Params: {param_key} → {len(keyphrases)} keyphrases\")\n",
    "                if len(keyphrases) > 0:\n",
    "                    print(f\"  Sample: {', '.join(keyphrases[:3])}\" + (\"...\" if len(keyphrases) > 3 else \"\"))\n",
    "\n",
    "        # --- 2. Test diverse beam search ---\n",
    "        print(\"\\nTesting Diverse Beam Search:\")\n",
    "        print(\"-\"*40)\n",
    "\n",
    "        diverse_beam_configs = [\n",
    "            {\"num_beams\": 8, \"num_beam_groups\": 4, \"diversity_penalty\": 1.0},\n",
    "            {\"num_beams\": 8, \"num_beam_groups\": 2, \"diversity_penalty\": 1.5},\n",
    "            {\"num_beams\": 12, \"num_beam_groups\": 4, \"diversity_penalty\": 1.0},\n",
    "            {\"num_beams\": 12, \"num_beam_groups\": 3, \"diversity_penalty\": 1.5},\n",
    "            {\"num_beams\": 16, \"num_beam_groups\": 4, \"diversity_penalty\": 1.2},\n",
    "        ]\n",
    "\n",
    "        for config in diverse_beam_configs:\n",
    "            # Skip invalid combinations\n",
    "            if config[\"num_beams\"] % config[\"num_beam_groups\"] != 0:\n",
    "                continue\n",
    "\n",
    "            # Set parameters\n",
    "            self.num_beams = config[\"num_beams\"]\n",
    "            self.num_beam_groups = config[\"num_beam_groups\"]\n",
    "            self.diversity_penalty = config[\"diversity_penalty\"]\n",
    "            self.use_sampling = False  # Must be False for diverse beam search\n",
    "\n",
    "            # Test with different prompt templates\n",
    "            for prompt_idx in [0, 1]:  # Test with fewer prompts to save time\n",
    "                self.prompt_template_idx = prompt_idx\n",
    "\n",
    "                # Generate keyphrases\n",
    "                keyphrases = self.generate_keyphrases(test_text, debug_output=debug_output)\n",
    "\n",
    "                # Store results\n",
    "                param_key = f\"diverse_beam, beams={config['num_beams']}, groups={config['num_beam_groups']}, div_pen={config['diversity_penalty']}, prompt={prompt_idx}\"\n",
    "                results[param_key] = keyphrases\n",
    "\n",
    "                # Print parameter combination and number of keyphrases\n",
    "                print(f\"Params: {param_key} → {len(keyphrases)} keyphrases\")\n",
    "                if len(keyphrases) > 0:\n",
    "                    print(f\"  Sample: {', '.join(keyphrases[:3])}\" + (\"...\" if len(keyphrases) > 3 else \"\"))\n",
    "\n",
    "        # --- 3. Test sampling ---\n",
    "        print(\"\\nTesting Sampling:\")\n",
    "        print(\"-\"*40)\n",
    "\n",
    "        sampling_configs = [\n",
    "            {\"temperature\": 0.7, \"top_p\": 0.92, \"top_k\": 50, \"repetition_penalty\": 1.2},\n",
    "            {\"temperature\": 0.8, \"top_p\": 0.95, \"top_k\": 50, \"repetition_penalty\": 1.2},\n",
    "            {\"temperature\": 0.9, \"top_p\": 0.95, \"top_k\": 100, \"repetition_penalty\": 1.3},\n",
    "            {\"temperature\": 1.0, \"top_p\": 0.98, \"top_k\": 100, \"repetition_penalty\": 1.3},\n",
    "            {\"temperature\": 0.6, \"top_p\": 0.9, \"top_k\": 50, \"repetition_penalty\": 1.1},\n",
    "        ]\n",
    "\n",
    "        for config in sampling_configs:\n",
    "            # Set parameters\n",
    "            self.temperature = config[\"temperature\"]\n",
    "            self.top_p = config[\"top_p\"]\n",
    "            self.top_k = config[\"top_k\"]\n",
    "            self.repetition_penalty = config[\"repetition_penalty\"]\n",
    "            self.use_sampling = True\n",
    "\n",
    "            # Remove beam group attributes if they exist\n",
    "            if hasattr(self, 'num_beam_groups'):\n",
    "                delattr(self, 'num_beam_groups')\n",
    "            if hasattr(self, 'diversity_penalty'):\n",
    "                delattr(self, 'diversity_penalty')\n",
    "\n",
    "            # Test with different prompt templates\n",
    "            for prompt_idx in range(min(3, len(self.PROMPT_TEMPLATES))):\n",
    "                self.prompt_template_idx = prompt_idx\n",
    "\n",
    "                # Generate keyphrases\n",
    "                keyphrases = self.generate_keyphrases(test_text, debug_output=debug_output)\n",
    "\n",
    "                # Store results\n",
    "                param_key = f\"sampling, temp={config['temperature']}, top_p={config['top_p']}, top_k={config['top_k']}, rep_pen={config['repetition_penalty']}, prompt={prompt_idx}\"\n",
    "                results[param_key] = keyphrases\n",
    "\n",
    "                # Print parameter combination and number of keyphrases\n",
    "                print(f\"Params: {param_key} → {len(keyphrases)} keyphrases\")\n",
    "                if len(keyphrases) > 0:\n",
    "                    print(f\"  Sample: {', '.join(keyphrases[:3])}\" + (\"...\" if len(keyphrases) > 3 else \"\"))\n",
    "\n",
    "        # --- 4. Find best parameter combination ---\n",
    "        best_param_key = None\n",
    "        best_count = 0\n",
    "        best_quality_score = 0\n",
    "\n",
    "        for param_key, keyphrases in results.items():\n",
    "            # Skip if too few keyphrases\n",
    "            if len(keyphrases) < 5:\n",
    "                continue\n",
    "\n",
    "            # Calculate quality metrics\n",
    "            num_keyphrases = len(keyphrases)\n",
    "            multi_word_count = sum(1 for kp in keyphrases if len(kp.split()) > 1)\n",
    "            multi_word_percentage = multi_word_count / num_keyphrases if num_keyphrases > 0 else 0\n",
    "            avg_length = sum(len(kp.split()) for kp in keyphrases) / num_keyphrases\n",
    "\n",
    "            # Calculate quality score\n",
    "            # We want:\n",
    "            # - A good number of keyphrases (15-25 is ideal)\n",
    "            # - High percentage of multi-word phrases\n",
    "            # - Average length around 2-3 words\n",
    "\n",
    "            quantity_score = 0.0\n",
    "            if 15 <= num_keyphrases <= 25:\n",
    "                quantity_score = 1.0\n",
    "            elif 10 <= num_keyphrases < 15:\n",
    "                quantity_score = 0.8\n",
    "            elif 25 < num_keyphrases <= 35:\n",
    "                quantity_score = 0.8\n",
    "            elif 5 <= num_keyphrases < 10:\n",
    "                quantity_score = 0.6\n",
    "            elif num_keyphrases > 35:\n",
    "                quantity_score = 0.5\n",
    "\n",
    "            length_score = 1.0 - abs(avg_length - 2.5) / 2.5\n",
    "\n",
    "            quality_score = (\n",
    "                (quantity_score * 0.5) +\n",
    "                (multi_word_percentage * 0.3) +\n",
    "                (length_score * 0.2)\n",
    "            )\n",
    "\n",
    "            # Update best if this is better\n",
    "            if quality_score > best_quality_score:\n",
    "                best_quality_score = quality_score\n",
    "                best_param_key = param_key\n",
    "                best_count = num_keyphrases\n",
    "\n",
    "        # Print best parameter combination\n",
    "        if best_param_key:\n",
    "            print(\"\\nBest Parameter Combination:\")\n",
    "            print(f\"- {best_param_key}\")\n",
    "            print(f\"- Generated {best_count} keyphrases with quality score {best_quality_score:.2f}\")\n",
    "            print(f\"- Sample: {', '.join(results[best_param_key][:5])}\" + (\"...\" if len(results[best_param_key]) > 5 else \"\"))\n",
    "        else:\n",
    "            print(\"\\nNo optimal parameter combination found.\")\n",
    "\n",
    "        # Restore original parameters\n",
    "        self.temperature = original_params['temperature']\n",
    "        self.top_p = original_params['top_p']\n",
    "        self.top_k = original_params['top_k']\n",
    "        self.length_penalty = original_params['length_penalty']\n",
    "        self.max_new_tokens = original_params['max_new_tokens']\n",
    "        self.prompt_template_idx = original_params['prompt_template_idx']\n",
    "        self.num_beams = original_params['num_beams']\n",
    "        self.repetition_penalty = original_params['repetition_penalty']\n",
    "\n",
    "        # Restore or remove use_sampling\n",
    "        if 'use_sampling' in original_params:\n",
    "            self.use_sampling = original_params['use_sampling']\n",
    "\n",
    "        # Restore or remove beam group attributes\n",
    "        if original_params['num_beam_groups'] > 1:\n",
    "            self.num_beam_groups = original_params['num_beam_groups']\n",
    "            self.diversity_penalty = original_params['diversity_penalty']\n",
    "        else:\n",
    "            if hasattr(self, 'num_beam_groups'):\n",
    "                delattr(self, 'num_beam_groups')\n",
    "            if hasattr(self, 'diversity_penalty'):\n",
    "                delattr(self, 'diversity_penalty')\n",
    "\n",
    "        print(\"\\nParameter testing complete. Original parameters restored.\")\n",
    "\n",
    "        return results\n",
    "\n",
    "    def examine_raw_output(self, text: str, params: Dict[str, Any] = None) -> None:\n",
    "        \"\"\"\n",
    "        Examine the raw output from the T5 model for diagnostic purposes.\n",
    "        Prints the raw generated text before any parsing or post-processing.\n",
    "\n",
    "        Args:\n",
    "            text: Input text\n",
    "            params: Optional dictionary of parameters to use for generation\n",
    "        \"\"\"\n",
    "        # Preprocess text\n",
    "        text = self.preprocess_text(text)\n",
    "\n",
    "        # Truncate if very long\n",
    "        truncated_text = text\n",
    "        if len(text.split()) > 500:\n",
    "            truncated_text = ' '.join(text.split()[:500])\n",
    "            print(f\"Note: Text truncated to 500 words for analysis\")\n",
    "\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"EXAMINING RAW T5 OUTPUT\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        # Define parameter sets to test\n",
    "        if params:\n",
    "            # Use provided parameters\n",
    "            parameter_sets = [{\n",
    "                \"name\": \"Custom Parameters\",\n",
    "                \"params\": params\n",
    "            }]\n",
    "        else:\n",
    "            # Use default parameter sets\n",
    "            parameter_sets = [\n",
    "                {\n",
    "                    \"name\": \"Default Sampling\",\n",
    "                    \"params\": {\n",
    "                        \"do_sample\": True,\n",
    "                        \"temperature\": 0.7,\n",
    "                        \"top_p\": 0.92,\n",
    "                        \"top_k\": 50,\n",
    "                        \"repetition_penalty\": 1.2,\n",
    "                        \"max_new_tokens\": self.max_new_tokens\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"Pure Beam Search\",\n",
    "                    \"params\": {\n",
    "                        \"do_sample\": False,\n",
    "                        \"num_beams\": 8,\n",
    "                        \"repetition_penalty\": 1.2,\n",
    "                        \"length_penalty\": 0.8,\n",
    "                        \"max_new_tokens\": self.max_new_tokens\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"Diverse Beam Search\",\n",
    "                    \"params\": {\n",
    "                        \"do_sample\": False,\n",
    "                        \"num_beams\": 12,\n",
    "                        \"num_beam_groups\": 4,\n",
    "                        \"diversity_penalty\": 1.0,\n",
    "                        \"repetition_penalty\": 1.2,\n",
    "                        \"length_penalty\": 0.8,\n",
    "                        \"max_new_tokens\": self.max_new_tokens\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "\n",
    "        # Test with different prompt templates\n",
    "        for prompt_idx in range(min(2, len(self.PROMPT_TEMPLATES))):\n",
    "            print(f\"\\nPROMPT TEMPLATE {prompt_idx}:\")\n",
    "            print(\"-\"*80)\n",
    "\n",
    "            # Format prompt\n",
    "            prompt = self.PROMPT_TEMPLATES[prompt_idx].format(text=truncated_text)\n",
    "\n",
    "            # Print prompt (truncated if very long)\n",
    "            print(\"PROMPT:\")\n",
    "            if len(prompt) > 300:\n",
    "                print(prompt[:300] + \"...\")\n",
    "            else:\n",
    "                print(prompt)\n",
    "\n",
    "            # Tokenize input\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=self.max_length)\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "\n",
    "            # Test each parameter set\n",
    "            for param_set in parameter_sets:\n",
    "                print(f\"\\n{param_set['name']}:\")\n",
    "                print(\"-\"*40)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    outputs = self.model.generate(\n",
    "                        **inputs,\n",
    "                        **param_set['params'],\n",
    "                        early_stopping=True,\n",
    "                    )\n",
    "\n",
    "                # Decode output\n",
    "                generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "                # Print raw output\n",
    "                print(\"RAW OUTPUT:\")\n",
    "                print(generated_text)\n",
    "                print(\"-\"*40)\n",
    "\n",
    "                # Extract keyphrases\n",
    "                keyphrases = self.extract_keyphrases_from_generated_text(generated_text)\n",
    "                print(f\"EXTRACTED KEYPHRASES ({len(keyphrases)}):\")\n",
    "                for kp in keyphrases:\n",
    "                    print(f\"- {kp}\")\n",
    "\n",
    "                # Analyze output\n",
    "                print(\"\\nANALYSIS:\")\n",
    "                if len(keyphrases) < 5:\n",
    "                    print(\"- LOW KEYPHRASE COUNT: Model is not generating enough candidates\")\n",
    "\n",
    "                    if \",\" not in generated_text:\n",
    "                        print(\"- NO COMMAS: Model is not using comma separation as expected\")\n",
    "\n",
    "                    if any(example in generated_text.lower() for example in [\"example\", \"article:\"]):\n",
    "                        print(\"- EXAMPLE REPETITION: Model is repeating example text from prompt\")\n",
    "\n",
    "                    if len(generated_text) < 50:\n",
    "                        print(\"- SHORT OUTPUT: Model is generating very little text\")\n",
    "                elif len(keyphrases) > 30:\n",
    "                    print(\"- HIGH KEYPHRASE COUNT: Model is generating many candidates\")\n",
    "\n",
    "                    # Check for repetition or similar keyphrases\n",
    "                    similar_count = 0\n",
    "                    for i in range(len(keyphrases)):\n",
    "                        for j in range(i+1, len(keyphrases)):\n",
    "                            if keyphrases[i].lower() in keyphrases[j].lower() or keyphrases[j].lower() in keyphrases[i].lower():\n",
    "                                similar_count += 1\n",
    "\n",
    "                    if similar_count > len(keyphrases) * 0.3:\n",
    "                        print(f\"- HIGH REDUNDANCY: {similar_count} similar keyphrases detected\")\n",
    "                    else:\n",
    "                        print(\"- GOOD DIVERSITY: Low redundancy in generated keyphrases\")\n",
    "\n",
    "                if len(keyphrases) > 0:\n",
    "                    multi_word_count = sum(1 for kp in keyphrases if len(kp.split()) > 1)\n",
    "                    multi_word_percentage = multi_word_count / len(keyphrases)\n",
    "                    print(f\"- Multi-word phrases: {multi_word_percentage:.1%}\")\n",
    "\n",
    "                    avg_length = sum(len(kp.split()) for kp in keyphrases) / len(keyphrases)\n",
    "                    print(f\"- Average phrase length: {avg_length:.1f} words\")\n",
    "\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "\n",
    "    def optimize_generation_params(self, text: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Find the optimal generation parameters for a specific text.\n",
    "        Tests different parameter combinations and returns the best one.\n",
    "\n",
    "        Args:\n",
    "            text: Input text\n",
    "\n",
    "        Returns:\n",
    "            Dictionary of optimal parameters_\n",
    "        \"\"\"\n",
    "        print(\"Optimizing generation parameters...\")\n",
    "\n",
    "        # Test different parameter combinations\n",
    "        param_results = self.test_generation_params(text, debug_output=False)\n",
    "\n",
    "        # Evaluate each result\n",
    "        best_score = -1\n",
    "        best_params_key = None\n",
    "        best_keyphrases = []\n",
    "\n",
    "        for param_key, keyphrases in param_results.items():\n",
    "            # Skip if no keyphrases were generated\n",
    "            if not keyphrases:\n",
    "                continue\n",
    "\n",
    "            # Count keyphrases\n",
    "            num_keyphrases = len(keyphrases)\n",
    "\n",
    "            # Skip if too few keyphrases\n",
    "            if num_keyphrases < 5:\n",
    "                continue\n",
    "\n",
    "            # Count multi-word keyphrases\n",
    "            multi_word_count = sum(1 for kp in keyphrases if len(kp.split()) > 1)\n",
    "            multi_word_percentage = multi_word_count / num_keyphrases if num_keyphrases > 0 else 0\n",
    "\n",
    "            # Calculate average keyphrase length\n",
    "            avg_length = sum(len(kp.split()) for kp in keyphrases) / num_keyphrases\n",
    "\n",
    "            # Prefer 2-3 word keyphrases\n",
    "            length_score = 1.0 - abs(avg_length - 2.5) / 2.5\n",
    "\n",
    "            # Calculate final score\n",
    "            # We want:\n",
    "            # - A reasonable number of keyphrases (10-20 is good)\n",
    "            # - High percentage of multi-word phrases\n",
    "            # - Average length around 2-3 words\n",
    "\n",
    "            quantity_score = 0.0\n",
    "            if 10 <= num_keyphrases <= 20:\n",
    "                quantity_score = 1.0\n",
    "            elif 5 <= num_keyphrases < 10:\n",
    "                quantity_score = 0.7\n",
    "            elif 20 < num_keyphrases <= 30:\n",
    "                quantity_score = 0.8\n",
    "            elif num_keyphrases > 30:\n",
    "                quantity_score = 0.5\n",
    "\n",
    "            final_score = (\n",
    "                (quantity_score * 0.4) +\n",
    "                (multi_word_percentage * 0.4) +\n",
    "                (length_score * 0.2)\n",
    "            )\n",
    "\n",
    "            # Update best parameters if this score is higher\n",
    "            if final_score > best_score:\n",
    "                best_score = final_score\n",
    "                best_params_key = param_key\n",
    "                best_keyphrases = keyphrases\n",
    "\n",
    "        # Parse the best parameter key\n",
    "        if best_params_key:\n",
    "            print(f\"Best parameters: {best_params_key}\")\n",
    "            print(f\"Generated {len(best_keyphrases)} keyphrases with score {best_score:.2f}\")\n",
    "\n",
    "            # Parse the parameter key to extract actual parameters\n",
    "            if \"beam_search\" in best_params_key:\n",
    "                # Parse beam search parameters\n",
    "                params = {}\n",
    "                parts = best_params_key.split(\", \")\n",
    "                for part in parts:\n",
    "                    if \"beams=\" in part:\n",
    "                        params[\"num_beams\"] = int(part.split(\"=\")[1])\n",
    "                    elif \"len_pen=\" in part:\n",
    "                        params[\"length_penalty\"] = float(part.split(\"=\")[1])\n",
    "                    elif \"prompt=\" in part:\n",
    "                        params[\"prompt_template_idx\"] = int(part.split(\"=\")[1])\n",
    "\n",
    "                params[\"use_sampling\"] = False\n",
    "                return params\n",
    "\n",
    "            elif \"diverse_beam\" in best_params_key:\n",
    "                # Parse diverse beam search parameters\n",
    "                params = {}\n",
    "                parts = best_params_key.split(\", \")\n",
    "                for part in parts:\n",
    "                    if \"beams=\" in part:\n",
    "                        params[\"num_beams\"] = int(part.split(\"=\")[1])\n",
    "                    elif \"groups=\" in part:\n",
    "                        params[\"num_beam_groups\"] = int(part.split(\"=\")[1])\n",
    "                    elif \"div_pen=\" in part:\n",
    "                        params[\"diversity_penalty\"] = float(part.split(\"=\")[1])\n",
    "                    elif \"prompt=\" in part:\n",
    "                        params[\"prompt_template_idx\"] = int(part.split(\"=\")[1])\n",
    "\n",
    "                params[\"use_sampling\"] = False\n",
    "                return params\n",
    "\n",
    "            elif \"sampling\" in best_params_key:\n",
    "                # Parse sampling parameters\n",
    "                params = {}\n",
    "                parts = best_params_key.split(\", \")\n",
    "                for part in parts:\n",
    "                    if \"temp=\" in part:\n",
    "                        params[\"temperature\"] = float(part.split(\"=\")[1])\n",
    "                    elif \"top_p=\" in part:\n",
    "                        params[\"top_p\"] = float(part.split(\"=\")[1])\n",
    "                    elif \"top_k=\" in part:\n",
    "                        params[\"top_k\"] = int(part.split(\"=\")[1])\n",
    "                    elif \"prompt=\" in part:\n",
    "                        params[\"prompt_template_idx\"] = int(part.split(\"=\")[1])\n",
    "\n",
    "                params[\"use_sampling\"] = True\n",
    "                return params\n",
    "\n",
    "        # Default parameters if no good combination found\n",
    "        print(\"No optimal parameters found, using defaults\")\n",
    "        return {\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 0.92,\n",
    "            \"top_k\": 50,\n",
    "            \"length_penalty\": 0.8,\n",
    "            \"prompt_template_idx\": 0,\n",
    "            \"use_sampling\": True,\n",
    "            \"num_beams\": 5\n",
    "        }\n",
    "\n",
    "    def add_domain_specific_prompts(self):\n",
    "        \"\"\"\n",
    "        Add domain-specific prompt templates using the Pareto principle:\n",
    "        Focus on the most common domains and the most important content within each domain.\n",
    "        \"\"\"\n",
    "        self.DOMAIN_PROMPTS = {\n",
    "            \"technology\": \"\"\"Identify all core concepts, specific entities, and key topics (1-5 words each) from this technology article.\n",
    "\n",
    "    Focus on:\n",
    "    - Technologies and innovations (artificial intelligence, quantum computing, blockchain)\n",
    "    - Technical components (processor, algorithm, database, API)\n",
    "    - Companies and products (Microsoft Azure, iPhone, Tesla Model 3)\n",
    "    - Technical processes (machine learning, data mining, cloud computing)\n",
    "    - Industry terms (SaaS, cybersecurity, digital transformation)\n",
    "\n",
    "    Avoid:\n",
    "    - Generic phrases (new development, important announcement)\n",
    "    - Common reporting verbs (said, announced, launched)\n",
    "    - Vague descriptions (cutting-edge, next-generation)\n",
    "\n",
    "    Example: The tech company announced a breakthrough in quantum computing that could revolutionize cryptography and drug discovery. The new quantum processor uses 64 qubits and demonstrates quantum supremacy for specific computational tasks.\n",
    "    Keyphrases: quantum computing, cryptography, drug discovery, quantum processor, 64 qubits, quantum supremacy, computational tasks\n",
    "\n",
    "    Article: {text}\n",
    "    Keyphrases:\"\"\",\n",
    "\n",
    "            \"business\": \"\"\"Identify all core concepts, specific entities, and key topics (1-5 words each) from this business article.\n",
    "\n",
    "    Focus on:\n",
    "    - Companies and organizations (Goldman Sachs, Federal Reserve, NASDAQ)\n",
    "    - Financial terms (interest rates, market capitalization, quarterly earnings)\n",
    "    - Economic concepts (inflation, supply chain, consumer spending)\n",
    "    - Business processes (merger, acquisition, IPO, restructuring)\n",
    "    - Market segments and industries (retail sector, manufacturing, e-commerce)\n",
    "\n",
    "    Avoid:\n",
    "    - Generic business phrases (business strategy, company growth)\n",
    "    - Common reporting language (announced, reported, stated)\n",
    "    - Vague qualifiers (significant impact, major development)\n",
    "\n",
    "    Example: Global stock markets fell sharply as inflation concerns and rising interest rates affected investor confidence. Central banks in several countries have signaled further monetary tightening to combat persistent inflation pressures.\n",
    "    Keyphrases: global stock markets, inflation concerns, rising interest rates, investor confidence, central banks, monetary tightening, inflation pressures\n",
    "\n",
    "    Article: {text}\n",
    "    Keyphrases:\"\"\",\n",
    "\n",
    "            \"health\": \"\"\"Identify all core concepts, specific entities, and key topics (1-5 words each) from this health article.\n",
    "\n",
    "    Focus on:\n",
    "    - Medical conditions and diseases (diabetes, COVID-19, cardiovascular disease)\n",
    "    - Treatments and procedures (immunotherapy, surgical intervention, vaccine)\n",
    "    - Healthcare organizations (WHO, CDC, Mayo Clinic)\n",
    "    - Medical research (clinical trial, peer-reviewed study, patient outcomes)\n",
    "    - Health technologies (MRI, mRNA technology, telemedicine)\n",
    "\n",
    "    Avoid:\n",
    "    - Generic health phrases (health benefits, medical advice)\n",
    "    - Common reporting language (doctors say, studies show)\n",
    "    - Vague qualifiers (breakthrough treatment, miracle cure)\n",
    "\n",
    "    Example: A groundbreaking gene therapy treatment for sickle cell disease has shown remarkable results in a Phase 3 clinical trial, potentially offering a functional cure for the debilitating blood disorder that affects millions worldwide.\n",
    "    Keyphrases: gene therapy treatment, sickle cell disease, Phase 3 clinical trial, functional cure, blood disorder, genetic modification\n",
    "\n",
    "    Article: {text}\n",
    "    Keyphrases:\"\"\",\n",
    "\n",
    "            \"politics\": \"\"\"Identify all core concepts, specific entities, and key topics (1-5 words each) from this political article.\n",
    "\n",
    "    Focus on:\n",
    "    - Political figures and organizations (President Biden, Republican Party, Supreme Court)\n",
    "    - Legislation and policies (infrastructure bill, voting rights act, executive order)\n",
    "    - Political processes (Senate vote, filibuster, impeachment proceedings)\n",
    "    - Government institutions (Department of Justice, Congress, Federal agencies)\n",
    "    - Political issues (immigration reform, tax policy, healthcare legislation)\n",
    "\n",
    "    Avoid:\n",
    "    - Generic political phrases (political debate, government action)\n",
    "    - Common reporting language (lawmakers said, officials announced)\n",
    "    - Partisan descriptors (radical agenda, common-sense policy)\n",
    "\n",
    "    Example: The Senate passed a $1.2 trillion infrastructure bill with bipartisan support after months of negotiations, marking a significant legislative victory for the President's domestic agenda.\n",
    "    Keyphrases: Senate, infrastructure bill, bipartisan support, negotiations, legislative victory, domestic agenda, $1.2 trillion\n",
    "\n",
    "    Article: {text}\n",
    "    Keyphrases:\"\"\",\n",
    "\n",
    "            \"sports\": \"\"\"Identify all core concepts, specific entities, and key topics (1-5 words each) from this sports article.\n",
    "\n",
    "    Focus on:\n",
    "    - Teams and franchises (Los Angeles Lakers, Manchester United, New York Yankees)\n",
    "    - Athletes and personnel (LeBron James, Serena Williams, Coach Belichick)\n",
    "    - Competitions and events (Super Bowl, World Cup, Olympic Games)\n",
    "    - Sports terminology (triple-double, penalty kick, home run)\n",
    "    - Performance metrics (career record, championship title, scoring average)\n",
    "\n",
    "    Avoid:\n",
    "    - Generic sports phrases (great performance, tough competition)\n",
    "    - Common reporting language (analysts say, sources report)\n",
    "    - Vague qualifiers (amazing play, incredible athlete)\n",
    "\n",
    "    Example: The Kansas City Chiefs claimed their third Super Bowl title in five years with a thrilling 27-24 overtime victory against the San Francisco 49ers on Sunday night.\n",
    "    Keyphrases: Kansas City Chiefs, Super Bowl title, overtime victory, San Francisco 49ers, 27-24 score, championship game\n",
    "\n",
    "    Article: {text}\n",
    "    Keyphrases:\"\"\",\n",
    "\n",
    "            \"entertainment\": \"\"\"Identify all core concepts, specific entities, and key topics (1-5 words each) from this entertainment article.\n",
    "\n",
    "    Focus on:\n",
    "    - Celebrities and artists (Beyoncé, Steven Spielberg, Taylor Swift)\n",
    "    - Movies, shows, and music (Barbie movie, Stranger Things, Renaissance album)\n",
    "    - Entertainment companies (Netflix, Disney, Universal Music)\n",
    "    - Industry events (Academy Awards, Grammy ceremony, film festival)\n",
    "    - Entertainment formats and genres (streaming series, sci-fi thriller, pop album)\n",
    "\n",
    "    Avoid:\n",
    "    - Generic entertainment phrases (hit movie, popular show)\n",
    "    - Common reporting language (sources say, insiders reveal)\n",
    "    - Vague qualifiers (stunning performance, blockbuster hit)\n",
    "\n",
    "    Example: Beyoncé made Grammy Awards history on Sunday night, becoming the most decorated artist in the ceremony's 65-year history after winning her 32nd Grammy.\n",
    "    Keyphrases: Beyoncé, Grammy Awards history, most decorated artist, 32nd Grammy, music achievement, award ceremony\n",
    "\n",
    "    Article: {text}\n",
    "    Keyphrases:\"\"\",\n",
    "\n",
    "            \"science\": \"\"\"Identify all core concepts, specific entities, and key topics (1-5 words each) from this science article.\n",
    "\n",
    "    Focus on:\n",
    "    - Scientific disciplines (astrophysics, molecular biology, quantum mechanics)\n",
    "    - Research institutions (NASA, CERN, Max Planck Institute)\n",
    "    - Scientific phenomena (black hole, genetic mutation, chemical reaction)\n",
    "    - Research methods (genome sequencing, particle acceleration, spectroscopy)\n",
    "    - Scientific discoveries (exoplanet, new species, fundamental particle)\n",
    "\n",
    "    Avoid:\n",
    "    - Generic science phrases (scientific breakthrough, important discovery)\n",
    "    - Common reporting language (scientists say, researchers found)\n",
    "    - Vague qualifiers (groundbreaking research, revolutionary finding)\n",
    "\n",
    "    Example: Astronomers using the James Webb Space Telescope have discovered the most distant galaxy ever observed, offering unprecedented insights into the early universe.\n",
    "    Keyphrases: James Webb Space Telescope, distant galaxy, early universe, astronomical discovery, cosmic observation, galactic formation\n",
    "\n",
    "    Article: {text}\n",
    "    Keyphrases:\"\"\",\n",
    "\n",
    "            \"environment\": \"\"\"Identify all core concepts, specific entities, and key topics (1-5 words each) from this environment article.\n",
    "\n",
    "    Focus on:\n",
    "    - Environmental issues (climate change, deforestation, ocean acidification)\n",
    "    - Natural resources (renewable energy, freshwater sources, biodiversity)\n",
    "    - Environmental policies (Paris Agreement, carbon tax, emissions standards)\n",
    "    - Ecological systems (marine ecosystem, rainforest, wetlands)\n",
    "    - Environmental organizations (EPA, Greenpeace, IPCC)\n",
    "\n",
    "    Avoid:\n",
    "    - Generic environmental phrases (environmental impact, green initiative)\n",
    "    - Common reporting language (experts warn, studies indicate)\n",
    "    - Vague qualifiers (devastating consequences, urgent action needed)\n",
    "\n",
    "    Example: An intense heatwave gripping southern Europe has fueled devastating wildfires across Greece, Italy, Spain, and Turkey, forcing thousands of evacuations.\n",
    "    Keyphrases: heatwave, southern Europe, wildfires, evacuations, Greece, Italy, Spain, Turkey, climate emergency, extreme weather\n",
    "\n",
    "    Article: {text}\n",
    "    Keyphrases:\"\"\",\n",
    "\n",
    "            \"world\": \"\"\"Identify all core concepts, specific entities, and key topics (1-5 words each) from this world news article.\n",
    "\n",
    "    Focus on:\n",
    "    - Countries and regions (United States, European Union, Middle East)\n",
    "    - International organizations (United Nations, NATO, World Health Organization)\n",
    "    - Global issues (international conflict, trade agreement, refugee crisis)\n",
    "    - Diplomatic relations (peace talks, summit meeting, bilateral agreement)\n",
    "    - World leaders and officials (prime minister, president, ambassador)\n",
    "\n",
    "    Avoid:\n",
    "    - Generic international phrases (global impact, worldwide concern)\n",
    "    - Common reporting language (officials stated, sources confirmed)\n",
    "    - Vague qualifiers (major development, significant agreement)\n",
    "\n",
    "    Example: The United Nations Security Council passed a resolution calling for a ceasefire in the conflict zone after weeks of diplomatic negotiations between member states.\n",
    "    Keyphrases: United Nations Security Council, resolution, ceasefire, conflict zone, diplomatic negotiations, member states\n",
    "\n",
    "    Article: {text}\n",
    "    Keyphrases:\"\"\"\n",
    "        }\n",
    "\n",
    "\n",
    "    def evaluate_domain_detection(self, articles_with_domains: List[Tuple[str, str]]) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Evaluate domain detection performance on a set of articles with known domains.\n",
    "\n",
    "        Args:\n",
    "            articles_with_domains: List of (article_text, true_domain) tuples\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with performance metrics\n",
    "        \"\"\"\n",
    "        results = {\n",
    "            \"total\": len(articles_with_domains),\n",
    "            \"correct\": 0,\n",
    "            \"zsl_used\": 0,\n",
    "            \"keyword_used\": 0,\n",
    "            \"domains\": {}\n",
    "        }\n",
    "\n",
    "        for text, true_domain in articles_with_domains:\n",
    "            # Save original stdout to suppress verbose output\n",
    "            original_stdout = sys.stdout\n",
    "            sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "            # Track which method was used\n",
    "            zsl_used = True\n",
    "\n",
    "            try:\n",
    "                # Try to detect domain using zero-shot classification\n",
    "                detected_domain = self.detect_domain(text)\n",
    "\n",
    "                # Check if we fell back to keyword-based detection\n",
    "                if not hasattr(self, 'zero_shot_classifier') or detected_domain != self._keyword_based_domain_detection(text):\n",
    "                    zsl_used = False\n",
    "            except Exception:\n",
    "                # If any error occurs, use keyword-based detection\n",
    "                detected_domain = self._keyword_based_domain_detection(text)\n",
    "                zsl_used = False\n",
    "\n",
    "            # Restore stdout\n",
    "            sys.stdout.close()\n",
    "            sys.stdout = original_stdout\n",
    "\n",
    "            # Update results\n",
    "            if detected_domain == true_domain:\n",
    "                results[\"correct\"] += 1\n",
    "\n",
    "            if zsl_used:\n",
    "                results[\"zsl_used\"] += 1\n",
    "            else:\n",
    "                results[\"keyword_used\"] += 1\n",
    "\n",
    "            # Track per-domain performance\n",
    "            if true_domain not in results[\"domains\"]:\n",
    "                results[\"domains\"][true_domain] = {\"total\": 0, \"correct\": 0}\n",
    "\n",
    "            results[\"domains\"][true_domain][\"total\"] += 1\n",
    "            if detected_domain == true_domain:\n",
    "                results[\"domains\"][true_domain][\"correct\"] += 1\n",
    "\n",
    "        # Calculate accuracy\n",
    "        results[\"accuracy\"] = results[\"correct\"] / results[\"total\"] if results[\"total\"] > 0 else 0\n",
    "        results[\"zsl_percentage\"] = results[\"zsl_used\"] / results[\"total\"] if results[\"total\"] > 0 else 0\n",
    "\n",
    "        # Calculate per-domain accuracy\n",
    "        for domain in results[\"domains\"]:\n",
    "            domain_total = results[\"domains\"][domain][\"total\"]\n",
    "            domain_correct = results[\"domains\"][domain][\"correct\"]\n",
    "            results[\"domains\"][domain][\"accuracy\"] = domain_correct / domain_total if domain_total > 0 else 0\n",
    "\n",
    "        return results\n",
    "\n",
    "    def train_domain_classifier(self, articles_with_domains: List[Tuple[str, str]]) -> None:\n",
    "        \"\"\"\n",
    "        Train a fallback domain classifier using labeled data.\n",
    "\n",
    "        Args:\n",
    "            articles_with_domains: List of (article_text, domain) tuples\n",
    "        \"\"\"\n",
    "        try:\n",
    "            from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "            from sklearn.svm import LinearSVC\n",
    "            from sklearn.pipeline import Pipeline\n",
    "            from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "            from sklearn.metrics import classification_report, confusion_matrix\n",
    "            import numpy as np\n",
    "            import pandas as pd\n",
    "\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(\"TRAINING FALLBACK DOMAIN CLASSIFIER\")\n",
    "            print(\"=\"*80)\n",
    "\n",
    "            # Extract texts and labels\n",
    "            texts = [text for text, _ in articles_with_domains]\n",
    "            domains = [domain for _, domain in articles_with_domains]\n",
    "\n",
    "            # Print domain distribution\n",
    "            domain_counts = {}\n",
    "            for domain in domains:\n",
    "                domain_counts[domain] = domain_counts.get(domain, 0) + 1\n",
    "\n",
    "            print(\"Domain distribution:\")\n",
    "            for domain, count in sorted(domain_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "                print(f\"- {domain}: {count} articles ({count/len(domains):.1%})\")\n",
    "\n",
    "            # Split into train and test sets\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                texts, domains, test_size=0.2, random_state=42, stratify=domains\n",
    "            )\n",
    "\n",
    "            print(f\"\\nTraining on {len(X_train)} articles, testing on {len(X_test)} articles\")\n",
    "\n",
    "            # Create and train the classifier with hyperparameter tuning\n",
    "            pipeline = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(max_features=5000, ngram_range=(1, 2))),\n",
    "                ('clf', LinearSVC(class_weight='balanced'))\n",
    "            ])\n",
    "\n",
    "            # Define parameter grid for grid search\n",
    "            param_grid = {\n",
    "                'tfidf__max_features': [3000, 5000, 7000],\n",
    "                'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "                'clf__C': [0.1, 1.0, 10.0]\n",
    "            }\n",
    "\n",
    "            # Perform grid search with cross-validation\n",
    "            print(\"Performing grid search for hyperparameter tuning...\")\n",
    "            grid_search = GridSearchCV(\n",
    "                pipeline, param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1\n",
    "            )\n",
    "            grid_search.fit(X_train, y_train)\n",
    "\n",
    "            # Get best parameters\n",
    "            best_params = grid_search.best_params_\n",
    "            print(f\"\\nBest parameters: {best_params}\")\n",
    "\n",
    "            # Create classifier with best parameters\n",
    "            self.domain_classifier = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(\n",
    "                    max_features=best_params['tfidf__max_features'],\n",
    "                    ngram_range=best_params['tfidf__ngram_range']\n",
    "                )),\n",
    "                ('clf', LinearSVC(C=best_params['clf__C'], class_weight='balanced'))\n",
    "            ])\n",
    "\n",
    "            # Train on full training set\n",
    "            self.domain_classifier.fit(X_train, y_train)\n",
    "\n",
    "            # Evaluate on test set\n",
    "            y_pred = self.domain_classifier.predict(X_test)\n",
    "            accuracy = np.mean(y_pred == y_test)\n",
    "\n",
    "            print(f\"\\nDomain classifier trained with accuracy: {accuracy:.4f}\")\n",
    "            print(\"\\nClassification Report:\")\n",
    "            print(classification_report(y_test, y_pred))\n",
    "\n",
    "            # Print confusion matrix\n",
    "            print(\"\\nConfusion Matrix:\")\n",
    "            cm = confusion_matrix(y_test, y_pred)\n",
    "            domains_unique = sorted(set(domains))\n",
    "            cm_df = pd.DataFrame(cm, index=domains_unique, columns=domains_unique)\n",
    "            print(cm_df)\n",
    "\n",
    "            # Set flag to indicate classifier is available\n",
    "            self.has_domain_classifier = True\n",
    "\n",
    "            print(\"\\nDomain classifier training complete. It will now be used as a fallback when zero-shot classification has low confidence.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error training domain classifier: {str(e)}\")\n",
    "            print(\"Domain classifier will not be available\")\n",
    "            self.has_domain_classifier = False\n",
    "\n",
    "    def detect_domain_with_bart(self, text: str) -> Tuple[str, float]:\n",
    "        \"\"\"\n",
    "        Use BART-large-mnli for improved zero-shot topic classification.\n",
    "        This model performs better than mDeBERTa for topic detection.\n",
    "\n",
    "        Args:\n",
    "            text: Input text\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (detected_domain, confidence_score)\n",
    "        \"\"\"\n",
    "        # Initialize the classifier if not already done\n",
    "        if not hasattr(self, 'bart_classifier'):\n",
    "            try:\n",
    "                from transformers import pipeline\n",
    "                # facebook/bart-large-mnli has better zero-shot performance for topic classification\n",
    "                self.bart_classifier = pipeline(\n",
    "                    \"zero-shot-classification\",\n",
    "                    model=\"facebook/bart-large-mnli\",\n",
    "                    device=0 if torch.cuda.is_available() else -1\n",
    "                )\n",
    "                print(\"Initialized BART-large for improved topic classification\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error initializing BART-large: {str(e)}\")\n",
    "                return None, 0.0\n",
    "\n",
    "        # Define our priority domains with descriptive labels\n",
    "        candidate_domains = [\n",
    "            # Priority domains (our 7 focus areas)\n",
    "            \"artificial intelligence\", \"cybersecurity\", \"automotive\",\n",
    "            \"food\", \"environment\", \"real estate\", \"entertainment\",\n",
    "\n",
    "            # Other domains\n",
    "            \"technology\", \"business\", \"health\", \"politics\", \"sports\",\n",
    "            \"science\", \"world\", \"education\", \"travel\", \"space\",\n",
    "            \"agriculture\", \"mental health\"\n",
    "        ]\n",
    "\n",
    "        try:\n",
    "            # Truncate text if very long (BART has 1024 token limit)\n",
    "            truncated_text = text[:2000]  # Characters, not tokens, but a safe limit\n",
    "\n",
    "            # Use a clear hypothesis template\n",
    "            result = self.bart_classifier(\n",
    "                truncated_text,\n",
    "                candidate_domains,\n",
    "                hypothesis_template=\"This text is about {}.\"\n",
    "            )\n",
    "\n",
    "            # Return top domain and score\n",
    "            return result['labels'][0], result['scores'][0]\n",
    "        except Exception as e:\n",
    "            print(f\"Error in BART domain detection: {str(e)}\")\n",
    "            return None, 0.0\n",
    "\n",
    "    def detect_domain(self, text: str, original_domain: str = None) -> str:\n",
    "        \"\"\"\n",
    "        Enhanced domain detection using a multi-stage approach:\n",
    "        1. Zero-shot classification with BART-large-mnli (primary method)\n",
    "        2. Zero-shot classification with mDeBERTa (secondary method)\n",
    "        3. Trained domain classifier (if available)\n",
    "        4. Rule-based fallback classifier for problematic domains (if available)\n",
    "        5. Domain confidence boosting using high-precision keywords\n",
    "        6. Keyword-based detection as final fallback\n",
    "\n",
    "        Args:\n",
    "            text: Input text\n",
    "\n",
    "        Returns:\n",
    "            Detected domain\n",
    "        \"\"\"\n",
    "        # If original domain is provided, use it directly\n",
    "        if original_domain:\n",
    "            print(f\"Using provided original domain: {original_domain}\")\n",
    "            # Print a clear message to indicate we're using the original domain\n",
    "            print(\"IMPORTANT: Using original domain from input data instead of detecting domain\")\n",
    "            return original_domain\n",
    "\n",
    "        # Try BART-large-mnli first (better for topic classification)\n",
    "        try:\n",
    "            domain, confidence = self.detect_domain_with_bart(text)\n",
    "            if domain and confidence >= 0.5:  # Higher threshold for BART as it tends to be more confident\n",
    "                print(f\"Domain detected using BART: {domain} (confidence: {confidence:.4f})\")\n",
    "\n",
    "                # Map to broader category if needed\n",
    "                # Define domain mapping here to avoid reference errors\n",
    "                domain_mapping = {\n",
    "                    # Only map very specific subdomains to slightly broader domains\n",
    "                    \"machine learning\": \"artificial intelligence\",\n",
    "                    \"deep learning\": \"artificial intelligence\",\n",
    "                    \"natural language processing\": \"artificial intelligence\",\n",
    "                    \"computer vision\": \"artificial intelligence\",\n",
    "\n",
    "                    \"network security\": \"cybersecurity\",\n",
    "                    \"information security\": \"cybersecurity\",\n",
    "                    \"data security\": \"cybersecurity\",\n",
    "\n",
    "                    \"electric vehicles\": \"automotive\",\n",
    "                    \"self-driving cars\": \"automotive\",\n",
    "                    \"autonomous vehicles\": \"automotive\",\n",
    "\n",
    "                    \"nutrition\": \"food\",\n",
    "                    \"cooking\": \"food\",\n",
    "                    \"cuisine\": \"food\",\n",
    "\n",
    "                    \"climate\": \"environment\",\n",
    "                    \"sustainability\": \"environment\",\n",
    "                    \"renewable energy\": \"environment\",\n",
    "\n",
    "                    \"housing\": \"real estate\",\n",
    "                    \"property\": \"real estate\",\n",
    "                    \"mortgage\": \"real estate\",\n",
    "\n",
    "                    \"film\": \"entertainment\",\n",
    "                    \"movie\": \"entertainment\",\n",
    "                    \"television\": \"entertainment\",\n",
    "                    \"streaming\": \"entertainment\",\n",
    "                    \"gaming\": \"entertainment\"\n",
    "                }\n",
    "\n",
    "                if domain in domain_mapping:\n",
    "                    mapped_domain = domain_mapping[domain]\n",
    "                    print(f\"Mapped specific domain '{domain}' to broader category '{mapped_domain}'\")\n",
    "                    return mapped_domain\n",
    "\n",
    "                return domain\n",
    "        except Exception as e:\n",
    "            print(f\"Error using BART for domain detection: {str(e)}\")\n",
    "\n",
    "        # Initialize mDeBERTa zero-shot classifier as backup\n",
    "        if not hasattr(self, 'zero_shot_classifier'):\n",
    "            try:\n",
    "                from transformers import pipeline\n",
    "                print(\"Initializing mDeBERTa zero-shot domain classifier (backup method)...\")\n",
    "                # Use GPU if available\n",
    "                device = 0 if self.device == \"cuda\" else -1\n",
    "                self.zero_shot_classifier = pipeline(\n",
    "                    \"zero-shot-classification\",\n",
    "                    model=\"MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\",\n",
    "                    device=device\n",
    "                )\n",
    "                print(\"mDeBERTa zero-shot classifier initialized successfully\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error initializing mDeBERTa zero-shot classifier: {str(e)}\")\n",
    "                print(\"Falling back to alternative domain detection methods\")\n",
    "\n",
    "                # Try trained classifier if available\n",
    "                if hasattr(self, 'has_domain_classifier') and self.has_domain_classifier:\n",
    "                    try:\n",
    "                        domain = self.domain_classifier.predict([text])[0]\n",
    "                        print(f\"Domain detected using trained classifier: {domain}\")\n",
    "                        return domain\n",
    "                    except Exception:\n",
    "                        pass\n",
    "\n",
    "                # Try rule-based fallback classifier if available\n",
    "                if 'DomainFallbackClassifier' in globals():\n",
    "                    try:\n",
    "                        if not hasattr(self, 'domain_fallback_classifier'):\n",
    "                            self.domain_fallback_classifier = globals()['DomainFallbackClassifier']()\n",
    "                            print(\"Initialized domain fallback classifier from global namespace\")\n",
    "\n",
    "                        domain, confidence = self.domain_fallback_classifier.detect_domain(text)\n",
    "                        if domain is not None and confidence >= 0.3:\n",
    "                            print(f\"Domain detected using fallback classifier: {domain} (confidence: {confidence:.4f})\")\n",
    "                            return domain\n",
    "                        else:\n",
    "                            print(f\"Fallback classifier result: {domain} (confidence: {confidence:.4f} - too low)\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error using fallback classifier: {str(e)}\")\n",
    "\n",
    "                # Fall back to keyword-based method\n",
    "                return self._keyword_based_domain_detection(text)\n",
    "\n",
    "        # Define candidate domains\n",
    "        # Prioritize our 7 focus domains by listing them first\n",
    "        candidate_domains = [\n",
    "            # Priority domains (our 7 focus areas)\n",
    "            \"artificial intelligence\", \"cybersecurity\", \"automotive\",\n",
    "            \"food\", \"environment\", \"real estate\", \"entertainment\",\n",
    "\n",
    "            # Other domains\n",
    "            \"technology\", \"business\", \"health\", \"politics\", \"sports\",\n",
    "            \"science\", \"world\", \"education\", \"travel\", \"space\",\n",
    "            \"agriculture\", \"mental health\"\n",
    "        ]\n",
    "\n",
    "        # Map specific domains to broader categories if needed\n",
    "        # IMPORTANT: We're now preserving more specific domains rather than mapping everything to broad categories\n",
    "        # This allows us to apply more precise domain-specific thresholds\n",
    "        domain_mapping = {\n",
    "            # Only map very specific subdomains to slightly broader domains\n",
    "            \"machine learning\": \"artificial intelligence\",\n",
    "            \"deep learning\": \"artificial intelligence\",\n",
    "            \"natural language processing\": \"artificial intelligence\",\n",
    "            \"computer vision\": \"artificial intelligence\",\n",
    "\n",
    "            \"network security\": \"cybersecurity\",\n",
    "            \"information security\": \"cybersecurity\",\n",
    "            \"data security\": \"cybersecurity\",\n",
    "\n",
    "            \"electric vehicles\": \"automotive\",\n",
    "            \"self-driving cars\": \"automotive\",\n",
    "            \"autonomous vehicles\": \"automotive\",\n",
    "\n",
    "            \"nutrition\": \"food\",\n",
    "            \"cooking\": \"food\",\n",
    "            \"cuisine\": \"food\",\n",
    "\n",
    "            \"climate\": \"environment\",\n",
    "            \"sustainability\": \"environment\",\n",
    "            \"renewable energy\": \"environment\",\n",
    "\n",
    "            \"housing\": \"real estate\",\n",
    "            \"property\": \"real estate\",\n",
    "            \"mortgage\": \"real estate\",\n",
    "\n",
    "            \"film\": \"entertainment\",\n",
    "            \"movie\": \"entertainment\",\n",
    "            \"television\": \"entertainment\",\n",
    "            \"streaming\": \"entertainment\",\n",
    "            \"gaming\": \"entertainment\"\n",
    "        }\n",
    "\n",
    "        # If BART failed or wasn't confident enough, try mDeBERTa as backup\n",
    "        try:\n",
    "            # Truncate text if very long for better performance\n",
    "            truncated_text = text[:1500]\n",
    "\n",
    "            # Domain-specific confidence thresholds based on empirical testing\n",
    "            # Lowered thresholds for problematic domains\n",
    "            domain_thresholds = {\n",
    "                # Priority domains with refined thresholds\n",
    "                'artificial intelligence': 0.35,  # Specific AI domain (was under technology)\n",
    "                'cybersecurity': 0.32,           # Adjusted based on testing\n",
    "                'automotive': 0.30,              # Adjusted based on testing\n",
    "                'food': 0.28,                    # Adjusted based on testing\n",
    "                'environment': 0.28,             # Adjusted based on testing\n",
    "                'real estate': 0.32,             # Adjusted based on testing\n",
    "                'entertainment': 0.35,           # Adjusted based on testing\n",
    "\n",
    "                # Other domains with standard thresholds\n",
    "                'politics': 0.5,                # Highest confidence (0.7903)\n",
    "                'technology': 0.45,             # High confidence (0.5321)\n",
    "                'business': 0.45,               # High confidence (0.5107)\n",
    "                'sports': 0.45,                 # High confidence (0.4982)\n",
    "                'science': 0.4,                 # Medium-high confidence (0.4387)\n",
    "                'health': 0.35,                 # Medium-high confidence (0.4321)\n",
    "                'world': 0.4,                   # Medium-high confidence (0.4298)\n",
    "                'education': 0.4,               # Medium-high confidence (0.4176)\n",
    "                'space': 0.3,                   # Lower confidence (0.3119)\n",
    "                'travel': 0.25,                 # Lowest confidence (0.2419)\n",
    "                'agriculture': 0.25,            # Added with lower threshold\n",
    "                'mental health': 0.25           # Added with lower threshold\n",
    "            }\n",
    "\n",
    "            # Default threshold for domains not in the list\n",
    "            default_threshold = 0.4\n",
    "\n",
    "            # Use mDeBERTa with a single optimized template\n",
    "            result = self.zero_shot_classifier(\n",
    "                truncated_text,\n",
    "                candidate_domains,\n",
    "                hypothesis_template=\"This text is about {}.\"\n",
    "            )\n",
    "\n",
    "            top_domain = result['labels'][0]\n",
    "            top_score = result['scores'][0]\n",
    "\n",
    "            # Print detailed results\n",
    "            print(\"\\nZero-Shot Domain Detection Results:\")\n",
    "            print(\"-\" * 40)\n",
    "            print(f\"Top domain: {top_domain} (Score: {top_score:.4f})\")\n",
    "\n",
    "            # Show top 5 domains with scores\n",
    "            for i in range(min(5, len(result['labels']))):\n",
    "                domain = result['labels'][i]\n",
    "                score = result['scores'][i]\n",
    "                print(f\"  {i+1}. {domain}: {score:.4f}\")\n",
    "            print(\"-\" * 40)\n",
    "\n",
    "            # Check domain-specific threshold\n",
    "            threshold = domain_thresholds.get(top_domain, default_threshold)\n",
    "            print(f\"Threshold for '{top_domain}': {threshold:.2f}\")\n",
    "\n",
    "            if top_score >= threshold:\n",
    "                # Map to broader category if needed\n",
    "                if top_domain in domain_mapping:\n",
    "                    mapped_domain = domain_mapping[top_domain]\n",
    "                    print(f\"Mapped specific domain '{top_domain}' to broader category '{mapped_domain}'\")\n",
    "                    return mapped_domain\n",
    "\n",
    "                return top_domain\n",
    "\n",
    "            # Check top 3 domains against their thresholds\n",
    "            for i in range(1, min(3, len(result['labels']))):\n",
    "                domain = result['labels'][i]\n",
    "                score = result['scores'][i]\n",
    "                domain_threshold = domain_thresholds.get(domain, default_threshold)\n",
    "\n",
    "                if score >= domain_threshold:\n",
    "                    print(f\"Using secondary domain '{domain}' (Score: {score:.4f}, Threshold: {domain_threshold:.2f})\")\n",
    "\n",
    "                    # Map to broader category if needed\n",
    "                    if domain in domain_mapping:\n",
    "                        mapped_domain = domain_mapping[domain]\n",
    "                        print(f\"Mapped specific domain '{domain}' to broader category '{mapped_domain}'\")\n",
    "                        return mapped_domain\n",
    "\n",
    "                    return domain\n",
    "\n",
    "            print(f\"Low confidence ({top_score:.4f}) for domain '{top_domain}'.\")\n",
    "\n",
    "            # Try trained classifier if available\n",
    "            if hasattr(self, 'has_domain_classifier') and self.has_domain_classifier:\n",
    "                try:\n",
    "                    domain = self.domain_classifier.predict([text])[0]\n",
    "                    print(f\"Domain detected using trained classifier: {domain}\")\n",
    "                    return domain\n",
    "                except Exception as e:\n",
    "                    print(f\"Error using trained classifier: {str(e)}\")\n",
    "\n",
    "            # If no trained classifier or it failed, use a hybrid approach\n",
    "            # Check if any domain has a score above 0.25\n",
    "            if top_score >= 0.25:\n",
    "                print(f\"Score is above 0.25, using detected domain despite being below threshold\")\n",
    "\n",
    "                # Map to broader category if needed\n",
    "                if top_domain in domain_mapping:\n",
    "                    mapped_domain = domain_mapping[top_domain]\n",
    "                    print(f\"Mapped specific domain '{top_domain}' to broader category '{mapped_domain}'\")\n",
    "                    return mapped_domain\n",
    "\n",
    "                return top_domain\n",
    "\n",
    "            # Try rule-based fallback classifier for problematic domains\n",
    "            if 'DomainFallbackClassifier' in globals():\n",
    "                try:\n",
    "                    if not hasattr(self, 'domain_fallback_classifier'):\n",
    "                        self.domain_fallback_classifier = globals()['DomainFallbackClassifier']()\n",
    "                        print(\"Initialized domain fallback classifier from global namespace\")\n",
    "\n",
    "                    domain, confidence = self.domain_fallback_classifier.detect_domain(text)\n",
    "                    if domain is not None and confidence >= 0.3:\n",
    "                        print(f\"Domain detected using fallback classifier: {domain} (confidence: {confidence:.4f})\")\n",
    "                        return domain\n",
    "                    else:\n",
    "                        print(f\"Fallback classifier result: {domain} (confidence: {confidence:.4f} - too low)\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error using fallback classifier: {str(e)}\")\n",
    "\n",
    "            # Domain confidence booster - check for high-precision domain keywords\n",
    "            print(\"Applying domain confidence booster before keyword fallback\")\n",
    "            boosted_domain = self._apply_domain_confidence_booster(text, top_domain, top_score)\n",
    "            if boosted_domain:\n",
    "                print(f\"Domain boosted to '{boosted_domain}' based on high-precision keywords\")\n",
    "\n",
    "                # Map to broader category if needed\n",
    "                if boosted_domain in domain_mapping:\n",
    "                    mapped_domain = domain_mapping[boosted_domain]\n",
    "                    print(f\"Mapped boosted domain '{boosted_domain}' to broader category '{mapped_domain}'\")\n",
    "                    return mapped_domain\n",
    "\n",
    "                return boosted_domain\n",
    "\n",
    "            # Fall back to keyword-based method as last resort\n",
    "            print(\"Using keyword-based domain detection as fallback\")\n",
    "            keyword_domain = self._keyword_based_domain_detection(text)\n",
    "\n",
    "            # If keyword domain is different from ZSL domain, compare scores\n",
    "            if keyword_domain != top_domain:\n",
    "                # Check if the ZSL score is at least 0.2\n",
    "                if top_score >= 0.2:\n",
    "                    print(f\"Using ZSL domain '{top_domain}' (score: {top_score:.4f}) instead of keyword domain '{keyword_domain}'\")\n",
    "\n",
    "                    # Map to broader category if needed\n",
    "                    if top_domain in domain_mapping:\n",
    "                        mapped_domain = domain_mapping[top_domain]\n",
    "                        print(f\"Mapped specific domain '{top_domain}' to broader category '{mapped_domain}'\")\n",
    "                        return mapped_domain\n",
    "\n",
    "                    return top_domain\n",
    "\n",
    "            return keyword_domain\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in mDeBERTa domain detection: {str(e)}\")\n",
    "\n",
    "            # Try trained classifier if available\n",
    "            if hasattr(self, 'has_domain_classifier') and self.has_domain_classifier:\n",
    "                try:\n",
    "                    domain = self.domain_classifier.predict([text])[0]\n",
    "                    print(f\"Domain detected using trained classifier: {domain}\")\n",
    "                    return domain\n",
    "                except Exception as e:\n",
    "                    print(f\"Error using trained classifier: {str(e)}\")\n",
    "\n",
    "            # Try rule-based fallback classifier if available\n",
    "            if 'DomainFallbackClassifier' in globals():\n",
    "                try:\n",
    "                    if not hasattr(self, 'domain_fallback_classifier'):\n",
    "                        self.domain_fallback_classifier = globals()['DomainFallbackClassifier']()\n",
    "                        print(\"Initialized domain fallback classifier from global namespace\")\n",
    "\n",
    "                    domain, confidence = self.domain_fallback_classifier.detect_domain(text)\n",
    "                    if domain is not None and confidence >= 0.3:\n",
    "                        print(f\"Domain detected using fallback classifier: {domain} (confidence: {confidence:.4f})\")\n",
    "                        return domain\n",
    "                    else:\n",
    "                        print(f\"Fallback classifier result: {domain} (confidence: {confidence:.4f} - too low)\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error using fallback classifier: {str(e)}\")\n",
    "\n",
    "            # Fall back to keyword-based method\n",
    "            print(\"Falling back to keyword-based domain detection\")\n",
    "            return self._keyword_based_domain_detection(text)\n",
    "\n",
    "    def train_domain_classifier(self, articles_with_domains: List[Tuple[str, str]]) -> None:\n",
    "        \"\"\"\n",
    "        Train a fallback domain classifier using labeled data.\n",
    "\n",
    "        Args:\n",
    "            articles_with_domains: List of (article_text, domain) tuples\n",
    "        \"\"\"\n",
    "        try:\n",
    "            from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "            from sklearn.svm import LinearSVC\n",
    "            from sklearn.pipeline import Pipeline\n",
    "            from sklearn.model_selection import train_test_split\n",
    "\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(\"TRAINING FALLBACK DOMAIN CLASSIFIER\")\n",
    "            print(\"=\"*80)\n",
    "\n",
    "            # Extract texts and labels\n",
    "            texts = [text for text, _ in articles_with_domains]\n",
    "            domains = [domain for _, domain in articles_with_domains]\n",
    "\n",
    "            # Print domain distribution\n",
    "            domain_counts = {}\n",
    "            for domain in domains:\n",
    "                domain_counts[domain] = domain_counts.get(domain, 0) + 1\n",
    "\n",
    "            print(\"Domain distribution:\")\n",
    "            for domain, count in sorted(domain_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "                print(f\"- {domain}: {count} articles ({count/len(domains):.1%})\")\n",
    "\n",
    "            # Split into train and test sets\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                texts, domains, test_size=0.2, random_state=42, stratify=domains\n",
    "            )\n",
    "\n",
    "            print(f\"\\nTraining on {len(X_train)} articles, testing on {len(X_test)} articles\")\n",
    "\n",
    "            # Create and train the classifier\n",
    "            self.domain_classifier = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(max_features=5000, ngram_range=(1, 2))),\n",
    "                ('clf', LinearSVC(C=1.0, class_weight='balanced'))\n",
    "            ])\n",
    "\n",
    "            self.domain_classifier.fit(X_train, y_train)\n",
    "\n",
    "            # Evaluate on test set\n",
    "            accuracy = self.domain_classifier.score(X_test, y_test)\n",
    "            print(f\"Domain classifier trained with accuracy: {accuracy:.4f}\")\n",
    "\n",
    "            # Set flag to indicate classifier is available\n",
    "            self.has_domain_classifier = True\n",
    "\n",
    "            print(\"\\nDomain classifier training complete. It will now be used as a fallback when zero-shot classification has low confidence.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error training domain classifier: {str(e)}\")\n",
    "            print(\"Domain classifier will not be available\")\n",
    "            self.has_domain_classifier = False\n",
    "\n",
    "    def _apply_domain_confidence_booster(self, text: str, candidate_domain: str, candidate_score: float) -> str:\n",
    "        \"\"\"\n",
    "        Apply domain confidence booster using high-precision keywords.\n",
    "        This method checks for the presence of domain-specific high-precision keywords\n",
    "        that strongly indicate a particular domain.\n",
    "\n",
    "        Args:\n",
    "            text: Input text\n",
    "            candidate_domain: The domain detected by the primary method\n",
    "            candidate_score: The confidence score of the candidate domain\n",
    "\n",
    "        Returns:\n",
    "            Boosted domain if high-precision keywords are found, otherwise None\n",
    "        \"\"\"\n",
    "        text_lower = text.lower()\n",
    "\n",
    "        # High-precision keywords that strongly indicate specific domains\n",
    "        # These are carefully selected terms that have very high precision (low false positives)\n",
    "        high_precision_keywords = {\n",
    "            \"artificial intelligence\": [\n",
    "                \"neural network\", \"deep learning\", \"machine learning\", \"natural language processing\",\n",
    "                \"computer vision\", \"reinforcement learning\", \"transformer model\", \"large language model\",\n",
    "                \"generative ai\", \"gpt\", \"bert\", \"llm\", \"diffusion model\", \"stable diffusion\"\n",
    "            ],\n",
    "            \"cybersecurity\": [\n",
    "                \"ransomware\", \"zero-day\", \"vulnerability\", \"data breach\", \"phishing\", \"malware\",\n",
    "                \"ddos attack\", \"firewall\", \"encryption\", \"cyber attack\", \"penetration testing\",\n",
    "                \"security breach\", \"threat actor\", \"infosec\", \"cybersecurity\"\n",
    "            ],\n",
    "            \"automotive\": [\n",
    "                \"electric vehicle\", \"autonomous driving\", \"self-driving\", \"ev charging\",\n",
    "                \"battery electric\", \"hybrid vehicle\", \"automotive industry\", \"automaker\",\n",
    "                \"vehicle emissions\", \"car manufacturer\", \"automotive supplier\", \"powertrain\"\n",
    "            ],\n",
    "            \"food\": [\n",
    "                \"culinary\", \"cuisine\", \"ingredient\", \"recipe\", \"restaurant\", \"chef\", \"food safety\",\n",
    "                \"nutrition\", \"dietary\", \"food processing\", \"food production\", \"food industry\",\n",
    "                \"food supply\", \"food security\", \"food system\", \"food waste\", \"sustainable food\"\n",
    "            ],\n",
    "            \"environment\": [\n",
    "                \"climate change\", \"global warming\", \"carbon emissions\", \"greenhouse gas\",\n",
    "                \"renewable energy\", \"sustainability\", \"biodiversity\", \"conservation\",\n",
    "                \"environmental impact\", \"pollution\", \"ecosystem\", \"carbon footprint\",\n",
    "                \"environmental protection\", \"climate crisis\", \"climate action\"\n",
    "            ],\n",
    "            \"real estate\": [\n",
    "                \"property market\", \"housing market\", \"real estate market\", \"commercial property\",\n",
    "                \"residential property\", \"mortgage rate\", \"home buyer\", \"property value\",\n",
    "                \"real estate investment\", \"property development\", \"real estate agent\",\n",
    "                \"housing affordability\", \"rental market\", \"property price\"\n",
    "            ],\n",
    "            \"entertainment\": [\n",
    "                \"box office\", \"streaming service\", \"film industry\", \"movie studio\", \"television series\",\n",
    "                \"entertainment industry\", \"media company\", \"production studio\", \"streaming platform\",\n",
    "                \"theatrical release\", \"content creator\", \"media streaming\", \"subscription service\"\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        # Check if the candidate domain is close but below threshold\n",
    "        if candidate_score >= 0.2:\n",
    "            # Check for high-precision keywords for the candidate domain\n",
    "            if candidate_domain in high_precision_keywords:\n",
    "                for keyword in high_precision_keywords[candidate_domain]:\n",
    "                    if keyword in text_lower:\n",
    "                        # Found a high-precision keyword that confirms the candidate domain\n",
    "                        print(f\"Found high-precision keyword '{keyword}' confirming domain '{candidate_domain}'\")\n",
    "                        return candidate_domain\n",
    "\n",
    "        # Check all domains for very strong keyword matches\n",
    "        for domain, keywords in high_precision_keywords.items():\n",
    "            # Count how many high-precision keywords are found\n",
    "            matches = [keyword for keyword in keywords if keyword in text_lower]\n",
    "            if len(matches) >= 3:  # If 3 or more high-precision keywords are found, it's a strong signal\n",
    "                print(f\"Found {len(matches)} high-precision keywords for domain '{domain}': {', '.join(matches[:3])}...\")\n",
    "                return domain\n",
    "            elif len(matches) >= 2 and candidate_score < 0.3:  # If 2 keywords and low confidence, still a good signal\n",
    "                print(f\"Found {len(matches)} high-precision keywords for domain '{domain}': {', '.join(matches)}\")\n",
    "                return domain\n",
    "\n",
    "        # No strong keyword matches found\n",
    "        return None\n",
    "\n",
    "    def _keyword_based_domain_detection(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Enhanced keyword-based domain detection method.\n",
    "        Used as fallback when zero-shot classification fails or has low confidence.\n",
    "        Includes expanded keyword lists for domains that previously showed low confidence.\n",
    "\n",
    "        Args:\n",
    "            text: Input text\n",
    "\n",
    "        Returns:\n",
    "            Detected domain\n",
    "        \"\"\"\n",
    "        # Try to import expanded keyword lists\n",
    "        try:\n",
    "            from domain_keywords_expansion import (\n",
    "                TECHNOLOGY_KEYWORDS, BUSINESS_KEYWORDS, HEALTH_KEYWORDS, POLITICS_KEYWORDS,\n",
    "                SPORTS_KEYWORDS, ENTERTAINMENT_KEYWORDS, SCIENCE_KEYWORDS, ENVIRONMENT_KEYWORDS,\n",
    "                WORLD_KEYWORDS, EDUCATION_KEYWORDS, FOOD_KEYWORDS, TRAVEL_KEYWORDS,\n",
    "                AUTOMOTIVE_KEYWORDS, REAL_ESTATE_KEYWORDS, CYBERSECURITY_KEYWORDS,\n",
    "                AI_KEYWORDS, SPACE_KEYWORDS, AGRICULTURE_KEYWORDS, MENTAL_HEALTH_KEYWORDS\n",
    "            )\n",
    "            # Successfully loaded expanded keyword lists\n",
    "            print(\"Loaded comprehensive domain keyword lists\")\n",
    "        except ImportError:\n",
    "            # Could not load expanded keyword lists, will use default lists\n",
    "            print(\"Expanded domain keyword lists not found, using default lists\")\n",
    "\n",
    "        # Core terminology for each domain (20% that covers 80% of cases)\n",
    "        domain_keywords = {\n",
    "            \"technology\": [\n",
    "                \"technology\", \"software\", \"hardware\", \"AI\", \"artificial intelligence\", \"machine learning\",\n",
    "                \"digital\", \"data\", \"algorithm\", \"computing\", \"chip\", \"processor\", \"semiconductor\",\n",
    "                \"app\", \"application\", \"device\", \"platform\", \"cloud\", \"network\", \"internet\", \"cyber\",\n",
    "                \"tech\", \"innovation\", \"startup\", \"smartphone\", \"computer\", \"robot\", \"automation\",\n",
    "                \"programming\", \"code\", \"developer\", \"encryption\", \"blockchain\", \"cryptocurrency\",\n",
    "                \"virtual reality\", \"augmented reality\", \"IoT\", \"5G\", \"broadband\", \"server\", \"database\"\n",
    "            ],\n",
    "            \"business\": [\n",
    "                \"business\", \"company\", \"market\", \"industry\", \"corporate\", \"firm\", \"enterprise\",\n",
    "                \"CEO\", \"executive\", \"management\", \"strategy\", \"investment\", \"investor\", \"stock\",\n",
    "                \"finance\", \"financial\", \"economy\", \"economic\", \"revenue\", \"profit\", \"earnings\",\n",
    "                \"growth\", \"startup\", \"entrepreneur\", \"venture capital\", \"merger\", \"acquisition\",\n",
    "                \"IPO\", \"quarterly\", \"fiscal\", \"shareholder\", \"stakeholder\", \"consumer\", \"customer\",\n",
    "                \"client\", \"retail\", \"wholesale\", \"supply chain\", \"logistics\", \"manufacturing\", \"product\"\n",
    "            ],\n",
    "            \"health\": [\n",
    "                \"health\", \"medical\", \"medicine\", \"doctor\", \"physician\", \"patient\", \"hospital\",\n",
    "                \"clinic\", \"treatment\", \"therapy\", \"disease\", \"condition\", \"symptom\", \"diagnosis\",\n",
    "                \"drug\", \"medication\", \"pharmaceutical\", \"surgery\", \"vaccine\", \"vaccination\",\n",
    "                \"immune\", \"immunity\", \"virus\", \"bacterial\", \"infection\", \"cancer\", \"diabetes\",\n",
    "                \"heart disease\", \"obesity\", \"mental health\", \"psychology\", \"psychiatry\", \"wellness\",\n",
    "                \"nutrition\", \"diet\", \"exercise\", \"fitness\", \"healthcare\", \"clinical\", \"trial\"\n",
    "            ],\n",
    "            \"politics\": [\n",
    "                \"politics\", \"government\", \"policy\", \"election\", \"vote\", \"voter\", \"campaign\",\n",
    "                \"candidate\", \"president\", \"senator\", \"representative\", \"congress\", \"parliament\",\n",
    "                \"democrat\", \"republican\", \"liberal\", \"conservative\", \"progressive\", \"legislation\",\n",
    "                \"law\", \"bill\", \"regulation\", \"federal\", \"state\", \"local\", \"national\", \"international\",\n",
    "                \"administration\", \"White House\", \"Supreme Court\", \"constitutional\", \"democracy\",\n",
    "                \"democratic\", \"authoritarian\", \"populist\", \"political\", \"politician\", \"party\"\n",
    "            ],\n",
    "            \"sports\": [\n",
    "                \"sports\", \"game\", \"team\", \"player\", \"coach\", \"championship\", \"tournament\", \"match\",\n",
    "                \"season\", \"league\", \"score\", \"win\", \"lose\", \"victory\", \"defeat\", \"record\", \"fan\",\n",
    "                \"stadium\", \"arena\", \"field\", \"court\", \"NFL\", \"NBA\", \"MLB\", \"NHL\", \"soccer\", \"football\",\n",
    "                \"basketball\", \"baseball\", \"hockey\", \"tennis\", \"golf\", \"Olympic\", \"Olympics\", \"medal\",\n",
    "                \"athlete\", \"athletic\", \"performance\", \"contract\", \"draft\", \"trade\", \"championship\"\n",
    "            ],\n",
    "            \"entertainment\": [\n",
    "                \"entertainment\", \"movie\", \"film\", \"television\", \"TV\", \"show\", \"series\", \"episode\",\n",
    "                \"actor\", \"actress\", \"director\", \"producer\", \"Hollywood\", \"celebrity\", \"star\", \"fame\",\n",
    "                \"award\", \"Oscar\", \"Emmy\", \"Grammy\", \"Golden Globe\", \"box office\", \"streaming\",\n",
    "                \"Netflix\", \"Disney\", \"HBO\", \"Amazon\", \"Hulu\", \"music\", \"album\", \"song\", \"artist\",\n",
    "                \"band\", \"concert\", \"tour\", \"performance\", \"release\", \"review\", \"critic\", \"rating\"\n",
    "            ],\n",
    "            \"science\": [\n",
    "                \"science\", \"scientific\", \"research\", \"researcher\", \"study\", \"discovery\", \"experiment\",\n",
    "                \"laboratory\", \"lab\", \"theory\", \"hypothesis\", \"data\", \"analysis\", \"evidence\", \"journal\",\n",
    "                \"publication\", \"peer-review\", \"academic\", \"university\", \"college\", \"professor\",\n",
    "                \"student\", \"education\", \"degree\", \"PhD\", \"STEM\", \"physics\", \"chemistry\", \"biology\",\n",
    "                \"astronomy\", \"geology\", \"mathematics\", \"statistics\", \"engineering\", \"technology\",\n",
    "                \"innovation\", \"breakthrough\", \"grant\", \"funding\", \"collaboration\"\n",
    "            ],\n",
    "            \"environment\": [\n",
    "                \"environment\", \"environmental\", \"climate\", \"climate change\", \"global warming\",\n",
    "                \"carbon\", \"emission\", \"greenhouse gas\", \"pollution\", \"renewable\", \"sustainable\",\n",
    "                \"sustainability\", \"conservation\", \"ecosystem\", \"biodiversity\", \"species\", \"wildlife\",\n",
    "                \"habitat\", \"forest\", \"deforestation\", \"ocean\", \"marine\", \"water\", \"air quality\",\n",
    "                \"energy\", \"solar\", \"wind\", \"hydroelectric\", \"fossil fuel\", \"coal\", \"natural gas\",\n",
    "                \"oil\", \"recycling\", \"waste\", \"plastic\", \"EPA\", \"regulation\", \"policy\", \"agreement\",\n",
    "                \"Paris Agreement\", \"drought\", \"flood\", \"extreme weather\", \"temperature\"\n",
    "            ],\n",
    "            \"world\": [\n",
    "                \"international\", \"global\", \"world\", \"foreign\", \"country\", \"nation\", \"region\",\n",
    "                \"United Nations\", \"UN\", \"NATO\", \"EU\", \"European Union\", \"treaty\", \"agreement\",\n",
    "                \"diplomacy\", \"diplomatic\", \"embassy\", \"ambassador\", \"foreign minister\",\n",
    "                \"foreign policy\", \"trade\", \"tariff\", \"sanction\", \"conflict\", \"war\", \"peace\",\n",
    "                \"military\", \"army\", \"defense\", \"security\", \"terrorism\", \"refugee\", \"immigration\",\n",
    "                \"border\", \"summit\", \"bilateral\", \"multilateral\", \"alliance\", \"coalition\",\n",
    "                \"Middle East\", \"Asia\", \"Africa\", \"Europe\", \"North America\", \"South America\"\n",
    "            ],\n",
    "            # Add basic keywords for problematic domains (will be expanded if expanded lists are available)\n",
    "            \"food\": [\n",
    "                \"food\", \"cuisine\", \"dish\", \"meal\", \"recipe\", \"ingredient\", \"cooking\", \"baking\",\n",
    "                \"chef\", \"restaurant\", \"dining\", \"menu\", \"taste\", \"flavor\", \"culinary\", \"kitchen\",\n",
    "                \"breakfast\", \"lunch\", \"dinner\", \"appetizer\", \"dessert\", \"snack\", \"beverage\", \"drink\"\n",
    "            ],\n",
    "            \"agriculture\": [\n",
    "                \"agriculture\", \"farming\", \"farm\", \"crop\", \"harvest\", \"cultivation\", \"livestock\",\n",
    "                \"agricultural\", \"farmer\", \"ranch\", \"plantation\", \"orchard\", \"vineyard\", \"greenhouse\",\n",
    "                \"soil\", \"irrigation\", \"fertilizer\", \"pesticide\", \"organic farming\", \"sustainable agriculture\"\n",
    "            ],\n",
    "            \"education\": [\n",
    "                \"education\", \"school\", \"university\", \"college\", \"student\", \"teacher\", \"professor\",\n",
    "                \"classroom\", \"curriculum\", \"course\", \"degree\", \"learning\", \"teaching\", \"academic\",\n",
    "                \"study\", \"research\", \"lecture\", \"assignment\", \"exam\", \"test\", \"grade\", \"graduation\"\n",
    "            ],\n",
    "            \"mental health\": [\n",
    "                \"mental health\", \"psychology\", \"psychiatry\", \"therapy\", \"counseling\", \"depression\",\n",
    "                \"anxiety\", \"stress\", \"trauma\", \"disorder\", \"wellbeing\", \"emotional health\", \"mindfulness\",\n",
    "                \"self-care\", \"mental illness\", \"psychiatric\", \"psychological\", \"therapist\", \"counselor\"\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        # Expand domain keywords with specialized lists if available\n",
    "        # Check if expanded keywords are available in global or local namespace\n",
    "        try:\n",
    "            # First try to get keywords from globals\n",
    "            if 'TECHNOLOGY_KEYWORDS' in globals():\n",
    "                domain_keywords[\"technology\"] = globals()['TECHNOLOGY_KEYWORDS']\n",
    "                domain_keywords[\"business\"] = globals()['BUSINESS_KEYWORDS']\n",
    "                domain_keywords[\"health\"] = globals()['HEALTH_KEYWORDS']\n",
    "                domain_keywords[\"politics\"] = globals()['POLITICS_KEYWORDS']\n",
    "                domain_keywords[\"sports\"] = globals()['SPORTS_KEYWORDS']\n",
    "                domain_keywords[\"entertainment\"] = globals()['ENTERTAINMENT_KEYWORDS']\n",
    "                domain_keywords[\"science\"] = globals()['SCIENCE_KEYWORDS']\n",
    "                domain_keywords[\"environment\"] = globals()['ENVIRONMENT_KEYWORDS']\n",
    "                domain_keywords[\"world\"] = globals()['WORLD_KEYWORDS']\n",
    "                domain_keywords[\"education\"] = globals()['EDUCATION_KEYWORDS']\n",
    "                domain_keywords[\"food\"] = globals()['FOOD_KEYWORDS']\n",
    "                domain_keywords[\"travel\"] = globals()['TRAVEL_KEYWORDS']\n",
    "                domain_keywords[\"automotive\"] = globals()['AUTOMOTIVE_KEYWORDS']\n",
    "                domain_keywords[\"real estate\"] = globals()['REAL_ESTATE_KEYWORDS']\n",
    "                domain_keywords[\"cybersecurity\"] = globals()['CYBERSECURITY_KEYWORDS']\n",
    "                domain_keywords[\"artificial intelligence\"] = globals()['AI_KEYWORDS']\n",
    "                domain_keywords[\"space\"] = globals()['SPACE_KEYWORDS']\n",
    "                domain_keywords[\"agriculture\"] = globals()['AGRICULTURE_KEYWORDS']\n",
    "                domain_keywords[\"mental health\"] = globals()['MENTAL_HEALTH_KEYWORDS']\n",
    "\n",
    "                print(\"Successfully updated domain keywords from global namespace\")\n",
    "            # If not in globals, try locals\n",
    "            elif 'TECHNOLOGY_KEYWORDS' in locals():\n",
    "                domain_keywords[\"technology\"] = TECHNOLOGY_KEYWORDS\n",
    "                domain_keywords[\"business\"] = BUSINESS_KEYWORDS\n",
    "                domain_keywords[\"health\"] = HEALTH_KEYWORDS\n",
    "                domain_keywords[\"politics\"] = POLITICS_KEYWORDS\n",
    "                domain_keywords[\"sports\"] = SPORTS_KEYWORDS\n",
    "                domain_keywords[\"entertainment\"] = ENTERTAINMENT_KEYWORDS\n",
    "                domain_keywords[\"science\"] = SCIENCE_KEYWORDS\n",
    "                domain_keywords[\"environment\"] = ENVIRONMENT_KEYWORDS\n",
    "                domain_keywords[\"world\"] = WORLD_KEYWORDS\n",
    "                domain_keywords[\"education\"] = EDUCATION_KEYWORDS\n",
    "                domain_keywords[\"food\"] = FOOD_KEYWORDS\n",
    "                domain_keywords[\"travel\"] = TRAVEL_KEYWORDS\n",
    "                domain_keywords[\"automotive\"] = AUTOMOTIVE_KEYWORDS\n",
    "                domain_keywords[\"real estate\"] = REAL_ESTATE_KEYWORDS\n",
    "                domain_keywords[\"cybersecurity\"] = CYBERSECURITY_KEYWORDS\n",
    "                domain_keywords[\"artificial intelligence\"] = AI_KEYWORDS\n",
    "                domain_keywords[\"space\"] = SPACE_KEYWORDS\n",
    "                domain_keywords[\"agriculture\"] = AGRICULTURE_KEYWORDS\n",
    "                domain_keywords[\"mental health\"] = MENTAL_HEALTH_KEYWORDS\n",
    "\n",
    "                print(\"Successfully updated domain keywords from local namespace\")\n",
    "            else:\n",
    "                print(\"No expanded domain keywords found in global or local namespace, using default keywords\")\n",
    "\n",
    "            # Print statistics about the keywords\n",
    "            if 'TECHNOLOGY_KEYWORDS' in globals() or 'TECHNOLOGY_KEYWORDS' in locals():\n",
    "                print(\"\\nDomain keyword statistics:\")\n",
    "                print(f\"Technology: {len(domain_keywords['technology'])} terms\")\n",
    "                print(f\"Business: {len(domain_keywords['business'])} terms\")\n",
    "                print(f\"Health: {len(domain_keywords['health'])} terms\")\n",
    "                print(f\"Politics: {len(domain_keywords['politics'])} terms\")\n",
    "                print(f\"Sports: {len(domain_keywords['sports'])} terms\")\n",
    "                print(f\"Entertainment: {len(domain_keywords['entertainment'])} terms\")\n",
    "                print(f\"Science: {len(domain_keywords['science'])} terms\")\n",
    "                print(f\"Environment: {len(domain_keywords['environment'])} terms\")\n",
    "                print(f\"World: {len(domain_keywords['world'])} terms\")\n",
    "                print(f\"Education: {len(domain_keywords['education'])} terms\")\n",
    "                print(f\"Food: {len(domain_keywords['food'])} terms\")\n",
    "                print(f\"Travel: {len(domain_keywords['travel'])} terms\")\n",
    "                print(f\"Automotive: {len(domain_keywords['automotive'])} terms\")\n",
    "                print(f\"Real Estate: {len(domain_keywords['real estate'])} terms\")\n",
    "                print(f\"Cybersecurity: {len(domain_keywords['cybersecurity'])} terms\")\n",
    "                print(f\"AI: {len(domain_keywords['artificial intelligence'])} terms\")\n",
    "                print(f\"Space: {len(domain_keywords['space'])} terms\")\n",
    "                print(f\"Agriculture: {len(domain_keywords['agriculture'])} terms\")\n",
    "                print(f\"Mental Health: {len(domain_keywords['mental health'])} terms\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error applying expanded keywords: {str(e)}\")\n",
    "                # Count domain keywords in text (case-insensitive)\n",
    "        text_lower = text.lower()\n",
    "        words = set(re.findall(r'\\b\\w+\\b', text_lower))  # Extract unique words\n",
    "        domain_scores = {}\n",
    "        domain_keyword_matches = {}\n",
    "\n",
    "        for domain, keywords in domain_keywords.items():\n",
    "            # Initialize scores and matches\n",
    "            score = 0\n",
    "            keyword_matches = []\n",
    "\n",
    "            for keyword in keywords:\n",
    "                keyword_lower = keyword.lower()\n",
    "\n",
    "                # Exact match (higher weight)\n",
    "                exact_count = text_lower.count(keyword_lower)\n",
    "                if exact_count > 0:\n",
    "                    match_score = exact_count * 2  # Double weight for exact matches\n",
    "                    score += match_score\n",
    "                    keyword_matches.append((keyword, match_score))\n",
    "                    continue\n",
    "\n",
    "                # For multi-word keywords, try partial matching\n",
    "                if ' ' in keyword_lower:\n",
    "                    keyword_parts = keyword_lower.split()\n",
    "                    # If all parts of a multi-word keyword are present, count as partial match\n",
    "                    if all(part in words for part in keyword_parts):\n",
    "                        score += 1\n",
    "                        keyword_matches.append((keyword, 1))\n",
    "                        continue\n",
    "\n",
    "                    # If most parts are present (for longer keywords)\n",
    "                    if len(keyword_parts) > 2 and sum(1 for part in keyword_parts if part in words) >= len(keyword_parts) * 0.7:\n",
    "                        score += 0.5\n",
    "                        keyword_matches.append((keyword, 0.5))\n",
    "\n",
    "                # For single-word keywords, check for word presence\n",
    "                elif keyword_lower in words:\n",
    "                    score += 1\n",
    "                    keyword_matches.append((keyword, 1))\n",
    "\n",
    "            domain_scores[domain] = score\n",
    "            domain_keyword_matches[domain] = keyword_matches\n",
    "\n",
    "        # Debug output for domain detection\n",
    "        print(\"\\nKeyword-Based Domain Detection Results\")\n",
    "        print(\"-\" * 40)\n",
    "        for domain, score in sorted(domain_scores.items(), key=lambda x: x[1], reverse=True):\n",
    "            print(f\"{domain}: {score:.4f}\")\n",
    "\n",
    "            # Show top matching keywords for domains with non-zero scores\n",
    "            if score > 0 and len(domain_keyword_matches[domain]) > 0:\n",
    "                # Sort matches by score\n",
    "                sorted_matches = sorted(domain_keyword_matches[domain], key=lambda x: x[1], reverse=True)\n",
    "                # Show top 5 matches\n",
    "                top_matches = sorted_matches[:5]\n",
    "                print(f\"  Top keywords: {', '.join([f'{kw} ({count})' for kw, count in top_matches])}\")\n",
    "                if len(sorted_matches) > 5:\n",
    "                    print(f\"  ...and {len(sorted_matches) - 5} more matches\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        # Return domain with highest score, default to \"general\" if all scores are 0\n",
    "        if max(domain_scores.values(), default=0) > 0:\n",
    "            detected_domain = max(domain_scores.items(), key=lambda x: x[1])[0]\n",
    "            print(f\"Detected domain: {detected_domain}\\n\")\n",
    "            return detected_domain\n",
    "        else:\n",
    "            print(\"No domain detected, using 'general'\\n\")\n",
    "            return \"general\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fu code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T06:27:38.257498Z",
     "iopub.status.busy": "2025-04-12T06:27:38.257165Z",
     "iopub.status.idle": "2025-04-12T06:27:38.815806Z",
     "shell.execute_reply": "2025-04-12T06:27:38.814948Z",
     "shell.execute_reply.started": "2025-04-12T06:27:38.257465Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import re\n",
    "import gc\n",
    "from typing import List, Dict, Tuple, Set, Optional, Union, Any\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "\n",
    "class FusionKeyphraseExtractor:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        abstractive_extractor: AbstractiveKeyphraseExtractor = None,\n",
    "        extractive_extractor: HybridExtractiveKeyphraseExtractor = None,\n",
    "        use_gpu: bool = True,\n",
    "        abstractive_weight: float = 0.5,\n",
    "        extractive_weight: float = 0.5,\n",
    "        redundancy_threshold: float = 0.72,\n",
    "        min_score: float = 0.1,\n",
    "    ):\n",
    "\n",
    "        if abstractive_extractor is None:\n",
    "            print(\"Initializing abstractive extractor with optimized parameters...\")\n",
    "            self.abstractive_extractor = AbstractiveKeyphraseExtractor(\n",
    "                 model_name=\"google/flan-t5-large\",\n",
    "                use_gpu=True,\n",
    "                max_length=512,\n",
    "                num_beams=24,\n",
    "                top_k=100,\n",
    "                top_p=0.95,\n",
    "                temperature=0.8,\n",
    "                repetition_penalty=1.5,\n",
    "                length_penalty=1.0,\n",
    "                max_new_tokens=300,\n",
    "                prompt_template_idx=0,\n",
    "                use_fp16=True,\n",
    "                max_input_length=1024,\n",
    "                use_chunking=True,\n",
    "                post_process=True,\n",
    "                filter_stopwords=True,\n",
    "                min_phrase_length=1,\n",
    "                max_phrase_length=5,\n",
    "                prioritize_multi_word=True,\n",
    "                use_lemmatization=True,\n",
    "                use_ner=True,\n",
    "                ner_model=\"en_core_web_sm\",\n",
    "                use_sampling=False,\n",
    "                num_beam_groups=6,\n",
    "                diversity_penalty=1.7,\n",
    "                use_mdeberta_domain_detection=False\n",
    "            )\n",
    "        else:\n",
    "            self.abstractive_extractor = abstractive_extractor\n",
    "\n",
    "        if extractive_extractor is None:\n",
    "            print(\"Initializing extractive extractor with optimized parameters...\")\n",
    "            self.extractive_extractor = HybridExtractiveKeyphraseExtractor(\n",
    "             model_name=\"all-mpnet-base-v2\",\n",
    "                use_gpu=True,\n",
    "                top_n=10,\n",
    "                redundancy_threshold=0.82,\n",
    "                diversity_penalty=0.65,\n",
    "                prioritize_named_entities=False,\n",
    "                ngram_range=(1, 3),\n",
    "                clean_boundaries=True,\n",
    "                use_noun_chunks=True,\n",
    "                boost_exact_matches=True,\n",
    "                use_position_weight=True,\n",
    "                use_tfidf_weight=True,\n",
    "                use_ensemble=True,\n",
    "                use_lemmatization=True,\n",
    "                use_partial_matching=True,\n",
    "                use_semantic_matching=True,\n",
    "                use_enhanced_pos_filtering=True,\n",
    "                use_title_lead_boost=True,\n",
    "                method_weights={\n",
    "                    'keybert': 0.38,\n",
    "                    'multipartiterank': 0.27,\n",
    "                    'yake': 0.18,\n",
    "                    'textrank': 0.17\n",
    "                }\n",
    "                )\n",
    "        else:\n",
    "            self.extractive_extractor = extractive_extractor\n",
    "\n",
    "        self.use_gpu = use_gpu\n",
    "        self.abstractive_weight = abstractive_weight\n",
    "        self.extractive_weight = extractive_weight\n",
    "        self.redundancy_threshold = redundancy_threshold\n",
    "        self.min_score = min_score\n",
    "\n",
    "        print(\"Fusion Keyphrase Extractor initialized\")\n",
    "\n",
    "    def normalize_keyphrase_count(self, keyphrases: List[Tuple[str, float]], text: str, domain: str,\n",
    "                               target_min: int = 12, target_max: int = 18, debug: bool = False) -> List[Tuple[str, float]]:\n",
    "        if not keyphrases:\n",
    "            return []\n",
    "\n",
    "        word_count = len(text.split())\n",
    "\n",
    "        sorted_keyphrases = sorted(keyphrases, key=lambda x: x[1], reverse=True)\n",
    "        current_count = len(sorted_keyphrases)\n",
    "\n",
    "        domain_quality_thresholds = {\n",
    "            \"artificial intelligence\": 0.60,\n",
    "            \"technology\": 0.55,\n",
    "            \"cybersecurity\": 0.55,\n",
    "            \"automotive\": 0.50,\n",
    "            \"food\": 0.40,\n",
    "            \"environment\": 0.45,\n",
    "            \"real estate\": 0.40,\n",
    "            \"entertainment\": 0.35,\n",
    "            \"default\": 0.40\n",
    "        }\n",
    "\n",
    "        normalized_domain = domain.lower().strip()\n",
    "\n",
    "        domain_mapping = {\n",
    "            \"ai\": \"artificial intelligence\",\n",
    "            \"tech\": \"technology\",\n",
    "            \"cyber\": \"cybersecurity\",\n",
    "            \"auto\": \"automotive\",\n",
    "            \"cars\": \"automotive\",\n",
    "            \"nutrition\": \"food\",\n",
    "            \"climate\": \"environment\",\n",
    "            \"housing\": \"real estate\",\n",
    "            \"property\": \"real estate\",\n",
    "            \"media\": \"entertainment\",\n",
    "            \"movie\": \"entertainment\",\n",
    "            \"film\": \"entertainment\"\n",
    "        }\n",
    "\n",
    "        for key, mapped_domain in domain_mapping.items():\n",
    "            if key in normalized_domain:\n",
    "                normalized_domain = mapped_domain\n",
    "                break\n",
    "\n",
    "        quality_threshold = domain_quality_thresholds.get(normalized_domain, domain_quality_thresholds[\"default\"])\n",
    "        print(f\"Using domain '{normalized_domain}' with quality threshold: {quality_threshold:.2f}\")\n",
    "\n",
    "        if word_count < 300:\n",
    "            quality_threshold *= 0.9\n",
    "        elif word_count > 600:\n",
    "            quality_threshold *= 1.1\n",
    "\n",
    "        if debug:\n",
    "            print(f\"\\nNormalizing keyphrase count for {domain} domain:\")\n",
    "            print(f\"- Current count: {current_count}\")\n",
    "            print(f\"- Target range: {target_min}-{target_max}\")\n",
    "            print(f\"- Quality threshold: {quality_threshold:.2f}\")\n",
    "            print(f\"- Text length: {word_count} words\")\n",
    "\n",
    "        if current_count < target_min:\n",
    "            if debug:\n",
    "                print(f\"Too few keyphrases ({current_count} < {target_min}), lowering quality threshold\")\n",
    "\n",
    "            remaining_needed = target_min - current_count\n",
    "\n",
    "            for reduction_factor in [0.9, 0.8, 0.7, 0.6, 0.5]:\n",
    "                adjusted_threshold = quality_threshold * reduction_factor\n",
    "\n",
    "                additional_keyphrases = []\n",
    "\n",
    "                abstractive_candidates = self.abstractive_extractor.extract_keyphrases_with_scores(text)\n",
    "                for kp, score in abstractive_candidates:\n",
    "                    kp_lower = kp.lower()\n",
    "                    if not any(kp_lower == existing_kp.lower() for existing_kp, _ in sorted_keyphrases):\n",
    "                        if score >= adjusted_threshold:\n",
    "                            additional_keyphrases.append((kp, score))\n",
    "\n",
    "                if len(additional_keyphrases) >= remaining_needed:\n",
    "                    additional_keyphrases = sorted(additional_keyphrases, key=lambda x: x[1], reverse=True)\n",
    "                    sorted_keyphrases.extend(additional_keyphrases[:remaining_needed])\n",
    "\n",
    "                    if debug:\n",
    "                        print(f\"Added {len(additional_keyphrases[:remaining_needed])} keyphrases with threshold {adjusted_threshold:.2f}\")\n",
    "\n",
    "                    break\n",
    "\n",
    "            sorted_keyphrases = sorted(sorted_keyphrases, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        elif current_count > target_max:\n",
    "            if debug:\n",
    "                print(f\"Too many keyphrases ({current_count} > {target_max}), applying stricter filtering\")\n",
    "\n",
    "            diversity_weight = 0.3\n",
    "\n",
    "            sorted_keyphrases = self.abstractive_extractor.select_diverse_keyphrases(\n",
    "                sorted_keyphrases,\n",
    "                target_max,\n",
    "                diversity_weight\n",
    "            )\n",
    "\n",
    "            if debug:\n",
    "                print(f\"Selected {len(sorted_keyphrases)} diverse keyphrases\")\n",
    "\n",
    "        final_keyphrases = [(kp, score) for kp, score in sorted_keyphrases if score >= quality_threshold * 0.8]\n",
    "\n",
    "        if len(final_keyphrases) < target_min and len(sorted_keyphrases) > len(final_keyphrases):\n",
    "            remaining_needed = target_min - len(final_keyphrases)\n",
    "\n",
    "            filtered_out = [(kp, score) for kp, score in sorted_keyphrases if (kp, score) not in final_keyphrases]\n",
    "            filtered_out = sorted(filtered_out, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "            final_keyphrases.extend(filtered_out[:remaining_needed])\n",
    "\n",
    "            final_keyphrases = sorted(final_keyphrases, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "            if debug:\n",
    "                print(f\"Added back {min(remaining_needed, len(filtered_out))} keyphrases to reach minimum target\")\n",
    "\n",
    "        if debug:\n",
    "            print(f\"Final normalized count: {len(final_keyphrases)}\")\n",
    "\n",
    "        return final_keyphrases\n",
    "\n",
    "    def extract_keyphrases_with_scores(self, text: str, debug: bool = False, original_domain: str = None) -> List[Tuple[str, float]]:\n",
    "        start_time = time.time()\n",
    "\n",
    "        if debug:\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(\"FUSION KEYPHRASE EXTRACTION\")\n",
    "            print(\"=\"*80)\n",
    "\n",
    "        if debug:\n",
    "            print(\"\\nStep 1: Generating independent candidate lists...\")\n",
    "\n",
    "        abstractive_start = time.time()\n",
    "        abstractive_candidates = self.abstractive_extractor.extract_keyphrases_with_scores(text, original_domain=original_domain)\n",
    "        abstractive_time = time.time() - abstractive_start\n",
    "\n",
    "        if debug:\n",
    "            print(f\"Generated {len(abstractive_candidates)} abstractive candidates in {abstractive_time:.2f}s\")\n",
    "            if abstractive_candidates:\n",
    "                print(\"Sample abstractive candidates:\")\n",
    "                for kp, score in abstractive_candidates[:3]:\n",
    "                    print(f\"- {kp}: {score:.4f}\")\n",
    "                if len(abstractive_candidates) > 3:\n",
    "                    print(f\"... and {len(abstractive_candidates) - 3} more\")\n",
    "\n",
    "        extractive_start = time.time()\n",
    "        extractive_candidates = self.extractive_extractor.extract_keyphrases_with_scores(text)\n",
    "        extractive_time = time.time() - extractive_start\n",
    "\n",
    "        if debug:\n",
    "            print(f\"Generated {len(extractive_candidates)} extractive candidates in {extractive_time:.2f}s\")\n",
    "            if extractive_candidates:\n",
    "                print(\"Sample extractive candidates:\")\n",
    "                for kp, score in extractive_candidates[:3]:\n",
    "                    print(f\"- {kp}: {score:.4f}\")\n",
    "                if len(extractive_candidates) > 3:\n",
    "                    print(f\"... and {len(extractive_candidates) - 3} more\")\n",
    "\n",
    "        if debug:\n",
    "            print(\"\\nStep 2: Pooling all candidates...\")\n",
    "\n",
    "        pooled_candidates = []\n",
    "        pooled_candidates.extend([(kp, score, 'abstractive') for kp, score in abstractive_candidates])\n",
    "        pooled_candidates.extend([(kp, score, 'extractive') for kp, score in extractive_candidates])\n",
    "\n",
    "        unique_keyphrases = {}\n",
    "        for kp, score, source in pooled_candidates:\n",
    "            kp_lower = kp.lower()\n",
    "            if kp_lower not in unique_keyphrases:\n",
    "                unique_keyphrases[kp_lower] = (kp, score, source)\n",
    "            else:\n",
    "                if score > unique_keyphrases[kp_lower][1]:\n",
    "                    unique_keyphrases[kp_lower] = (kp, score, source)\n",
    "\n",
    "        pooled_candidates = list(unique_keyphrases.values())\n",
    "\n",
    "        if debug:\n",
    "            print(f\"Pooled {len(pooled_candidates)} unique candidates\")\n",
    "\n",
    "        if debug:\n",
    "            print(\"\\nStep 3: Uniform re-scoring of all candidates...\")\n",
    "\n",
    "        candidate_phrases = [kp for kp, _, _ in pooled_candidates]\n",
    "\n",
    "        rescoring_start = time.time()\n",
    "        rescored_candidates = self.abstractive_extractor.score_keyphrases_by_relevance(candidate_phrases, text)\n",
    "\n",
    "        boosted_multiword = []\n",
    "        for kp, score in rescored_candidates:\n",
    "            word_count = len(kp.split())\n",
    "            if word_count > 1:\n",
    "                if word_count == 2:\n",
    "                    length_boost = 0.25\n",
    "                    words = kp.lower().split()\n",
    "                    if len(set(words)) == len(words):\n",
    "                        length_boost += 0.05\n",
    "                elif word_count == 3:\n",
    "                    length_boost = 0.35\n",
    "                    words = kp.lower().split()\n",
    "                    if len(set(words)) == len(words):\n",
    "                        length_boost += 0.07\n",
    "                elif word_count >= 4:\n",
    "                    length_boost = 0.45\n",
    "                    words = kp.lower().split()\n",
    "                    if len(set(words)) == len(words):\n",
    "                        length_boost += 0.10\n",
    "\n",
    "                boosted_score = min(score * (1 + length_boost), 1.0)\n",
    "                boosted_multiword.append((kp, boosted_score))\n",
    "            else:\n",
    "                boosted_multiword.append((kp, score))\n",
    "\n",
    "        rescored_candidates = boosted_multiword\n",
    "\n",
    "        if original_domain:\n",
    "            domain = original_domain\n",
    "            if debug:\n",
    "                print(f\"Using original domain: {domain}\")\n",
    "        else:\n",
    "            domain = self.abstractive_extractor.detect_domain(text)\n",
    "            if debug:\n",
    "                print(f\"Detected domain: {domain}\")\n",
    "\n",
    "        boosted_candidates = self.abstractive_extractor.boost_domain_specific_concepts(rescored_candidates, domain)\n",
    "\n",
    "        coherent_candidates = self.abstractive_extractor.enhance_semantic_coherence(boosted_candidates, text)\n",
    "\n",
    "        diverse_candidates = self.abstractive_extractor.enhance_semantic_diversity(coherent_candidates, text)\n",
    "        rescoring_time = time.time() - rescoring_start\n",
    "\n",
    "        if debug:\n",
    "            print(f\"Re-scored, boosted, enhanced, and diversified {len(diverse_candidates)} candidates in {rescoring_time:.2f}s\")\n",
    "            if diverse_candidates:\n",
    "                print(\"Sample diverse candidates:\")\n",
    "                for kp, score in sorted(diverse_candidates, key=lambda x: x[1], reverse=True)[:3]:\n",
    "                    print(f\"- {kp}: {score:.4f}\")\n",
    "\n",
    "        if debug:\n",
    "            print(\"\\nStep 4: Applying enhanced redundancy filtering...\")\n",
    "\n",
    "        redundancy_start = time.time()\n",
    "\n",
    "        domain_redundancy_thresholds = {\n",
    "            \"artificial intelligence\": 0.45,\n",
    "            \"cybersecurity\": 0.45,\n",
    "            \"automotive\": 0.45,\n",
    "            \"food\": 0.45,\n",
    "            \"environment\": 0.45,\n",
    "            \"real estate\": 0.45,\n",
    "            \"entertainment\": 0.45,\n",
    "            \"default\": 0.50\n",
    "        }\n",
    "\n",
    "        domain_threshold = domain_redundancy_thresholds.get(\n",
    "            domain.lower(), domain_redundancy_thresholds[\"default\"]\n",
    "        )\n",
    "\n",
    "        if debug:\n",
    "            print(f\"Using domain-specific redundancy threshold for '{domain}': {domain_threshold:.2f}\")\n",
    "\n",
    "        domain_descriptions = {\n",
    "            \"artificial intelligence\": \"Artificial intelligence, machine learning, neural networks, deep learning, computer vision, natural language processing, algorithms, data science, predictive models\",\n",
    "            \"cybersecurity\": \"Cybersecurity, information security, network security, data protection, encryption, threats, vulnerabilities, malware, hacking, privacy, authentication\",\n",
    "            \"automotive\": \"Automotive industry, vehicles, cars, electric vehicles, autonomous driving, transportation, engines, mobility, fuel efficiency, safety systems\",\n",
    "            \"food\": \"Food, cuisine, cooking, recipes, ingredients, nutrition, culinary arts, gastronomy, restaurants, dietary, flavors, meals\",\n",
    "            \"environment\": \"Environment, climate change, sustainability, renewable energy, conservation, pollution, ecosystems, biodiversity, green technology, carbon emissions\",\n",
    "            \"real estate\": \"Real estate, property, housing market, commercial property, residential homes, mortgages, investment properties, land development, construction\",\n",
    "            \"entertainment\": \"Entertainment, movies, music, television, streaming, gaming, celebrities, media, performances, shows, films, production, audience\"\n",
    "        }\n",
    "\n",
    "        normalized_domain = domain.lower().strip()\n",
    "        domain_mapping = {\n",
    "            \"ai\": \"artificial intelligence\",\n",
    "            \"tech\": \"technology\",\n",
    "            \"cyber\": \"cybersecurity\",\n",
    "            \"auto\": \"automotive\",\n",
    "            \"cars\": \"automotive\",\n",
    "            \"nutrition\": \"food\",\n",
    "            \"climate\": \"environment\",\n",
    "            \"housing\": \"real estate\",\n",
    "            \"property\": \"real estate\",\n",
    "            \"media\": \"entertainment\",\n",
    "            \"movie\": \"entertainment\",\n",
    "            \"film\": \"entertainment\"\n",
    "        }\n",
    "\n",
    "        for key, mapped_domain in domain_mapping.items():\n",
    "            if key in normalized_domain:\n",
    "                normalized_domain = mapped_domain\n",
    "                break\n",
    "\n",
    "        domain_boosted = []\n",
    "\n",
    "        domain_desc = domain_descriptions.get(normalized_domain, \"\")\n",
    "        if not domain_desc and normalized_domain in domain_mapping:\n",
    "            mapped_domain = domain_mapping[normalized_domain]\n",
    "            domain_desc = domain_descriptions.get(mapped_domain, \"\")\n",
    "\n",
    "        contrastive_domains = []\n",
    "        for d_name, d_desc in domain_descriptions.items():\n",
    "            if d_name != normalized_domain and d_name != mapped_domain:\n",
    "                contrastive_domains.append(d_desc)\n",
    "\n",
    "        if len(contrastive_domains) > 3:\n",
    "            contrastive_domains = contrastive_domains[:3]\n",
    "\n",
    "        if domain_desc:\n",
    "            try:\n",
    "                all_texts_to_encode = [domain_desc] + contrastive_domains\n",
    "                all_embeddings = self.abstractive_extractor.sentence_model.encode(all_texts_to_encode)\n",
    "                domain_embedding = all_embeddings[0]\n",
    "                contrastive_embeddings = all_embeddings[1:] if len(all_embeddings) > 1 else []\n",
    "\n",
    "                keyphrase_texts = [kp for kp, _ in diverse_candidates]\n",
    "                keyphrase_embeddings = self.abstractive_extractor.sentence_model.encode(keyphrase_texts)\n",
    "\n",
    "                for i, ((kp, score), kp_embedding) in enumerate(zip(diverse_candidates, keyphrase_embeddings)):\n",
    "                    target_similarity = np.dot(domain_embedding, kp_embedding) / (np.linalg.norm(domain_embedding) * np.linalg.norm(kp_embedding))\n",
    "\n",
    "                    contrastive_score = 0\n",
    "                    if contrastive_embeddings:\n",
    "                        other_similarities = []\n",
    "                        for contrast_emb in contrastive_embeddings:\n",
    "                            contrast_sim = np.dot(contrast_emb, kp_embedding) / (np.linalg.norm(contrast_emb) * np.linalg.norm(kp_embedding))\n",
    "                            other_similarities.append(contrast_sim)\n",
    "                        avg_other_sim = sum(other_similarities) / len(other_similarities)\n",
    "\n",
    "                        contrastive_score = max(0, target_similarity - avg_other_sim)\n",
    "\n",
    "                    combined_similarity = 0.7 * target_similarity + 0.3 * contrastive_score\n",
    "\n",
    "                    if combined_similarity > 0.45:\n",
    "                        domain_boost = (combined_similarity - 0.45) * 0.85 + 0.15\n",
    "                        old_score = score\n",
    "                        boosted_score = min(score * (1 + domain_boost), 1.0)\n",
    "                        domain_boosted.append((kp, boosted_score))\n",
    "                        if debug:\n",
    "                            print(f\"DEBUG: Boosted domain-relevant term '{kp}' (similarity: {combined_similarity:.4f}, contrastive: {contrastive_score:.4f}): {old_score:.4f} -> {boosted_score:.4f}\")\n",
    "                        continue\n",
    "\n",
    "                    domain_boosted.append((kp, score))\n",
    "\n",
    "                if debug:\n",
    "                    print(f\"Applied domain embedding similarity boosting for '{normalized_domain}' domain\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Domain embedding similarity failed: {str(e)}. Using no domain boost.\")\n",
    "                domain_boosted = diverse_candidates\n",
    "        else:\n",
    "            domain_boosted = diverse_candidates\n",
    "            if debug:\n",
    "                print(f\"No domain description available for '{normalized_domain}'. Using no domain boost.\")\n",
    "\n",
    "        diverse_candidates = domain_boosted\n",
    "\n",
    "        try:\n",
    "            keyphrase_texts = [kp for kp, _ in diverse_candidates]\n",
    "            if len(keyphrase_texts) > 1:\n",
    "                keyphrase_embeddings = self.abstractive_extractor.sentence_model.encode(keyphrase_texts)\n",
    "\n",
    "                similarity_matrix = np.zeros((len(keyphrase_texts), len(keyphrase_texts)))\n",
    "                for i in range(len(keyphrase_texts)):\n",
    "                    for j in range(len(keyphrase_texts)):\n",
    "                        if i != j:\n",
    "                            similarity = np.dot(keyphrase_embeddings[i], keyphrase_embeddings[j]) / \\\n",
    "                                        (np.linalg.norm(keyphrase_embeddings[i]) * np.linalg.norm(keyphrase_embeddings[j]))\n",
    "                            similarity_matrix[i, j] = similarity\n",
    "\n",
    "                centrality_scores = np.sum(similarity_matrix, axis=1) / (len(keyphrase_texts) - 1)\n",
    "\n",
    "                related_candidates = []\n",
    "\n",
    "                for i, ((kp1, score1), emb1) in enumerate(zip(diverse_candidates, keyphrase_embeddings)):\n",
    "                    if len(kp1.split()) > 1:\n",
    "                        centrality_boost = centrality_scores[i] * 0.25\n",
    "                        word_count_factor = min(0.05 * len(kp1.split()), 0.15)\n",
    "                        total_boost = centrality_boost + word_count_factor\n",
    "\n",
    "                        old_score = score1\n",
    "                        boosted_score = min(score1 * (1 + total_boost), 1.0)\n",
    "                        related_candidates.append((kp1, boosted_score))\n",
    "                        if debug:\n",
    "                            print(f\"DEBUG: Boosted central multi-word term '{kp1}' (centrality: {centrality_scores[i]:.4f}): {old_score:.4f} -> {boosted_score:.4f}\")\n",
    "\n",
    "                clusters = []\n",
    "                assigned = set()\n",
    "\n",
    "                for i in range(len(keyphrase_texts)):\n",
    "                    if i in assigned:\n",
    "                        continue\n",
    "\n",
    "                    cluster = [i]\n",
    "                    assigned.add(i)\n",
    "\n",
    "                    for j in range(len(keyphrase_texts)):\n",
    "                        if j not in assigned and similarity_matrix[i, j] > 0.65:\n",
    "                            cluster.append(j)\n",
    "                            assigned.add(j)\n",
    "\n",
    "                    if len(cluster) > 1:\n",
    "                        clusters.append(cluster)\n",
    "\n",
    "                for cluster in clusters:\n",
    "                    cluster_centrality = [centrality_scores[i] for i in cluster]\n",
    "                    most_central_idx = cluster[np.argmax(cluster_centrality)]\n",
    "\n",
    "                    kp, score = diverse_candidates[most_central_idx]\n",
    "                    cluster_size_boost = min(0.05 * len(cluster), 0.15)\n",
    "                    old_score = score\n",
    "                    boosted_score = min(score * (1 + 0.15 + cluster_size_boost), 1.0)\n",
    "                    related_candidates.append((kp, boosted_score))\n",
    "                    if debug:\n",
    "                        print(f\"DEBUG: Boosted cluster representative '{kp}' (cluster size: {len(cluster)}): {old_score:.4f} -> {boosted_score:.4f}\")\n",
    "\n",
    "                for related, score in related_candidates:\n",
    "                    for i, (kp, old_score) in enumerate(diverse_candidates):\n",
    "                        if kp == related:\n",
    "                            diverse_candidates[i] = (kp, max(old_score, score))\n",
    "                            if debug and max(old_score, score) > old_score:\n",
    "                                print(f\"DEBUG: Updated semantically related term '{kp}' score: {old_score:.4f} -> {max(old_score, score):.4f}\")\n",
    "                            break\n",
    "\n",
    "                if debug and related_candidates:\n",
    "                    print(f\"Applied advanced semantic relationship boosting to {len(related_candidates)} keyphrases\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Advanced semantic relationship detection failed: {str(e)}. Skipping this step.\")\n",
    "\n",
    "        if debug:\n",
    "            print(f\"Applied domain-specific term boosting for '{normalized_domain}' domain\")\n",
    "\n",
    "        deduplicated_candidates = self.abstractive_extractor.remove_redundant_keyphrases(\n",
    "            diverse_candidates,\n",
    "            base_threshold=domain_threshold,\n",
    "            domain=domain\n",
    "        )\n",
    "        redundancy_time = time.time() - redundancy_start\n",
    "\n",
    "        if debug:\n",
    "            print(f\"Removed redundancy, {len(deduplicated_candidates)} candidates remaining ({redundancy_time:.2f}s)\")\n",
    "\n",
    "        if debug:\n",
    "            print(\"\\nStep 5: Applying final quality and generic filtering...\")\n",
    "\n",
    "        filtering_start = time.time()\n",
    "        filtered_candidates = self.abstractive_extractor.filter_generic_terms(deduplicated_candidates, domain, text)\n",
    "\n",
    "        quality_filtered_keyphrases = self.abstractive_extractor.filter_and_select_by_quality(text, domain, filtered_candidates)\n",
    "        filtering_time = time.time() - filtering_start\n",
    "\n",
    "        if debug:\n",
    "            print(f\"Applied quality filtering, {len(quality_filtered_keyphrases)} keyphrases remaining ({filtering_time:.2f}s)\")\n",
    "\n",
    "        if debug:\n",
    "            print(\"\\nStep 6: Applying keyphrase count normalization...\")\n",
    "\n",
    "        normalization_start = time.time()\n",
    "        final_keyphrases = self.normalize_keyphrase_count(\n",
    "            quality_filtered_keyphrases,\n",
    "            text,\n",
    "            domain,\n",
    "            target_min=12,\n",
    "            target_max=18,\n",
    "            debug=debug\n",
    "        )\n",
    "        normalization_time = time.time() - normalization_start\n",
    "\n",
    "        if debug:\n",
    "            print(f\"Applied count normalization in {normalization_time:.2f}s\")\n",
    "\n",
    "        total_time = time.time() - start_time\n",
    "\n",
    "        if debug:\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(\"FUSION EXTRACTION COMPLETE\")\n",
    "            print(\"=\"*80)\n",
    "            print(f\"Total processing time: {total_time:.2f}s\")\n",
    "            print(f\"Final keyphrases ({len(final_keyphrases)}):\")\n",
    "            for kp, score in final_keyphrases:\n",
    "                print(f\"- {kp}: {score:.4f}\")\n",
    "\n",
    "        return final_keyphrases\n",
    "\n",
    "    def extract_keyphrases(self, text: str, original_domain: str = None) -> List[str]:\n",
    "        \n",
    "        scored_keyphrases = self.extract_keyphrases_with_scores(text, original_domain=original_domain)\n",
    "        keyphrases = [kp for kp, _ in scored_keyphrases]\n",
    "        print(f\"Returning {len(keyphrases)} keyphrases from extract_keyphrases\")\n",
    "        return keyphrases\n",
    "\n",
    "    def benchmark(self, articles: List[str], num_articles: int = 5) -> Dict[str, Any]:\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"BENCHMARKING FUSION EXTRACTOR\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        test_articles = articles[:num_articles] if len(articles) > num_articles else articles\n",
    "        print(f\"Testing on {len(test_articles)} articles\")\n",
    "\n",
    "        results = {\n",
    "            \"abstractive\": {\n",
    "                \"counts\": [],\n",
    "                \"times\": [],\n",
    "                \"multi_word_percentages\": [],\n",
    "                \"avg_lengths\": [],\n",
    "                \"keyphrases\": []\n",
    "            },\n",
    "            \"extractive\": {\n",
    "                \"counts\": [],\n",
    "                \"times\": [],\n",
    "                \"multi_word_percentages\": [],\n",
    "                \"avg_lengths\": [],\n",
    "                \"keyphrases\": []\n",
    "            },\n",
    "            \"fusion\": {\n",
    "                \"counts\": [],\n",
    "                \"times\": [],\n",
    "                \"multi_word_percentages\": [],\n",
    "                \"avg_lengths\": [],\n",
    "                \"keyphrases\": []\n",
    "            }\n",
    "        }\n",
    "\n",
    "        for i, article in enumerate(test_articles):\n",
    "            print(f\"\\nTesting article {i+1}/{len(test_articles)}\")\n",
    "\n",
    "            print(\"Testing abstractive extractor...\")\n",
    "            abstractive_start = time.time()\n",
    "            abstractive_keyphrases = self.abstractive_extractor.extract_keyphrases_with_scores(article)\n",
    "            abstractive_time = time.time() - abstractive_start\n",
    "\n",
    "            abstractive_count = len(abstractive_keyphrases)\n",
    "            abstractive_multi_word = sum(1 for kp, _ in abstractive_keyphrases if len(kp.split()) > 1)\n",
    "            abstractive_multi_word_percentage = abstractive_multi_word / abstractive_count if abstractive_count > 0 else 0\n",
    "            abstractive_avg_length = sum(len(kp.split()) for kp, _ in abstractive_keyphrases) / abstractive_count if abstractive_count > 0 else 0\n",
    "\n",
    "            results[\"abstractive\"][\"counts\"].append(abstractive_count)\n",
    "            results[\"abstractive\"][\"times\"].append(abstractive_time)\n",
    "            results[\"abstractive\"][\"multi_word_percentages\"].append(abstractive_multi_word_percentage)\n",
    "            results[\"abstractive\"][\"avg_lengths\"].append(abstractive_avg_length)\n",
    "            results[\"abstractive\"][\"keyphrases\"].append(abstractive_keyphrases)\n",
    "\n",
    "            print(f\"Abstractive: {abstractive_count} keyphrases in {abstractive_time:.2f}s\")\n",
    "            print(f\"Multi-word: {abstractive_multi_word_percentage:.1%}, Avg length: {abstractive_avg_length:.1f}\")\n",
    "\n",
    "            print(\"Testing extractive extractor...\")\n",
    "            extractive_start = time.time()\n",
    "            extractive_keyphrases = self.extractive_extractor.extract_keyphrases_with_scores(article)\n",
    "            extractive_time = time.time() - extractive_start\n",
    "\n",
    "            extractive_count = len(extractive_keyphrases)\n",
    "            extractive_multi_word = sum(1 for kp, _ in extractive_keyphrases if len(kp.split()) > 1)\n",
    "            extractive_multi_word_percentage = extractive_multi_word / extractive_count if extractive_count > 0 else 0\n",
    "            extractive_avg_length = sum(len(kp.split()) for kp, _ in extractive_keyphrases) / extractive_count if extractive_count > 0 else 0\n",
    "\n",
    "            results[\"extractive\"][\"counts\"].append(extractive_count)\n",
    "            results[\"extractive\"][\"times\"].append(extractive_time)\n",
    "            results[\"extractive\"][\"multi_word_percentages\"].append(extractive_multi_word_percentage)\n",
    "            results[\"extractive\"][\"avg_lengths\"].append(extractive_avg_length)\n",
    "            results[\"extractive\"][\"keyphrases\"].append(extractive_keyphrases)\n",
    "\n",
    "            print(f\"Extractive: {extractive_count} keyphrases in {extractive_time:.2f}s\")\n",
    "            print(f\"Multi-word: {extractive_multi_word_percentage:.1%}, Avg length: {extractive_avg_length:.1f}\")\n",
    "\n",
    "            print(\"Testing fusion extractor...\")\n",
    "            fusion_start = time.time()\n",
    "            fusion_keyphrases = self.extract_keyphrases_with_scores(article)\n",
    "            fusion_time = time.time() - fusion_start\n",
    "\n",
    "            fusion_count = len(fusion_keyphrases)\n",
    "            fusion_multi_word = sum(1 for kp, _ in fusion_keyphrases if len(kp.split()) > 1)\n",
    "            fusion_multi_word_percentage = fusion_multi_word / fusion_count if fusion_count > 0 else 0\n",
    "            fusion_avg_length = sum(len(kp.split()) for kp, _ in fusion_keyphrases) / fusion_count if fusion_count > 0 else 0\n",
    "\n",
    "            results[\"fusion\"][\"counts\"].append(fusion_count)\n",
    "            results[\"fusion\"][\"times\"].append(fusion_time)\n",
    "            results[\"fusion\"][\"multi_word_percentages\"].append(fusion_multi_word_percentage)\n",
    "            results[\"fusion\"][\"avg_lengths\"].append(fusion_avg_length)\n",
    "            results[\"fusion\"][\"keyphrases\"].append(fusion_keyphrases)\n",
    "\n",
    "            print(f\"Fusion: {fusion_count} keyphrases in {fusion_time:.2f}s\")\n",
    "            print(f\"Multi-word: {fusion_multi_word_percentage:.1%}, Avg length: {fusion_avg_length:.1f}\")\n",
    "\n",
    "        for extractor in [\"abstractive\", \"extractive\", \"fusion\"]:\n",
    "            results[extractor][\"avg_count\"] = sum(results[extractor][\"counts\"]) / len(results[extractor][\"counts\"])\n",
    "            results[extractor][\"avg_time\"] = sum(results[extractor][\"times\"]) / len(results[extractor][\"times\"])\n",
    "            results[extractor][\"avg_multi_word\"] = sum(results[extractor][\"multi_word_percentages\"]) / len(results[extractor][\"multi_word_percentages\"])\n",
    "            results[extractor][\"avg_length\"] = sum(results[extractor][\"avg_lengths\"]) / len(results[extractor][\"avg_lengths\"])\n",
    "\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"BENCHMARK SUMMARY\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"{'Extractor':<15} {'Avg Count':<10} {'Avg Time':<10} {'Multi-word':<10} {'Avg Length':<10}\")\n",
    "        print(\"-\"*55)\n",
    "\n",
    "        for extractor in [\"abstractive\", \"extractive\", \"fusion\"]:\n",
    "            print(f\"{extractor:<15} {results[extractor]['avg_count']:<10.1f} {results[extractor]['avg_time']:<10.2f}s {results[extractor]['avg_multi_word']:<10.1%} {results[extractor]['avg_length']:<10.1f}\")\n",
    "\n",
    "        print(\"\\nKeyphrase Overlap Analysis:\")\n",
    "\n",
    "        abstractive_extractive_overlap = []\n",
    "        abstractive_fusion_overlap = []\n",
    "        extractive_fusion_overlap = []\n",
    "\n",
    "        for i in range(len(test_articles)):\n",
    "            abstractive_kps = set(kp.lower() for kp, _ in results[\"abstractive\"][\"keyphrases\"][i])\n",
    "            extractive_kps = set(kp.lower() for kp, _ in results[\"extractive\"][\"keyphrases\"][i])\n",
    "            fusion_kps = set(kp.lower() for kp, _ in results[\"fusion\"][\"keyphrases\"][i])\n",
    "\n",
    "            if abstractive_kps and extractive_kps:\n",
    "                overlap = len(abstractive_kps.intersection(extractive_kps)) / len(abstractive_kps.union(extractive_kps))\n",
    "                abstractive_extractive_overlap.append(overlap)\n",
    "\n",
    "            if abstractive_kps and fusion_kps:\n",
    "                overlap = len(abstractive_kps.intersection(fusion_kps)) / len(abstractive_kps.union(fusion_kps))\n",
    "                abstractive_fusion_overlap.append(overlap)\n",
    "\n",
    "            if extractive_kps and fusion_kps:\n",
    "                overlap = len(extractive_kps.intersection(fusion_kps)) / len(extractive_kps.union(fusion_kps))\n",
    "                extractive_fusion_overlap.append(overlap)\n",
    "\n",
    "        if abstractive_extractive_overlap:\n",
    "            avg_overlap = sum(abstractive_extractive_overlap) / len(abstractive_extractive_overlap)\n",
    "            print(f\"Abstractive-Extractive overlap: {avg_overlap:.1%}\")\n",
    "\n",
    "        if abstractive_fusion_overlap:\n",
    "            avg_overlap = sum(abstractive_fusion_overlap) / len(abstractive_fusion_overlap)\n",
    "            print(f\"Abstractive-Fusion overlap: {avg_overlap:.1%}\")\n",
    "\n",
    "        if extractive_fusion_overlap:\n",
    "            avg_overlap = sum(extractive_fusion_overlap) / len(extractive_fusion_overlap)\n",
    "            print(f\"Extractive-Fusion overlap: {avg_overlap:.1%}\")\n",
    "\n",
    "        print(\"\\nFusion Contribution Analysis:\")\n",
    "        fusion_unique = []\n",
    "        abstractive_unique = []\n",
    "        extractive_unique = []\n",
    "\n",
    "        for i in range(len(test_articles)):\n",
    "            abstractive_kps = set(kp.lower() for kp, _ in results[\"abstractive\"][\"keyphrases\"][i])\n",
    "            extractive_kps = set(kp.lower() for kp, _ in results[\"extractive\"][\"keyphrases\"][i])\n",
    "            fusion_kps = set(kp.lower() for kp, _ in results[\"fusion\"][\"keyphrases\"][i])\n",
    "\n",
    "            fusion_unique_kps = fusion_kps - (abstractive_kps.union(extractive_kps))\n",
    "            abstractive_unique_kps = abstractive_kps - fusion_kps\n",
    "            extractive_unique_kps = extractive_kps - fusion_kps\n",
    "\n",
    "            if fusion_kps:\n",
    "                fusion_unique.append(len(fusion_unique_kps) / len(fusion_kps))\n",
    "\n",
    "            if abstractive_kps:\n",
    "                abstractive_unique.append(len(abstractive_unique_kps) / len(abstractive_kps))\n",
    "\n",
    "            if extractive_kps:\n",
    "                extractive_unique.append(len(extractive_unique_kps) / len(extractive_kps))\n",
    "\n",
    "        if fusion_unique:\n",
    "            avg_unique = sum(fusion_unique) / len(fusion_unique)\n",
    "            print(f\"Fusion unique keyphrases: {avg_unique:.1%}\")\n",
    "\n",
    "        if abstractive_unique:\n",
    "            avg_unique = sum(abstractive_unique) / len(abstractive_unique)\n",
    "            print(f\"Abstractive keyphrases not in fusion: {avg_unique:.1%}\")\n",
    "\n",
    "        if extractive_unique:\n",
    "            avg_unique = sum(extractive_unique) / len(extractive_unique)\n",
    "            print(f\"Extractive keyphrases not in fusion: {avg_unique:.1%}\")\n",
    "\n",
    "        return results\n",
    "\n",
    "    def analyze_article(self, text: str) -> Dict[str, Any]:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"DETAILED ARTICLE ANALYSIS\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        print(\"\\nExtracting keyphrases from all methods...\")\n",
    "\n",
    "        abstractive_start = time.time()\n",
    "        abstractive_keyphrases = self.abstractive_extractor.extract_keyphrases_with_scores(text)\n",
    "        abstractive_time = time.time() - abstractive_start\n",
    "\n",
    "        extractive_start = time.time()\n",
    "        extractive_keyphrases = self.extractive_extractor.extract_keyphrases_with_scores(text)\n",
    "        extractive_time = time.time() - extractive_start\n",
    "\n",
    "        fusion_start = time.time()\n",
    "        fusion_keyphrases = self.extract_keyphrases_with_scores(text)\n",
    "        fusion_time = time.time() - fusion_start\n",
    "\n",
    "        print(\"\\nSummary:\")\n",
    "        print(f\"- Abstractive: {len(abstractive_keyphrases)} keyphrases in {abstractive_time:.2f}s\")\n",
    "        print(f\"- Extractive: {len(extractive_keyphrases)} keyphrases in {extractive_time:.2f}s\")\n",
    "        print(f\"- Fusion: {len(fusion_keyphrases)} keyphrases in {fusion_time:.2f}s\")\n",
    "\n",
    "        domain = self.abstractive_extractor.detect_domain(text)\n",
    "        print(f\"\\nDetected domain: {domain}\")\n",
    "\n",
    "        print(\"\\nAbstractive Keyphrases:\")\n",
    "        for kp, score in abstractive_keyphrases:\n",
    "            print(f\"- {kp}: {score:.4f}\")\n",
    "\n",
    "        print(\"\\nExtractive Keyphrases:\")\n",
    "        for kp, score in extractive_keyphrases:\n",
    "            print(f\"- {kp}: {score:.4f}\")\n",
    "\n",
    "        print(\"\\nFusion Keyphrases:\")\n",
    "        for kp, score in fusion_keyphrases:\n",
    "            print(f\"- {kp}: {score:.4f}\")\n",
    "\n",
    "        abstractive_kps = set(kp.lower() for kp, _ in abstractive_keyphrases)\n",
    "        extractive_kps = set(kp.lower() for kp, _ in extractive_keyphrases)\n",
    "        fusion_kps = set(kp.lower() for kp, _ in fusion_keyphrases)\n",
    "\n",
    "        abstractive_extractive_overlap = len(abstractive_kps.intersection(extractive_kps))\n",
    "        abstractive_fusion_overlap = len(abstractive_kps.intersection(fusion_kps))\n",
    "        extractive_fusion_overlap = len(extractive_kps.intersection(fusion_kps))\n",
    "\n",
    "        fusion_unique_kps = fusion_kps - (abstractive_kps.union(extractive_kps))\n",
    "        abstractive_unique_kps = abstractive_kps - extractive_kps\n",
    "        extractive_unique_kps = extractive_kps - abstractive_kps\n",
    "\n",
    "        print(\"\\nOverlap Analysis:\")\n",
    "        print(f\"- Abstractive-Extractive overlap: {abstractive_extractive_overlap} keyphrases\")\n",
    "        print(f\"- Abstractive-Fusion overlap: {abstractive_fusion_overlap} keyphrases\")\n",
    "        print(f\"- Extractive-Fusion overlap: {extractive_fusion_overlap} keyphrases\")\n",
    "\n",
    "        print(\"\\nUnique Keyphrases:\")\n",
    "        print(f\"- Fusion unique: {len(fusion_unique_kps)} keyphrases\")\n",
    "        if fusion_unique_kps:\n",
    "            print(\"  \" + \", \".join(fusion_unique_kps))\n",
    "\n",
    "        print(f\"- Abstractive unique (not in Extractive): {len(abstractive_unique_kps)} keyphrases\")\n",
    "        if abstractive_unique_kps:\n",
    "            print(\"  \" + \", \".join(abstractive_unique_kps))\n",
    "\n",
    "        print(f\"- Extractive unique (not in Abstractive): {len(extractive_unique_kps)} keyphrases\")\n",
    "        if extractive_unique_kps:\n",
    "            print(\"  \" + \", \".join(extractive_unique_kps))\n",
    "\n",
    "        return {\n",
    "            \"abstractive\": {\n",
    "                \"keyphrases\": abstractive_keyphrases,\n",
    "                \"time\": abstractive_time\n",
    "            },\n",
    "            \"extractive\": {\n",
    "                \"keyphrases\": extractive_keyphrases,\n",
    "                \"time\": extractive_time\n",
    "            },\n",
    "            \"fusion\": {\n",
    "                \"keyphrases\": fusion_keyphrases,\n",
    "                \"time\": fusion_time\n",
    "            },\n",
    "            \"domain\": domain,\n",
    "            \"overlap\": {\n",
    "                \"abstractive_extractive\": abstractive_extractive_overlap,\n",
    "                \"abstractive_fusion\": abstractive_fusion_overlap,\n",
    "                \"extractive_fusion\": extractive_fusion_overlap\n",
    "            },\n",
    "            \"unique\": {\n",
    "                \"fusion\": list(fusion_unique_kps),\n",
    "                \"abstractive\": list(abstractive_unique_kps),\n",
    "                \"extractive\": list(extractive_unique_kps)\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def clean_memory(self):\n",
    "        \n",
    "        gc.collect()\n",
    "        if self.use_gpu and torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T06:27:39.110695Z",
     "iopub.status.busy": "2025-04-12T06:27:39.110442Z",
     "iopub.status.idle": "2025-04-12T06:49:38.534735Z",
     "shell.execute_reply": "2025-04-12T06:49:38.533885Z",
     "shell.execute_reply.started": "2025-04-12T06:27:39.110676Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
      "  warnings.warn(Warnings.W111)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing AbstractiveKeyphraseExtractor...\n",
      "Initializing abstractive extractor with optimized parameters...\n",
      "Domain-specific components initialized\n",
      "Loading NER model: en_core_web_sm\n",
      "NER model loaded successfully\n",
      "Loading sentence transformer model...\n",
      "Using device: cuda\n",
      "Loading tokenizer: google/flan-t5-large\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03c0bb365cb24fcdafd270cac3b2b00f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c04cd259ae0c49689badbccaab4bad5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d1951902bac466681ca7261f6071b50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37591f35f78642b1b73390972dcdf791",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: google/flan-t5-large\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c916220b30664230b1e07a74347e9ee0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ffb6406b0ac4de8b3014e51595d3e5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "206500b2b4ac44b2bc0dfab6bb10e376",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully on cuda\n",
      "Initialized generic terms for 15 domains using Pareto principle\n",
      "Domain-specific components initialized\n",
      "Abstractive Keyphrase Extractor initialized\n",
      "Initializing extractive extractor with optimized parameters...\n",
      "Creating comprehensive news-focused IDF corpus\n",
      "Created comprehensive IDF corpus with 206 terms\n",
      "Added 20 additional news-specific terms\n",
      "Final IDF corpus contains 225 terms\n",
      "Using device: cuda\n",
      "Loading SentenceTransformer model: all-mpnet-base-v2\n",
      "Loading KeyBERT\n",
      "Advanced Hybrid Extractive Keyphrase Extractor initialized with ensemble methods\n",
      "Fusion Keyphrase Extractor initialized\n",
      "Generated 21 articles.\n",
      "\n",
      "Testing on 21 diverse news articles...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing articles:   0%|          | 0/21 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing article 1/21 (artificial intelligence)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cd5b08158064a5b9d27c1cc8c181e85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.15k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60fd3c0e01bd4b6797846640dc83e640",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca8836b2b4ca43b683b9d87eabc417bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "418aecc322fd44a7b0f8c9f545513bd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32835b86d91e44b88445ce2c835f47e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "274f65b512b940e491a74351ee2d3cff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized BART-large for improved topic classification\n",
      "Initializing mDeBERTa zero-shot domain classifier (backup method)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78275adfcd6c4262b17892911e276630",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.09k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "861b3b18f0064c77929cf1a4acef8c4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/558M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca573a1987df46f89647881375e3f041",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/467 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5c795243b9c486fa2bda24085c688ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm.model:   0%|          | 0.00/4.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c30f4a7efc7f482fb4c1b4ec73c7aae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/16.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "519865502b4f401ebd4237d40f43936a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/23.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f585f3bbd4904e7fb44ec307ec3693a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/173 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mDeBERTa zero-shot classifier initialized successfully\n",
      "\n",
      "Zero-Shot Domain Detection Results:\n",
      "----------------------------------------\n",
      "Top domain: technology (Score: 0.4954)\n",
      "  1. technology: 0.4954\n",
      "  2. artificial intelligence: 0.4064\n",
      "  3. entertainment: 0.0664\n",
      "  4. business: 0.0185\n",
      "  5. science: 0.0062\n",
      "----------------------------------------\n",
      "Threshold for 'technology': 0.45\n",
      "Detected domain: technology\n",
      "\n",
      "Zero-Shot Domain Detection Results:\n",
      "----------------------------------------\n",
      "Top domain: technology (Score: 0.5250)\n",
      "  1. technology: 0.5250\n",
      "  2. artificial intelligence: 0.4308\n",
      "  3. entertainment: 0.0290\n",
      "  4. business: 0.0081\n",
      "  5. education: 0.0048\n",
      "----------------------------------------\n",
      "Threshold for 'technology': 0.45\n",
      "Extracted 2 named entities\n",
      "Generated 30 candidate keyphrases\n",
      "Semantic diversity: 0.761 → 0.761\n",
      "Keyphrase count: 30 → 30\n",
      "\n",
      "Generic term filtering: 20 -> 19 keyphrases (1 removed)\n",
      "\n",
      "Filtered out generic terms (examples):\n",
      "- Industry expert (0.33): 100% generic words\n",
      "\n",
      "Using parameters for domain 'technology':\n",
      "  - base_threshold: 0.070\n",
      "  - quality_threshold: 0.390\n",
      "  - percentile: 4\n",
      "  - target_count: 10\n",
      "Using domain 'technology' with base threshold: 0.070, quality threshold: 0.390\n",
      "Using 4th percentile for domain 'technology'\n",
      "- Using 5th percentile from top: 0.817\n",
      "\n",
      "Quality Filtering Stats:\n",
      "- Domain: technology\n",
      "- Text Length: 357 words\n",
      "- Content Density: 0.73\n",
      "- Initial Candidates: 19\n",
      "- Base Threshold: 0.070\n",
      "- Quality Threshold: 0.350\n",
      "- After Quality Filter: 10\n",
      "Using target count 10 for domain 'technology'\n",
      "- Final keyphrase count: 10\n",
      "- Semantic diversity score: 0.72\n",
      "DEBUG: Extracting keyphrases with use_position_weight=True, use_tfidf_weight=True, use_ensemble=True\n",
      "DEBUG: KeyBERT extracted 85 keyphrases\n",
      "DEBUG: MultipartiteRank position weight for 'AI artificial intelligence': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'creative industries': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'content creation': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'early': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'film music design': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'practical application': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'impacting workflows': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'potential projects tools': 2.00\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'AI artificial intelligence': 2.00\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'creative industries': 1.03\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'content creation': 0.85\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'early': 0.50\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'film music design': 0.67\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'practical application': 0.53\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'impacting workflows': 0.60\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'potential projects tools': 1.18\n",
      "DEBUG: MultipartiteRank extracted 8 keyphrases\n",
      "DEBUG: YAKE position weight for 'artificial intelligence tools': 2.00\n",
      "DEBUG: YAKE position weight for 'platforms like cinelytic': 2.00\n",
      "DEBUG: YAKE position weight for 'intelligence -powered pcs': 2.00\n",
      "DEBUG: YAKE position weight for 'artificial intelligence': 2.00\n",
      "DEBUG: YAKE position weight for 'language models llms': 2.00\n",
      "DEBUG: YAKE position weight for 'creative avenues industry': 2.00\n",
      "DEBUG: YAKE position weight for 'production platforms like': 2.00\n",
      "DEBUG: YAKE position weight for 'industry experts predict': 2.00\n",
      "DEBUG: YAKE position weight for 'avenues industry experts': 2.00\n",
      "DEBUG: YAKE position weight for 'benefits equitably striking': 2.00\n",
      "DEBUG: YAKE TF-IDF weight for 'artificial intelligence tools': 1.81\n",
      "DEBUG: YAKE TF-IDF weight for 'platforms like cinelytic': 0.60\n",
      "DEBUG: YAKE TF-IDF weight for 'intelligence -powered pcs': 1.07\n",
      "DEBUG: YAKE TF-IDF weight for 'artificial intelligence': 2.00\n",
      "DEBUG: YAKE TF-IDF weight for 'language models llms': 0.53\n",
      "DEBUG: YAKE TF-IDF weight for 'creative avenues industry': 0.65\n",
      "DEBUG: YAKE TF-IDF weight for 'production platforms like': 0.84\n",
      "DEBUG: YAKE TF-IDF weight for 'industry experts predict': 0.53\n",
      "DEBUG: YAKE TF-IDF weight for 'avenues industry experts': 0.60\n",
      "DEBUG: YAKE TF-IDF weight for 'benefits equitably striking': 0.50\n",
      "DEBUG: YAKE extracted 10 keyphrases\n",
      "DEBUG: TextRank position weight for 'artificial intelligence ai artificial intelligence': 2.00\n",
      "DEBUG: TextRank position weight for 'generative ai artificial intelligence tools': 2.00\n",
      "DEBUG: TextRank position weight for 'ai artificial intelligence algorithms assisting': 2.00\n",
      "DEBUG: TextRank position weight for 'ai artificial intelligence tools': 2.00\n",
      "DEBUG: TextRank position weight for 'ai artificial intelligence models': 2.00\n",
      "DEBUG: TextRank position weight for 'ai artificial intelligence': 2.00\n",
      "DEBUG: TextRank position weight for 'film intelligence': 2.00\n",
      "DEBUG: TextRank position weight for 'human talent issues surrounding copyright': 2.00\n",
      "DEBUG: TextRank position weight for 'human creativity': 2.00\n",
      "DEBUG: TextRank position weight for 'personalized creative assistance': 2.00\n",
      "DEBUG: TextRank TF-IDF weight for 'artificial intelligence ai artificial intelligence': 2.00\n",
      "DEBUG: TextRank TF-IDF weight for 'generative ai artificial intelligence tools': 1.31\n",
      "DEBUG: TextRank TF-IDF weight for 'ai artificial intelligence algorithms assisting': 1.23\n",
      "DEBUG: TextRank TF-IDF weight for 'ai artificial intelligence tools': 1.49\n",
      "DEBUG: TextRank TF-IDF weight for 'ai artificial intelligence models': 1.43\n",
      "DEBUG: TextRank TF-IDF weight for 'ai artificial intelligence': 1.59\n",
      "DEBUG: TextRank TF-IDF weight for 'film intelligence': 1.05\n",
      "DEBUG: TextRank TF-IDF weight for 'human talent issues surrounding copyright': 0.52\n",
      "DEBUG: TextRank TF-IDF weight for 'human creativity': 0.61\n",
      "DEBUG: TextRank TF-IDF weight for 'personalized creative assistance': 0.50\n",
      "DEBUG: TextRank extracted 10 keyphrases\n",
      "DEBUG: Raw domain score for sports: 5.00\n",
      "DEBUG: Top domains detected: technology: 0.5098, entertainment: 0.2647, business: 0.0760\n",
      "DEBUG: All domain scores: {'technology': 0.5098039215686274, 'business': 0.07598039215686274, 'health': 0.0, 'science': 0.058823529411764705, 'news': 0.007352941176470588, 'academic': 0.03676470588235294, 'politics': 0.014705882352941176, 'environment': 0.007352941176470588, 'entertainment': 0.2647058823529412, 'sports': 0.024509803921568627}\n",
      "DEBUG: Adaptive method weights:\n",
      "DEBUG:   - keybert: 0.3313\n",
      "DEBUG:   - multipartiterank: 0.3502\n",
      "DEBUG:   - yake: 0.1012\n",
      "DEBUG:   - textrank: 0.2172\n",
      "DEBUG: Length bonus for 'ai artificial intelligence' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'Artificial Intelligence' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'artificial intelligence tool' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'human creativity' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'seismic shift' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'rapid advancement' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'new creative avenue' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'industry expert' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'further integration' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'artificial intelligence ai artificial intelligence' (5 words): 1.60x\n",
      "DEBUG: Length bonus for 'ai artificial intelligence tool' (4 words): 1.45x\n",
      "DEBUG: Length bonus for 'generative ai artificial intelligence tool' (5 words): 1.60x\n",
      "DEBUG: Length bonus for 'intelligence -powered pc' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'content creation' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'film music design' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'practical application' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'avenues industry expert' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'film intelligence' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'video editor' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'predictive analytics' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'more informed decision' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'potential project' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'advertising creation' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'performance testing' (2 words): 1.15x\n",
      "DEBUG: Length bonus for '2023 hollywood strike' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'deep concern' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'human talent issue' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'copyright fair remuneration' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'production platform' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'critical ethical consideration' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'key challenge' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'research expert' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'content abundance' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'new artist' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'algorithmic curation' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'tiktok 's booktok' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'potential benefit' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'creative industry' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'application range' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'new strategy' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'stream recommendation' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'repetitive task' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'large team' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'personalize creative assistance' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'multiple creative task' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'human oversight' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'ethical risk' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'human ingenuity' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'Everything everywhere all' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'impact workflow' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'potential project tool' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'platforms cinelytic' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'language model llm' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'creative avenue industry' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'industry expert predict' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'benefit equitably strike' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'ai artificial intelligence algorithm assist' (5 words): 1.60x\n",
      "DEBUG: Length bonus for 'ai artificial intelligence model' (4 words): 1.45x\n",
      "DEBUG: Length bonus for 'human talent issue surround copyright' (5 words): 1.60x\n",
      "DEBUG: Ensemble combined into 102 keyphrases\n",
      "DEBUG: After redundancy removal: 71 keyphrases\n",
      "DEBUG: Before post-filtering: 71 keyphrases\n",
      "DEBUG: Multi-word phrases: 32\n",
      "DEBUG: Filtered out single word 'work' as it's part of multi-word phrase(s)\n",
      "DEBUG: Filtered out single word 'once' as it's part of multi-word phrase(s)\n",
      "DEBUG: After post-filtering: 69 keyphrases\n",
      "DEBUG: Final top 10 keyphrases: ['potential', 'integration', 'creation', 'film', 'industry', 'hollywood', 'application', 'furthermore', 'however', 'cinelytic']\n",
      "\n",
      "Zero-Shot Domain Detection Results:\n",
      "----------------------------------------\n",
      "Top domain: technology (Score: 0.4954)\n",
      "  1. technology: 0.4954\n",
      "  2. artificial intelligence: 0.4064\n",
      "  3. entertainment: 0.0664\n",
      "  4. business: 0.0185\n",
      "  5. science: 0.0062\n",
      "----------------------------------------\n",
      "Threshold for 'technology': 0.45\n",
      "Semantic diversity: 0.776 → 0.776\n",
      "Keyphrase count: 16 → 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing articles:   5%|▍         | 1/21 [01:33<31:17, 93.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generic term filtering: 15 -> 13 keyphrases (2 removed)\n",
      "\n",
      "Filtered out generic terms (examples):\n",
      "- industry (0.43): exact generic match\n",
      "- furthermore (0.10): exact generic match\n",
      "\n",
      "Using parameters for domain 'technology':\n",
      "  - base_threshold: 0.070\n",
      "  - quality_threshold: 0.390\n",
      "  - percentile: 4\n",
      "  - target_count: 10\n",
      "Using domain 'technology' with base threshold: 0.070, quality threshold: 0.390\n",
      "Using 4th percentile for domain 'technology'\n",
      "- Using 5th percentile from top: 0.816\n",
      "\n",
      "Quality Filtering Stats:\n",
      "- Domain: technology\n",
      "- Text Length: 357 words\n",
      "- Content Density: 0.73\n",
      "- Initial Candidates: 13\n",
      "- Base Threshold: 0.070\n",
      "- Quality Threshold: 0.350\n",
      "- After Quality Filter: 8\n",
      "Using target count 10 for domain 'technology'\n",
      "- Quality filter too strict, falling back to top 10 keyphrases\n",
      "- Using lenient threshold 0.175: 12 keyphrases\n",
      "- Final keyphrase count: 12\n",
      "- Semantic diversity score: 0.77\n",
      "Using domain 'technology' with quality threshold: 0.55\n",
      "Title: AI Advancements Set to Transform Creative Industries in 2025\n",
      "Domain: artificial intelligence\n",
      "Extracted 8 keyphrases:\n",
      "- AI tool: 0.82\n",
      "- Artificial Intelligence: 0.81\n",
      "- not necessarily replacing human creativity: 0.71\n",
      "- and content creation: 0.65\n",
      "- film: 0.61\n",
      "- music: 0.59\n",
      "- language model: 0.51\n",
      "- hollywood: 0.43\n",
      "\n",
      "\n",
      "Processing article 2/21 (artificial intelligence)...\n",
      "\n",
      "Zero-Shot Domain Detection Results:\n",
      "----------------------------------------\n",
      "Top domain: technology (Score: 0.7695)\n",
      "  1. technology: 0.7695\n",
      "  2. artificial intelligence: 0.2095\n",
      "  3. science: 0.0092\n",
      "  4. education: 0.0061\n",
      "  5. business: 0.0023\n",
      "----------------------------------------\n",
      "Threshold for 'technology': 0.45\n",
      "Detected domain: technology\n",
      "Domain detected using BART: artificial intelligence (confidence: 0.5865)\n",
      "Extracted 2 named entities\n",
      "Generated 13 candidate keyphrases\n",
      "Semantic diversity: 0.703 → 0.703\n",
      "Keyphrase count: 13 → 13\n",
      "\n",
      "Generic term filtering: 11 -> 10 keyphrases (1 removed)\n",
      "\n",
      "Filtered out generic terms (examples):\n",
      "- development (0.19): exact generic match\n",
      "\n",
      "Using parameters for domain 'technology':\n",
      "  - base_threshold: 0.090\n",
      "  - quality_threshold: 0.420\n",
      "  - percentile: 5\n",
      "  - target_count: 10\n",
      "Using domain 'technology' with base threshold: 0.090, quality threshold: 0.420\n",
      "Using 5th percentile for domain 'technology'\n",
      "- Using 5th percentile from top: 0.829\n",
      "\n",
      "Quality Filtering Stats:\n",
      "- Domain: technology\n",
      "- Text Length: 351 words\n",
      "- Content Density: 0.69\n",
      "- Initial Candidates: 10\n",
      "- Base Threshold: 0.090\n",
      "- Quality Threshold: 0.350\n",
      "- After Quality Filter: 4\n",
      "Using target count 10 for domain 'technology'\n",
      "- Quality filter too strict, falling back to top 10 keyphrases\n",
      "- Falling back to top 10 keyphrases by score\n",
      "- Final keyphrase count: 10\n",
      "- Semantic diversity score: 0.73\n",
      "DEBUG: Extracting keyphrases with use_position_weight=True, use_tfidf_weight=True, use_ensemble=True\n",
      "DEBUG: KeyBERT extracted 88 keyphrases\n",
      "DEBUG: MultipartiteRank position weight for 'historical data': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'Fairness': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'algorithms': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'AI artificial intelligence': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'paramount': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'human oversight': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'benefits': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'autonomous systems': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'ethical considerations': 2.00\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'historical data': 1.09\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'Fairness': 0.74\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'algorithms': 0.63\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'AI artificial intelligence': 2.00\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'paramount': 0.50\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'human oversight': 0.85\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'benefits': 0.50\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'autonomous systems': 1.02\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'ethical considerations': 1.02\n",
      "DEBUG: MultipartiteRank extracted 9 keyphrases\n",
      "DEBUG: YAKE position weight for 'artificial intelligence systems': 2.00\n",
      "DEBUG: YAKE position weight for 'ensure fairness amazon': 2.00\n",
      "DEBUG: YAKE position weight for 'artificial intelligence': 2.00\n",
      "DEBUG: YAKE position weight for 'artificial intelligence ensuring': 2.00\n",
      "DEBUG: YAKE position weight for 'artificial intelligence lifecycle': 2.00\n",
      "DEBUG: YAKE position weight for 'building trust users': 2.00\n",
      "DEBUG: YAKE position weight for 'prevent harm clear': 2.00\n",
      "DEBUG: YAKE position weight for 'harm clear lines': 2.00\n",
      "DEBUG: YAKE position weight for 'intelligence systems trained': 2.00\n",
      "DEBUG: YAKE position weight for 'intelligence systems particularly': 2.00\n",
      "DEBUG: YAKE TF-IDF weight for 'artificial intelligence systems': 1.83\n",
      "DEBUG: YAKE TF-IDF weight for 'ensure fairness amazon': 0.62\n",
      "DEBUG: YAKE TF-IDF weight for 'artificial intelligence': 2.00\n",
      "DEBUG: YAKE TF-IDF weight for 'artificial intelligence ensuring': 1.52\n",
      "DEBUG: YAKE TF-IDF weight for 'artificial intelligence lifecycle': 1.52\n",
      "DEBUG: YAKE TF-IDF weight for 'building trust users': 0.50\n",
      "DEBUG: YAKE TF-IDF weight for 'prevent harm clear': 0.50\n",
      "DEBUG: YAKE TF-IDF weight for 'harm clear lines': 0.54\n",
      "DEBUG: YAKE TF-IDF weight for 'intelligence systems trained': 1.28\n",
      "DEBUG: YAKE TF-IDF weight for 'intelligence systems particularly': 1.28\n",
      "DEBUG: YAKE extracted 10 keyphrases\n",
      "DEBUG: TextRank position weight for 'artificial intelligence ai artificial intelligence systems': 2.00\n",
      "DEBUG: TextRank position weight for 'ai artificial intelligence systems trained': 2.00\n",
      "DEBUG: TextRank position weight for 'concern ai artificial intelligence systems': 2.00\n",
      "DEBUG: TextRank position weight for 'ai artificial intelligence systems': 2.00\n",
      "DEBUG: TextRank position weight for 'ensure ai artificial intelligence': 2.00\n",
      "DEBUG: TextRank position weight for 'ai artificial intelligence ensuring': 2.00\n",
      "DEBUG: TextRank position weight for 'ai artificial intelligence': 2.00\n",
      "DEBUG: TextRank position weight for 'significant risks several core ethical concerns': 2.00\n",
      "DEBUG: TextRank position weight for 'ethical discussions unesco': 2.00\n",
      "DEBUG: TextRank position weight for 'establishing robust ethical': 2.00\n",
      "DEBUG: TextRank TF-IDF weight for 'artificial intelligence ai artificial intelligence systems': 2.00\n",
      "DEBUG: TextRank TF-IDF weight for 'ai artificial intelligence systems trained': 1.36\n",
      "DEBUG: TextRank TF-IDF weight for 'concern ai artificial intelligence systems': 1.33\n",
      "DEBUG: TextRank TF-IDF weight for 'ai artificial intelligence systems': 1.53\n",
      "DEBUG: TextRank TF-IDF weight for 'ensure ai artificial intelligence': 1.44\n",
      "DEBUG: TextRank TF-IDF weight for 'ai artificial intelligence ensuring': 1.29\n",
      "DEBUG: TextRank TF-IDF weight for 'ai artificial intelligence': 1.61\n",
      "DEBUG: TextRank TF-IDF weight for 'significant risks several core ethical concerns': 0.75\n",
      "DEBUG: TextRank TF-IDF weight for 'ethical discussions unesco': 0.50\n",
      "DEBUG: TextRank TF-IDF weight for 'establishing robust ethical': 0.50\n",
      "DEBUG: TextRank extracted 10 keyphrases\n",
      "DEBUG: Raw domain score for sports: 4.00\n",
      "DEBUG: Top domains detected: technology: 0.4961, entertainment: 0.1137, academic: 0.0982\n",
      "DEBUG: All domain scores: {'technology': 0.49612403100775193, 'business': 0.041343669250646, 'health': 0.023255813953488372, 'science': 0.020671834625323, 'news': 0.056847545219638244, 'academic': 0.09819121447028424, 'politics': 0.06718346253229975, 'environment': 0.06201550387596899, 'entertainment': 0.11369509043927649, 'sports': 0.020671834625323}\n",
      "DEBUG: Adaptive method weights:\n",
      "DEBUG:   - keybert: 0.3351\n",
      "DEBUG:   - multipartiterank: 0.3256\n",
      "DEBUG:   - yake: 0.0953\n",
      "DEBUG:   - textrank: 0.2440\n",
      "DEBUG: Length bonus for 'Artificial Intelligence' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'artificial intelligence system' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'ai artificial intelligence' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'historical data' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'autonomous system' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'ethical consideration' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'critical sector' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'healthcare finance' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'robust ethical framework' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'significant risk' (2 words): 1.15x\n",
      "DEBUG: Length bonus for '2025 fairness' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'critical challenge' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'societal bias' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'race gender' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'socioeconomic status' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'artificial intelligence ai artificial intelligence system' (6 words): 1.60x\n",
      "DEBUG: Length bonus for 'ai artificial intelligence system' (4 words): 1.45x\n",
      "DEBUG: Length bonus for 'rigorous scrutiny' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'training data' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'model output' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'stark reminder' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'trust user' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'full transparency' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'right balance' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'data usage' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'essential privacy' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'major concern' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'concern ai artificial intelligence system' (5 words): 1.60x\n",
      "DEBUG: Length bonus for 'ensure fairness amazon' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'human oversight' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'vast amount' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'user information' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'misuse global standard' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'data storage' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'regulatory gap' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'high energy consumption' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'large model' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'sustainable practice' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'human behavior' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'bias hiring' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'inform consent' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'human safety' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'particularly autonomous one' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'medical diagnostic tool' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'clear line' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'disrupt labor market' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'existential risk' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'ethical discussion unesco' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'other international body' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'human value' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'ethical imperative risk' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'public trust' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'significant societal harm' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'artificial intelligence ensure' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'artificial intelligence lifecycle' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'build trust user' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'prevent harm clear' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'harm clear line' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'intelligence system train' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'intelligence system particularly' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'ai artificial intelligence system train' (5 words): 1.60x\n",
      "DEBUG: Length bonus for 'ensure ai artificial intelligence' (4 words): 1.45x\n",
      "DEBUG: Length bonus for 'ai artificial intelligence ensure' (4 words): 1.45x\n",
      "DEBUG: Length bonus for 'significant risk several core ethical concern' (6 words): 1.60x\n",
      "DEBUG: Length bonus for 'establish robust ethical' (3 words): 1.30x\n",
      "DEBUG: Ensemble combined into 106 keyphrases\n",
      "DEBUG: After redundancy removal: 66 keyphrases\n",
      "DEBUG: Before post-filtering: 66 keyphrases\n",
      "DEBUG: Multi-word phrases: 27\n",
      "DEBUG: After post-filtering: 66 keyphrases\n",
      "DEBUG: Final top 10 keyphrases: ['data', 'human', 'privacy', 'transparency', 'fairness', 'security', 'global', 'harm', 'algorithms', 'accountability']\n",
      "\n",
      "Zero-Shot Domain Detection Results:\n",
      "----------------------------------------\n",
      "Top domain: technology (Score: 0.7695)\n",
      "  1. technology: 0.7695\n",
      "  2. artificial intelligence: 0.2095\n",
      "  3. science: 0.0092\n",
      "  4. education: 0.0061\n",
      "  5. business: 0.0023\n",
      "----------------------------------------\n",
      "Threshold for 'technology': 0.45\n",
      "Semantic diversity: 0.743 → 0.743\n",
      "Keyphrase count: 17 → 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing articles:  10%|▉         | 2/21 [02:35<23:44, 74.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using parameters for domain 'technology':\n",
      "  - base_threshold: 0.090\n",
      "  - quality_threshold: 0.420\n",
      "  - percentile: 5\n",
      "  - target_count: 10\n",
      "Using domain 'technology' with base threshold: 0.090, quality threshold: 0.420\n",
      "Using 5th percentile for domain 'technology'\n",
      "- Using 5th percentile from top: 0.824\n",
      "\n",
      "Quality Filtering Stats:\n",
      "- Domain: technology\n",
      "- Text Length: 351 words\n",
      "- Content Density: 0.69\n",
      "- Initial Candidates: 17\n",
      "- Base Threshold: 0.090\n",
      "- Quality Threshold: 0.350\n",
      "- After Quality Filter: 10\n",
      "Using target count 10 for domain 'technology'\n",
      "- Final keyphrase count: 10\n",
      "- Semantic diversity score: 0.70\n",
      "Using domain 'technology' with quality threshold: 0.55\n",
      "Title: Navigating the Ethical Minefield: AI Development Demands Responsible Practices\n",
      "Domain: artificial intelligence\n",
      "Extracted 8 keyphrases:\n",
      "- Artificial Intelligence: 0.83\n",
      "- As Artificial Intelligence: 0.82\n",
      "- AI system: 0.76\n",
      "- privacy: 0.68\n",
      "- fairness: 0.60\n",
      "- data: 0.58\n",
      "- algorithms: 0.43\n",
      "- transparency: 0.42\n",
      "\n",
      "\n",
      "Processing article 3/21 (artificial intelligence)...\n",
      "\n",
      "Zero-Shot Domain Detection Results:\n",
      "----------------------------------------\n",
      "Top domain: artificial intelligence (Score: 0.4398)\n",
      "  1. artificial intelligence: 0.4398\n",
      "  2. technology: 0.4193\n",
      "  3. education: 0.1238\n",
      "  4. business: 0.0076\n",
      "  5. science: 0.0064\n",
      "----------------------------------------\n",
      "Threshold for 'artificial intelligence': 0.35\n",
      "Detected domain: artificial intelligence\n",
      "\n",
      "Zero-Shot Domain Detection Results:\n",
      "----------------------------------------\n",
      "Top domain: artificial intelligence (Score: 0.4489)\n",
      "  1. artificial intelligence: 0.4489\n",
      "  2. technology: 0.4035\n",
      "  3. education: 0.1388\n",
      "  4. science: 0.0063\n",
      "  5. world: 0.0013\n",
      "----------------------------------------\n",
      "Threshold for 'artificial intelligence': 0.35\n",
      "Extracted 2 named entities\n",
      "Generated 14 candidate keyphrases\n",
      "Semantic diversity: 0.758 → 0.758\n",
      "Keyphrase count: 14 → 14\n",
      "\n",
      "Using parameters for domain 'artificial intelligence':\n",
      "  - base_threshold: 0.070\n",
      "  - quality_threshold: 0.420\n",
      "  - percentile: 4\n",
      "  - target_count: 10\n",
      "Using domain 'artificial intelligence' with base threshold: 0.070, quality threshold: 0.420\n",
      "Using 4th percentile for domain 'artificial intelligence'\n",
      "- Using 5th percentile from top: 0.836\n",
      "\n",
      "Quality Filtering Stats:\n",
      "- Domain: artificial intelligence\n",
      "- Text Length: 343 words\n",
      "- Content Density: 0.73\n",
      "- Initial Candidates: 10\n",
      "- Base Threshold: 0.070\n",
      "- Quality Threshold: 0.350\n",
      "- After Quality Filter: 6\n",
      "Using target count 10 for domain 'artificial intelligence'\n",
      "- Quality filter too strict, falling back to top 10 keyphrases\n",
      "- Falling back to top 10 keyphrases by score\n",
      "- Final keyphrase count: 10\n",
      "- Semantic diversity score: 0.77\n",
      "DEBUG: Extracting keyphrases with use_position_weight=True, use_tfidf_weight=True, use_ensemble=True\n",
      "DEBUG: KeyBERT extracted 87 keyphrases\n",
      "DEBUG: MultipartiteRank position weight for 'LLMs': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'ability': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'research creative fields': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'capability': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'challenge': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'like text': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'human': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'natural language': 2.00\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'LLMs': 2.00\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'ability': 1.35\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'research creative fields': 1.41\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'capability': 0.50\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'challenge': 0.50\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'like text': 1.51\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'human': 1.03\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'natural language': 1.73\n",
      "DEBUG: MultipartiteRank extracted 8 keyphrases\n",
      "DEBUG: YAKE position weight for 'language models llms': 2.00\n",
      "DEBUG: YAKE position weight for 'models llms continue': 2.00\n",
      "DEBUG: YAKE position weight for 'large language models': 2.00\n",
      "DEBUG: YAKE position weight for 'llm large language': 2.00\n",
      "DEBUG: YAKE position weight for 'creative industries llms': 2.00\n",
      "DEBUG: YAKE position weight for '2025 large language': 2.00\n",
      "DEBUG: YAKE position weight for 'art llms raise': 2.00\n",
      "DEBUG: YAKE position weight for 'industries llms assist': 2.00\n",
      "DEBUG: YAKE position weight for 'llms raise environmental': 2.00\n",
      "DEBUG: YAKE position weight for 'assessing llm large': 2.00\n",
      "DEBUG: YAKE TF-IDF weight for 'language models llms': 2.00\n",
      "DEBUG: YAKE TF-IDF weight for 'models llms continue': 1.62\n",
      "DEBUG: YAKE TF-IDF weight for 'large language models': 1.87\n",
      "DEBUG: YAKE TF-IDF weight for 'llm large language': 0.91\n",
      "DEBUG: YAKE TF-IDF weight for 'creative industries llms': 1.15\n",
      "DEBUG: YAKE TF-IDF weight for '2025 large language': 1.20\n",
      "DEBUG: YAKE TF-IDF weight for 'art llms raise': 0.50\n",
      "DEBUG: YAKE TF-IDF weight for 'industries llms assist': 0.62\n",
      "DEBUG: YAKE TF-IDF weight for 'llms raise environmental': 0.93\n",
      "DEBUG: YAKE TF-IDF weight for 'assessing llm large': 0.52\n",
      "DEBUG: YAKE extracted 10 keyphrases\n",
      "DEBUG: TextRank position weight for 'llm large language model capabilities': 2.00\n",
      "DEBUG: TextRank position weight for 'large language models llms': 2.00\n",
      "DEBUG: TextRank position weight for 'capabilities models released': 2.00\n",
      "DEBUG: TextRank position weight for 'natural language processing': 2.00\n",
      "DEBUG: TextRank position weight for 'business llms': 2.00\n",
      "DEBUG: TextRank position weight for 'code generation customer service': 2.00\n",
      "DEBUG: TextRank position weight for 'research creative fields': 2.00\n",
      "DEBUG: TextRank position weight for 'ai artificial intelligence systems': 2.00\n",
      "DEBUG: TextRank position weight for 'concerns ensuring factual reliability': 2.00\n",
      "DEBUG: TextRank position weight for 'llms': 2.00\n",
      "DEBUG: TextRank TF-IDF weight for 'llm large language model capabilities': 1.08\n",
      "DEBUG: TextRank TF-IDF weight for 'large language models llms': 2.00\n",
      "DEBUG: TextRank TF-IDF weight for 'capabilities models released': 1.02\n",
      "DEBUG: TextRank TF-IDF weight for 'natural language processing': 1.04\n",
      "DEBUG: TextRank TF-IDF weight for 'business llms': 1.28\n",
      "DEBUG: TextRank TF-IDF weight for 'code generation customer service': 0.50\n",
      "DEBUG: TextRank TF-IDF weight for 'research creative fields': 1.01\n",
      "DEBUG: TextRank TF-IDF weight for 'ai artificial intelligence systems': 1.27\n",
      "DEBUG: TextRank TF-IDF weight for 'concerns ensuring factual reliability': 0.52\n",
      "DEBUG: TextRank TF-IDF weight for 'llms': 1.57\n",
      "DEBUG: TextRank extracted 10 keyphrases\n",
      "DEBUG: Raw domain score for sports: 10.00\n",
      "DEBUG: Top domains detected: technology: 0.2370, business: 0.1524, entertainment: 0.1330\n",
      "DEBUG: All domain scores: {'technology': 0.2370012091898428, 'business': 0.1523579201934704, 'health': 0.014510278113663845, 'science': 0.094316807738815, 'news': 0.10519951632406288, 'academic': 0.11608222490931076, 'politics': 0.05562273276904474, 'environment': 0.04353083434099154, 'entertainment': 0.13301088270858524, 'sports': 0.04836759371221282}\n",
      "DEBUG: Adaptive method weights:\n",
      "DEBUG:   - keybert: 0.3172\n",
      "DEBUG:   - multipartiterank: 0.3210\n",
      "DEBUG:   - yake: 0.1006\n",
      "DEBUG:   - textrank: 0.2613\n",
      "DEBUG: Length bonus for 'large Language Models' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'large language model' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'rapid evolution' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'significant milestone' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'simple chatbots' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'recent advancement' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'contextual understanding' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'past year' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'complex task' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'research creative field' (3 words): 1.30x\n",
      "DEBUG: Length bonus for '2025 large language' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'ai artificial intelligence system' (4 words): 1.45x\n",
      "DEBUG: Length bonus for 'natural language' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'text image' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'even audio' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'new benchmark' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'various software' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'narrative suggestion' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'human interaction' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'complex system' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'natural language prompt' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'intricate instruction' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'natural language processing' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'business llm' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'textual description' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'visual content' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'data analysis' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'vast amount' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'tech giant' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'specific domain' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'progress challenge' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'multimodal capability' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'improve performance' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'good factual accuracy' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'creative industry' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'increase fidelity' (2 words): 1.15x\n",
      "DEBUG: Length bonus for '- art' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'environmental concern' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'factual reliability' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'inherent bias' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'ethical consideration' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'ongoing area' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'furthermore development' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'robust method' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'daily life' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'potential benefit' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'key focus' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'researcher developer' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'come year' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'language model llm' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'model llm continue' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'llm large language' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'creative industry llm' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'art llm raise' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'industry llm assist' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'llm raise environmental' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'assess llm large' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'llm large language model capability' (5 words): 1.60x\n",
      "DEBUG: Length bonus for 'large language model llm' (4 words): 1.45x\n",
      "DEBUG: Length bonus for 'capability model release' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'code generation customer service' (4 words): 1.45x\n",
      "DEBUG: Length bonus for 'concern ensure factual reliability' (4 words): 1.45x\n",
      "DEBUG: Ensemble combined into 98 keyphrases\n",
      "DEBUG: After redundancy removal: 72 keyphrases\n",
      "DEBUG: Before post-filtering: 72 keyphrases\n",
      "DEBUG: Multi-word phrases: 40\n",
      "DEBUG: Filtered out single word 'research' as it's part of multi-word phrase(s)\n",
      "DEBUG: Filtered out single word 'ability' as it's part of multi-word phrase(s)\n",
      "DEBUG: Filtered out single word 'text' as it's part of multi-word phrase(s)\n",
      "DEBUG: Filtered out single word 'model' as it's part of multi-word phrase(s)\n",
      "DEBUG: After post-filtering: 68 keyphrases\n",
      "DEBUG: Final top 10 keyphrases: ['data', 'progress', 'accuracy', 'furthermore', 'recent', 'integration', 'healthcare', 'misinformation', 'misuse', 'debate']\n",
      "\n",
      "Zero-Shot Domain Detection Results:\n",
      "----------------------------------------\n",
      "Top domain: artificial intelligence (Score: 0.4398)\n",
      "  1. artificial intelligence: 0.4398\n",
      "  2. technology: 0.4193\n",
      "  3. education: 0.1238\n",
      "  4. business: 0.0076\n",
      "  5. science: 0.0064\n",
      "----------------------------------------\n",
      "Threshold for 'artificial intelligence': 0.35\n",
      "Semantic diversity: 0.800 → 0.800\n",
      "Keyphrase count: 16 → 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing articles:  14%|█▍        | 3/21 [03:31<19:52, 66.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generic term filtering: 16 -> 13 keyphrases (3 removed)\n",
      "\n",
      "Filtered out generic terms (examples):\n",
      "- accuracy (0.34): exact generic match\n",
      "- recent (0.11): exact generic match\n",
      "- furthermore (0.08): exact generic match\n",
      "\n",
      "Using parameters for domain 'artificial intelligence':\n",
      "  - base_threshold: 0.070\n",
      "  - quality_threshold: 0.420\n",
      "  - percentile: 4\n",
      "  - target_count: 10\n",
      "Using domain 'artificial intelligence' with base threshold: 0.070, quality threshold: 0.420\n",
      "Using 4th percentile for domain 'artificial intelligence'\n",
      "- Using 5th percentile from top: 0.830\n",
      "\n",
      "Quality Filtering Stats:\n",
      "- Domain: artificial intelligence\n",
      "- Text Length: 343 words\n",
      "- Content Density: 0.73\n",
      "- Initial Candidates: 13\n",
      "- Base Threshold: 0.070\n",
      "- Quality Threshold: 0.350\n",
      "- After Quality Filter: 6\n",
      "Using target count 10 for domain 'artificial intelligence'\n",
      "- Quality filter too strict, falling back to top 10 keyphrases\n",
      "- Falling back to top 10 keyphrases by score\n",
      "- Final keyphrase count: 10\n",
      "- Semantic diversity score: 0.78\n",
      "Using domain 'artificial intelligence' with quality threshold: 0.60\n",
      "Title: Large Language Models Reach New Milestones, Pushing Boundaries of AI Interaction\n",
      "Domain: artificial intelligence\n",
      "Extracted 8 keyphrases:\n",
      "- Large Language Models: 0.84\n",
      "- Language Models: 0.82\n",
      "- chatbots: 0.82\n",
      "- AI system: 0.80\n",
      "- LLMs: 0.56\n",
      "- text: 0.52\n",
      "- progress: 0.33\n",
      "- data: 0.28\n",
      "\n",
      "\n",
      "Processing article 4/21 (automotive)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Zero-Shot Domain Detection Results:\n",
      "----------------------------------------\n",
      "Top domain: technology (Score: 0.3408)\n",
      "  1. technology: 0.3408\n",
      "  2. science: 0.2316\n",
      "  3. automotive: 0.2293\n",
      "  4. education: 0.1795\n",
      "  5. business: 0.0132\n",
      "----------------------------------------\n",
      "Threshold for 'technology': 0.45\n",
      "Low confidence (0.3408) for domain 'technology'.\n",
      "Score is above 0.25, using detected domain despite being below threshold\n",
      "Detected domain: technology\n",
      "Domain detected using BART: technology (confidence: 0.5093)\n",
      "Extracted 3 named entities\n",
      "Generated 17 candidate keyphrases\n",
      "Semantic diversity: 0.750 → 0.750\n",
      "Keyphrase count: 17 → 17\n",
      "\n",
      "Using parameters for domain 'technology':\n",
      "  - base_threshold: 0.090\n",
      "  - quality_threshold: 0.420\n",
      "  - percentile: 5\n",
      "  - target_count: 10\n",
      "Using domain 'technology' with base threshold: 0.090, quality threshold: 0.420\n",
      "Using 5th percentile for domain 'technology'\n",
      "- Using 5th percentile from top: 0.829\n",
      "\n",
      "Quality Filtering Stats:\n",
      "- Domain: technology\n",
      "- Text Length: 365 words\n",
      "- Content Density: 0.69\n",
      "- Initial Candidates: 13\n",
      "- Base Threshold: 0.090\n",
      "- Quality Threshold: 0.350\n",
      "- After Quality Filter: 5\n",
      "Using target count 10 for domain 'technology'\n",
      "- Quality filter too strict, falling back to top 10 keyphrases\n",
      "- Using lenient threshold 0.175: 10 keyphrases\n",
      "- Final keyphrase count: 10\n",
      "- Semantic diversity score: 0.71\n",
      "DEBUG: Extracting keyphrases with use_position_weight=True, use_tfidf_weight=True, use_ensemble=True\n",
      "DEBUG: KeyBERT extracted 80 keyphrases\n",
      "DEBUG: MultipartiteRank position weight for 'electric vehicles evs': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'state batteries': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'prototype solid': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'safety': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'capabilities korean researchers': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'solid material': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'advancements': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'longer range': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'significant innovation': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'mile evs': 2.00\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'electric vehicles evs': 2.00\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'state batteries': 1.96\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'prototype solid': 1.31\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'safety': 1.00\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'capabilities korean researchers': 1.63\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'solid material': 1.31\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'advancements': 1.43\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'longer range': 1.66\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'significant innovation': 0.50\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'mile evs': 1.01\n",
      "DEBUG: MultipartiteRank extracted 10 keyphrases\n",
      "DEBUG: YAKE position weight for 'manganese iron phosphate': 2.00\n",
      "DEBUG: YAKE position weight for 'iron phosphate lmfp': 2.00\n",
      "DEBUG: YAKE position weight for 'chinese oem byd': 2.00\n",
      "DEBUG: YAKE position weight for 'lithium manganese iron': 2.00\n",
      "DEBUG: YAKE position weight for 'benz leveraging formula': 2.00\n",
      "DEBUG: YAKE position weight for 'rapidly chinese oem': 2.00\n",
      "DEBUG: YAKE position weight for 'oem byd unveiled': 2.00\n",
      "DEBUG: YAKE position weight for 'evs daily efforts': 2.00\n",
      "DEBUG: YAKE position weight for 'capabilities korean researchers': 2.00\n",
      "DEBUG: YAKE position weight for 'minutes furthermore researchers': 2.00\n",
      "DEBUG: YAKE TF-IDF weight for 'manganese iron phosphate': 0.50\n",
      "DEBUG: YAKE TF-IDF weight for 'iron phosphate lmfp': 0.50\n",
      "DEBUG: YAKE TF-IDF weight for 'chinese oem byd': 0.50\n",
      "DEBUG: YAKE TF-IDF weight for 'lithium manganese iron': 1.36\n",
      "DEBUG: YAKE TF-IDF weight for 'benz leveraging formula': 0.50\n",
      "DEBUG: YAKE TF-IDF weight for 'rapidly chinese oem': 0.50\n",
      "DEBUG: YAKE TF-IDF weight for 'oem byd unveiled': 0.50\n",
      "DEBUG: YAKE TF-IDF weight for 'evs daily efforts': 1.37\n",
      "DEBUG: YAKE TF-IDF weight for 'capabilities korean researchers': 2.00\n",
      "DEBUG: YAKE TF-IDF weight for 'minutes furthermore researchers': 2.00\n",
      "DEBUG: YAKE extracted 10 keyphrases\n",
      "DEBUG: TextRank position weight for 'develop ev electric vehicle batteries': 2.00\n",
      "DEBUG: TextRank position weight for 'ev electric vehicle batteries': 2.00\n",
      "DEBUG: TextRank position weight for 'ev electric vehicle range': 2.00\n",
      "DEBUG: TextRank position weight for 'ev electric vehicle': 2.00\n",
      "DEBUG: TextRank position weight for 'electric vehicles evs': 2.00\n",
      "DEBUG: TextRank position weight for 'developed battery chemistries capable': 2.00\n",
      "DEBUG: TextRank position weight for 'mile evs improved safety': 2.00\n",
      "DEBUG: TextRank position weight for 'charging capabilities': 2.00\n",
      "DEBUG: TextRank position weight for 'several key breakthroughs announced': 2.00\n",
      "DEBUG: TextRank position weight for 'state batteries': 2.00\n",
      "DEBUG: TextRank TF-IDF weight for 'develop ev electric vehicle batteries': 1.70\n",
      "DEBUG: TextRank TF-IDF weight for 'ev electric vehicle batteries': 2.00\n",
      "DEBUG: TextRank TF-IDF weight for 'ev electric vehicle range': 1.84\n",
      "DEBUG: TextRank TF-IDF weight for 'ev electric vehicle': 1.87\n",
      "DEBUG: TextRank TF-IDF weight for 'electric vehicles evs': 1.12\n",
      "DEBUG: TextRank TF-IDF weight for 'developed battery chemistries capable': 1.23\n",
      "DEBUG: TextRank TF-IDF weight for 'mile evs improved safety': 0.50\n",
      "DEBUG: TextRank TF-IDF weight for 'charging capabilities': 1.17\n",
      "DEBUG: TextRank TF-IDF weight for 'several key breakthroughs announced': 1.07\n",
      "DEBUG: TextRank TF-IDF weight for 'state batteries': 1.09\n",
      "DEBUG: TextRank extracted 10 keyphrases\n",
      "DEBUG: Raw domain score for sports: 8.50\n",
      "DEBUG: Top domains detected: environment: 0.2075, news: 0.1992, technology: 0.1660\n",
      "DEBUG: All domain scores: {'technology': 0.16597510373443983, 'business': 0.012448132780082987, 'health': 0.04979253112033195, 'science': 0.0912863070539419, 'news': 0.1991701244813278, 'academic': 0.08713692946058091, 'politics': 0.012448132780082987, 'environment': 0.2074688796680498, 'entertainment': 0.1037344398340249, 'sports': 0.07053941908713693}\n",
      "DEBUG: Adaptive method weights:\n",
      "DEBUG:   - keybert: 0.2984\n",
      "DEBUG:   - multipartiterank: 0.3113\n",
      "DEBUG:   - yake: 0.1072\n",
      "DEBUG:   - textrank: 0.2830\n",
      "DEBUG: Length bonus for 'ev electric vehicle' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'solid material' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'longer range' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'significant innovation' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'electric vehicle' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'battery technology' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'several key breakthrough' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'major hurdle' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'major focus' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'mercedes - benz' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'formula 1 tech' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'commercial availability' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'next few year' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'liquid electrolyte' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'prototype solid' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'fire risk' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'korean researcher' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'solid electrolyte' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'manufacturing cost' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'novel cathode material' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'energy density' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'lithium manganese iron' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'ev electric vehicle range' (4 words): 1.45x\n",
      "DEBUG: Length bonus for 'manganese iron phosphate' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'iron phosphate lmfp' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'capabilities korean researcher' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'mile evs' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'chinese oem byd' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'dalhousie university' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'remarkable durability' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'silicon content' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'just five minute' (3 words): 1.30x\n",
      "DEBUG: Length bonus for '10 minute' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'several company' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'high energy density' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'exist material' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'speed charge technology' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'battery chemistry' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'key pain point' (3 words): 1.30x\n",
      "DEBUG: Length bonus for '500 kwh capacity' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'multiple ev' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'daily effort' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'combine advancement' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'electric vehicle evs' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'state battery' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'benz leverage formula' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'rapidly chinese oem' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'oem byd unveil' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'evs daily effort' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'minute researcher' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'develop ev electric vehicle battery' (5 words): 1.60x\n",
      "DEBUG: Length bonus for 'ev electric vehicle battery' (4 words): 1.45x\n",
      "DEBUG: Length bonus for 'develop battery chemistry capable' (4 words): 1.45x\n",
      "DEBUG: Length bonus for 'mile ev improve safety' (4 words): 1.45x\n",
      "DEBUG: Length bonus for 'charge capability' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'several key breakthrough announce' (4 words): 1.45x\n",
      "DEBUG: Ensemble combined into 97 keyphrases\n",
      "DEBUG: After redundancy removal: 62 keyphrases\n",
      "DEBUG: Before post-filtering: 62 keyphrases\n",
      "DEBUG: Multi-word phrases: 27\n",
      "DEBUG: Filtered out single word 'research' as it's part of multi-word phrase(s)\n",
      "DEBUG: After post-filtering: 61 keyphrases\n",
      "DEBUG: Final top 10 keyphrases: ['solid', 'several', 'battery', 'evs', 'chinese', 'furthermore', 'korean', 'safety', 'stationary', 'formula']\n",
      "\n",
      "Zero-Shot Domain Detection Results:\n",
      "----------------------------------------\n",
      "Top domain: technology (Score: 0.3408)\n",
      "  1. technology: 0.3408\n",
      "  2. science: 0.2316\n",
      "  3. automotive: 0.2293\n",
      "  4. education: 0.1795\n",
      "  5. business: 0.0132\n",
      "----------------------------------------\n",
      "Threshold for 'technology': 0.45\n",
      "Low confidence (0.3408) for domain 'technology'.\n",
      "Score is above 0.25, using detected domain despite being below threshold\n",
      "Semantic diversity: 0.791 → 0.791\n",
      "Keyphrase count: 16 → 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing articles:  19%|█▉        | 4/21 [04:28<17:44, 62.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generic term filtering: 14 -> 12 keyphrases (2 removed)\n",
      "\n",
      "Filtered out generic terms (examples):\n",
      "- furthermore (0.01): exact generic match\n",
      "- several (0.00): exact generic match\n",
      "\n",
      "Using parameters for domain 'technology':\n",
      "  - base_threshold: 0.090\n",
      "  - quality_threshold: 0.420\n",
      "  - percentile: 5\n",
      "  - target_count: 10\n",
      "Using domain 'technology' with base threshold: 0.090, quality threshold: 0.420\n",
      "Using 5th percentile for domain 'technology'\n",
      "- Using 5th percentile from top: 0.825\n",
      "\n",
      "Quality Filtering Stats:\n",
      "- Domain: technology\n",
      "- Text Length: 365 words\n",
      "- Content Density: 0.69\n",
      "- Initial Candidates: 12\n",
      "- Base Threshold: 0.090\n",
      "- Quality Threshold: 0.350\n",
      "- After Quality Filter: 5\n",
      "Using target count 10 for domain 'technology'\n",
      "- Quality filter too strict, falling back to top 10 keyphrases\n",
      "- Falling back to top 10 keyphrases by score\n",
      "- Final keyphrase count: 10\n",
      "- Semantic diversity score: 0.76\n",
      "Using domain 'technology' with quality threshold: 0.55\n",
      "Title: Electric Vehicle Battery Technology Sees Major Breakthroughs in Range and Charging\n",
      "Domain: automotive\n",
      "Extracted 8 keyphrases:\n",
      "- battery technology: 0.83\n",
      "- wider EV adoption: 0.82\n",
      "- charging: 0.71\n",
      "- evs: 0.55\n",
      "- innovation: 0.35\n",
      "- Mercedes-Benz: 0.31\n",
      "- Nissan: 0.29\n",
      "- solid: 0.18\n",
      "\n",
      "\n",
      "Processing article 5/21 (automotive)...\n",
      "Domain detected using BART: automotive (confidence: 0.5654)\n",
      "Detected domain: automotive\n",
      "Domain detected using BART: automotive (confidence: 0.6715)\n",
      "Extracted 4 named entities\n",
      "Generated 10 candidate keyphrases\n",
      "Semantic diversity: 0.729 → 0.729\n",
      "Keyphrase count: 10 → 10\n",
      "\n",
      "Using parameters for domain 'automotive':\n",
      "  - base_threshold: 0.050\n",
      "  - quality_threshold: 0.350\n",
      "  - percentile: 3\n",
      "  - target_count: 9\n",
      "Using domain 'automotive' with base threshold: 0.050, quality threshold: 0.350\n",
      "Using 3th percentile for domain 'automotive'\n",
      "- Using 5th percentile from top: 0.432\n",
      "\n",
      "Quality Filtering Stats:\n",
      "- Domain: automotive\n",
      "- Text Length: 367 words\n",
      "- Content Density: 0.71\n",
      "- Initial Candidates: 6\n",
      "- Base Threshold: 0.050\n",
      "- Quality Threshold: 0.350\n",
      "- After Quality Filter: 1\n",
      "Using target count 9 for domain 'automotive'\n",
      "- Final keyphrase count: 1\n",
      "DEBUG: Extracting keyphrases with use_position_weight=True, use_tfidf_weight=True, use_ensemble=True\n",
      "DEBUG: KeyBERT extracted 94 keyphrases\n",
      "DEBUG: MultipartiteRank position weight for 'component costs': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'High vehicle prices': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'electric vehicles evs': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'challenges': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'demand': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'supply chains': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'immense manufacturers': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'major markets': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'electrification': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'substantial risk': 2.00\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'component costs': 1.44\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'High vehicle prices': 0.99\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'electric vehicles evs': 1.38\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'challenges': 1.92\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'demand': 1.07\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'supply chains': 2.00\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'immense manufacturers': 1.08\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'major markets': 1.06\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'electrification': 1.07\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'substantial risk': 0.50\n",
      "DEBUG: MultipartiteRank extracted 10 keyphrases\n",
      "DEBUG: YAKE position weight for 'established oems like': 2.00\n",
      "DEBUG: YAKE position weight for 'oems like stellantis': 2.00\n",
      "DEBUG: YAKE position weight for 'middle east conflicts': 2.00\n",
      "DEBUG: YAKE position weight for 'batteries established oems': 2.00\n",
      "DEBUG: YAKE position weight for 'geopolitical events enhancing': 2.00\n",
      "DEBUG: YAKE position weight for 'electric vehicles evs': 2.00\n",
      "DEBUG: YAKE position weight for 'manufacturers market growth': 2.00\n",
      "DEBUG: YAKE position weight for 'advantaged chinese manufacturers': 2.00\n",
      "DEBUG: YAKE position weight for 'electric vehicle supply': 2.00\n",
      "DEBUG: YAKE position weight for 'vehicle supply chain': 2.00\n",
      "DEBUG: YAKE TF-IDF weight for 'established oems like': 1.01\n",
      "DEBUG: YAKE TF-IDF weight for 'oems like stellantis': 0.77\n",
      "DEBUG: YAKE TF-IDF weight for 'middle east conflicts': 0.50\n",
      "DEBUG: YAKE TF-IDF weight for 'batteries established oems': 0.57\n",
      "DEBUG: YAKE TF-IDF weight for 'geopolitical events enhancing': 0.70\n",
      "DEBUG: YAKE TF-IDF weight for 'electric vehicles evs': 1.03\n",
      "DEBUG: YAKE TF-IDF weight for 'manufacturers market growth': 0.98\n",
      "DEBUG: YAKE TF-IDF weight for 'advantaged chinese manufacturers': 0.82\n",
      "DEBUG: YAKE TF-IDF weight for 'electric vehicle supply': 1.91\n",
      "DEBUG: YAKE TF-IDF weight for 'vehicle supply chain': 2.00\n",
      "DEBUG: YAKE extracted 10 keyphrases\n",
      "DEBUG: TextRank position weight for 'economic pressures regulatory shifts supply chain vulnerabilities': 2.00\n",
      "DEBUG: TextRank position weight for 'ev electric vehicle supply chain': 2.00\n",
      "DEBUG: TextRank position weight for 'costs supply chain': 2.00\n",
      "DEBUG: TextRank position weight for 'risk potentially increasing component costs': 2.00\n",
      "DEBUG: TextRank position weight for 'complex global supply': 2.00\n",
      "DEBUG: TextRank position weight for 'supply chains': 2.00\n",
      "DEBUG: TextRank position weight for 'ev electric vehicle': 2.00\n",
      "DEBUG: TextRank position weight for 'electric vehicles evs': 2.00\n",
      "DEBUG: TextRank position weight for 'challenges requires significant': 2.00\n",
      "DEBUG: TextRank position weight for 'emission technologies adding pressure': 2.00\n",
      "DEBUG: TextRank TF-IDF weight for 'economic pressures regulatory shifts supply chain vulnerabilities': 1.75\n",
      "DEBUG: TextRank TF-IDF weight for 'ev electric vehicle supply chain': 2.00\n",
      "DEBUG: TextRank TF-IDF weight for 'costs supply chain': 1.86\n",
      "DEBUG: TextRank TF-IDF weight for 'risk potentially increasing component costs': 0.81\n",
      "DEBUG: TextRank TF-IDF weight for 'complex global supply': 1.63\n",
      "DEBUG: TextRank TF-IDF weight for 'supply chains': 1.18\n",
      "DEBUG: TextRank TF-IDF weight for 'ev electric vehicle': 0.99\n",
      "DEBUG: TextRank TF-IDF weight for 'electric vehicles evs': 0.59\n",
      "DEBUG: TextRank TF-IDF weight for 'challenges requires significant': 1.14\n",
      "DEBUG: TextRank TF-IDF weight for 'emission technologies adding pressure': 0.50\n",
      "DEBUG: TextRank extracted 10 keyphrases\n",
      "DEBUG: Raw domain score for sports: 15.00\n",
      "DEBUG: Top domains detected: business: 0.4012, technology: 0.1914, sports: 0.0926\n",
      "DEBUG: All domain scores: {'technology': 0.19135802469135801, 'business': 0.4012345679012346, 'health': 0.0, 'science': 0.08950617283950617, 'news': 0.05864197530864197, 'academic': 0.033950617283950615, 'politics': 0.08333333333333333, 'environment': 0.009259259259259259, 'entertainment': 0.040123456790123455, 'sports': 0.09259259259259259}\n",
      "DEBUG: Adaptive method weights:\n",
      "DEBUG:   - keybert: 0.2747\n",
      "DEBUG:   - multipartiterank: 0.3128\n",
      "DEBUG:   - yake: 0.1712\n",
      "DEBUG:   - textrank: 0.2412\n",
      "DEBUG: Length bonus for 'supply chain' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'major market' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'global automotive industry' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'mid - decade' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'complex mix' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'rapid technological evolution' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'autonomous driving' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'pace economic pressure' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'significant headwind' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'potential tariff increase' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'us eu' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'high vehicle price' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'Middle East' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'middle east conflict' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'component cost' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'immense manufacturer' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'substantial risk' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'consumer demand' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'component availability' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'full electrification' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'regulatory uncertainty' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'ev electric vehicle' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'rapid shift' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'significant advantage' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'labor cost' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'critical part' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'joint venture' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'supply chain resilience' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'ev electric vehicle supply chain' (5 words): 1.60x\n",
      "DEBUG: Length bonus for 'critical concern' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'heighten competition' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'rise consumer debt' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'broad economic uncertainty' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'car geopolitical tension' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'electric vehicle evs' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'emerge market' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'require investment' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'ice hybrid technology' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'establish oems' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'manufacture efficiency' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'industry 's reliance' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'semiconductor shortage' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'geopolitical event' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'supply tier' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'key strategy' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'modern vehicle' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'cybersecurity risk' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'remote attack' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'robust security measure' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'vehicle lifecycle' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'multifaceted challenge' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'oems stellantis' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'battery establish oems' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'geopolitical event enhance' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'manufacturer market growth' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'advantage chinese manufacturer' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'electric vehicle supply' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'vehicle supply chain' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'economic pressure regulatory shift supply chain vulnerability' (7 words): 1.60x\n",
      "DEBUG: Length bonus for 'cost supply chain' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'risk potentially increase component cost' (5 words): 1.60x\n",
      "DEBUG: Length bonus for 'complex global supply' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'challenge require significant' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'emission technology add pressure' (4 words): 1.45x\n",
      "DEBUG: Ensemble combined into 109 keyphrases\n",
      "DEBUG: After redundancy removal: 73 keyphrases\n",
      "DEBUG: Before post-filtering: 73 keyphrases\n",
      "DEBUG: Multi-word phrases: 29\n",
      "DEBUG: After post-filtering: 73 keyphrases\n",
      "DEBUG: Final top 10 keyphrases: ['supply', 'market', 'investment', 'vehicle', 'geopolitical', 'competition', 'cost', 'innovation', 'chinese', 'furthermore']\n",
      "Domain detected using BART: automotive (confidence: 0.5654)\n",
      "Semantic diversity: 0.730 → 0.730\n",
      "Keyphrase count: 11 → 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing articles:  24%|██▍       | 5/21 [05:31<16:46, 62.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generic term filtering: 11 -> 9 keyphrases (2 removed)\n",
      "\n",
      "Filtered out generic terms (examples):\n",
      "- market (0.22): exact generic match\n",
      "- furthermore (0.02): exact generic match\n",
      "\n",
      "Using parameters for domain 'automotive':\n",
      "  - base_threshold: 0.050\n",
      "  - quality_threshold: 0.350\n",
      "  - percentile: 3\n",
      "  - target_count: 9\n",
      "Using domain 'automotive' with base threshold: 0.050, quality threshold: 0.350\n",
      "Using 3th percentile for domain 'automotive'\n",
      "- Using 5th percentile from top: 0.515\n",
      "\n",
      "Quality Filtering Stats:\n",
      "- Domain: automotive\n",
      "- Text Length: 367 words\n",
      "- Content Density: 0.71\n",
      "- Initial Candidates: 9\n",
      "- Base Threshold: 0.050\n",
      "- Quality Threshold: 0.350\n",
      "- After Quality Filter: 6\n",
      "Using target count 9 for domain 'automotive'\n",
      "- Quality filter too strict, falling back to top 9 keyphrases\n",
      "- Falling back to top 9 keyphrases by score\n",
      "- Final keyphrase count: 9\n",
      "- Semantic diversity score: 0.71\n",
      "Using domain 'automotive' with quality threshold: 0.50\n",
      "Title: Automotive Industry Faces Headwinds in 2025 Despite Technological Strides\n",
      "Domain: automotive\n",
      "Extracted 8 keyphrases:\n",
      "- supply: 0.52\n",
      "- innovation: 0.51\n",
      "- industry enters: 0.49\n",
      "- vehicle: 0.46\n",
      "- competition: 0.44\n",
      "- geopolitical: 0.36\n",
      "- investment: 0.25\n",
      "- cost: 0.23\n",
      "\n",
      "\n",
      "Processing article 6/21 (automotive)...\n",
      "\n",
      "Zero-Shot Domain Detection Results:\n",
      "----------------------------------------\n",
      "Top domain: technology (Score: 0.3366)\n",
      "  1. technology: 0.3366\n",
      "  2. automotive: 0.2912\n",
      "  3. artificial intelligence: 0.1859\n",
      "  4. business: 0.0961\n",
      "  5. travel: 0.0796\n",
      "----------------------------------------\n",
      "Threshold for 'technology': 0.45\n",
      "Low confidence (0.3366) for domain 'technology'.\n",
      "Score is above 0.25, using detected domain despite being below threshold\n",
      "Detected domain: technology\n",
      "Domain detected using BART: technology (confidence: 0.5367)\n",
      "Extracted 2 named entities\n",
      "Generated 26 candidate keyphrases\n",
      "Semantic diversity: 0.815 → 0.815\n",
      "Keyphrase count: 26 → 26\n",
      "\n",
      "Generic term filtering: 16 -> 15 keyphrases (1 removed)\n",
      "\n",
      "Filtered out generic terms (examples):\n",
      "- development (0.21): exact generic match\n",
      "\n",
      "Using parameters for domain 'technology':\n",
      "  - base_threshold: 0.090\n",
      "  - quality_threshold: 0.420\n",
      "  - percentile: 5\n",
      "  - target_count: 10\n",
      "Using domain 'technology' with base threshold: 0.090, quality threshold: 0.420\n",
      "Using 5th percentile for domain 'technology'\n",
      "- Using 5th percentile from top: 0.657\n",
      "\n",
      "Quality Filtering Stats:\n",
      "- Domain: technology\n",
      "- Text Length: 382 words\n",
      "- Content Density: 0.69\n",
      "- Initial Candidates: 15\n",
      "- Base Threshold: 0.090\n",
      "- Quality Threshold: 0.350\n",
      "- After Quality Filter: 6\n",
      "Using target count 10 for domain 'technology'\n",
      "- Quality filter too strict, falling back to top 10 keyphrases\n",
      "- Using lenient threshold 0.175: 12 keyphrases\n",
      "- Final keyphrase count: 12\n",
      "- Semantic diversity score: 0.80\n",
      "DEBUG: Extracting keyphrases with use_position_weight=True, use_tfidf_weight=True, use_ensemble=True\n",
      "DEBUG: KeyBERT extracted 95 keyphrases\n",
      "DEBUG: MultipartiteRank position weight for 'AD systems': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'data': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'vehicles': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'safety': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'Level': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'conditional automation': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'world driving': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'software': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'development': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'autonomous': 2.00\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'AD systems': 1.65\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'data': 2.00\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'vehicles': 1.25\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'safety': 1.43\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'Level': 1.25\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'conditional automation': 0.50\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'world driving': 1.13\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'software': 0.68\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'development': 1.00\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'autonomous': 0.68\n",
      "DEBUG: MultipartiteRank extracted 10 keyphrases\n",
      "DEBUG: YAKE position weight for 'like waymo google': 2.00\n",
      "DEBUG: YAKE position weight for 'systems offering level': 2.00\n",
      "DEBUG: YAKE position weight for 'companies like waymo': 2.00\n",
      "DEBUG: YAKE position weight for 'tech companies like': 2.00\n",
      "DEBUG: YAKE position weight for 'assistance systems adas': 2.00\n",
      "DEBUG: YAKE position weight for 'levels tech companies': 2.00\n",
      "DEBUG: YAKE position weight for 'paramount rigorous testing': 2.00\n",
      "DEBUG: YAKE position weight for 'rigorous testing including': 2.00\n",
      "DEBUG: YAKE position weight for 'perception challenges technologically': 2.00\n",
      "DEBUG: YAKE position weight for 'traditional automakers like': 2.00\n",
      "DEBUG: YAKE TF-IDF weight for 'like waymo google': 0.74\n",
      "DEBUG: YAKE TF-IDF weight for 'systems offering level': 2.00\n",
      "DEBUG: YAKE TF-IDF weight for 'companies like waymo': 0.96\n",
      "DEBUG: YAKE TF-IDF weight for 'tech companies like': 1.51\n",
      "DEBUG: YAKE TF-IDF weight for 'assistance systems adas': 0.73\n",
      "DEBUG: YAKE TF-IDF weight for 'levels tech companies': 0.78\n",
      "DEBUG: YAKE TF-IDF weight for 'paramount rigorous testing': 0.74\n",
      "DEBUG: YAKE TF-IDF weight for 'rigorous testing including': 0.50\n",
      "DEBUG: YAKE TF-IDF weight for 'perception challenges technologically': 0.95\n",
      "DEBUG: YAKE TF-IDF weight for 'traditional automakers like': 0.74\n",
      "DEBUG: YAKE extracted 10 keyphrases\n",
      "DEBUG: TextRank position weight for 'autonomous driving ad systems continues': 2.00\n",
      "DEBUG: TextRank position weight for 'ad systems remains': 2.00\n",
      "DEBUG: TextRank position weight for 'increased efficiency drives ad development': 2.00\n",
      "DEBUG: TextRank position weight for 'driving vehicles remains': 2.00\n",
      "DEBUG: TextRank position weight for 'public perception challenges technologically advancements': 2.00\n",
      "DEBUG: TextRank position weight for 'higher autonomy levels tech companies': 2.00\n",
      "DEBUG: TextRank position weight for 'ad functionalities systems': 2.00\n",
      "DEBUG: TextRank position weight for 'ad systems': 2.00\n",
      "DEBUG: TextRank position weight for 'navigating significant regulatory hurdles': 2.00\n",
      "DEBUG: TextRank position weight for 'validate system': 2.00\n",
      "DEBUG: TextRank TF-IDF weight for 'autonomous driving ad systems continues': 1.85\n",
      "DEBUG: TextRank TF-IDF weight for 'ad systems remains': 2.00\n",
      "DEBUG: TextRank TF-IDF weight for 'increased efficiency drives ad development': 1.68\n",
      "DEBUG: TextRank TF-IDF weight for 'driving vehicles remains': 1.86\n",
      "DEBUG: TextRank TF-IDF weight for 'public perception challenges technologically advancements': 1.30\n",
      "DEBUG: TextRank TF-IDF weight for 'higher autonomy levels tech companies': 1.22\n",
      "DEBUG: TextRank TF-IDF weight for 'ad functionalities systems': 1.79\n",
      "DEBUG: TextRank TF-IDF weight for 'ad systems': 1.97\n",
      "DEBUG: TextRank TF-IDF weight for 'navigating significant regulatory hurdles': 1.13\n",
      "DEBUG: TextRank TF-IDF weight for 'validate system': 0.50\n",
      "DEBUG: TextRank extracted 10 keyphrases\n",
      "DEBUG: Raw domain score for sports: 9.00\n",
      "DEBUG: Top domains detected: technology: 0.4476, news: 0.1714, business: 0.1524\n",
      "DEBUG: All domain scores: {'technology': 0.44761904761904764, 'business': 0.1523809523809524, 'health': 0.009523809523809525, 'science': 0.0, 'news': 0.17142857142857143, 'academic': 0.050793650793650794, 'politics': 0.02857142857142857, 'environment': 0.01904761904761905, 'entertainment': 0.06349206349206349, 'sports': 0.05714285714285714}\n",
      "DEBUG: Adaptive method weights:\n",
      "DEBUG:   - keybert: 0.3074\n",
      "DEBUG:   - multipartiterank: 0.3273\n",
      "DEBUG:   - yake: 0.0954\n",
      "DEBUG:   - textrank: 0.2699\n",
      "DEBUG: Length bonus for 'ad system' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'forward march' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'widespread deployment' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'persistent public perception' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'technologically advancement' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'sensor fusion' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'specific condition' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'conditional automation' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'waymo google' (2 words): 1.15x\n",
      "DEBUG: Length bonus for '4 high automation' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'specific geographic area' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'weather condition' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'more refined projection' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'new vehicle' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'human intervention' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'distant goal' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'heavily traditional automaker' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'gm ford' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'mercedes - benz' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'traditional automaker' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'paramount rigorous testing' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'cruise gm' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'robotaxi service' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'intense scrutiny' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'regulatory framework' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'clear rule' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'data privacy' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'different jurisdiction' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'technological capability' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'grow percentage' (2 words): 1.15x\n",
      "DEBUG: Length bonus for '4 capability' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'most application company' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'limit urban environment' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'state country' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'major challenge' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'harmonize regulation' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'broad deployment' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'system performance' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'countless scenario' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'edge case' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'critical concern' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'drive function' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'potential target' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'malicious attack' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'secure software update' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'collect data' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'public trust' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'enhance safety' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'increase efficiency' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'ad development' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'full potential' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'autonomous vehicle' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'come year' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'world drive' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'system offer level' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'company waymo' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'tech company' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'assistance system ada' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'level tech company' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'rigorous testing include' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'perception challenge technologically' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'autonomous drive ad system continue' (5 words): 1.60x\n",
      "DEBUG: Length bonus for 'ad system remain' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'increase efficiency drive ad development' (5 words): 1.60x\n",
      "DEBUG: Length bonus for 'drive vehicle remain' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'public perception challenge technologically advancement' (5 words): 1.60x\n",
      "DEBUG: Length bonus for 'high autonomy level tech company' (5 words): 1.60x\n",
      "DEBUG: Length bonus for 'ad functionality system' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'navigate significant regulatory hurdle' (4 words): 1.45x\n",
      "DEBUG: Length bonus for 'validate system' (2 words): 1.15x\n",
      "DEBUG: Ensemble combined into 113 keyphrases\n",
      "DEBUG: After redundancy removal: 77 keyphrases\n",
      "DEBUG: Before post-filtering: 77 keyphrases\n",
      "DEBUG: Multi-word phrases: 41\n",
      "DEBUG: Filtered out single word 'tech' as it's part of multi-word phrase(s)\n",
      "DEBUG: After post-filtering: 76 keyphrases\n",
      "DEBUG: Final top 10 keyphrases: ['data', 'regulatory', 'level', 'development', 'software', 'safety', 'cybersecurity', 'edge', 'rigorous', 'car']\n",
      "\n",
      "Zero-Shot Domain Detection Results:\n",
      "----------------------------------------\n",
      "Top domain: technology (Score: 0.3366)\n",
      "  1. technology: 0.3366\n",
      "  2. automotive: 0.2912\n",
      "  3. artificial intelligence: 0.1859\n",
      "  4. business: 0.0961\n",
      "  5. travel: 0.0796\n",
      "----------------------------------------\n",
      "Threshold for 'technology': 0.45\n",
      "Low confidence (0.3366) for domain 'technology'.\n",
      "Score is above 0.25, using detected domain despite being below threshold\n",
      "Semantic diversity: 0.790 → 0.790\n",
      "Keyphrase count: 17 → 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing articles:  29%|██▊       | 6/21 [06:41<16:18, 65.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generic term filtering: 14 -> 13 keyphrases (1 removed)\n",
      "\n",
      "Filtered out generic terms (examples):\n",
      "- development (0.21): exact generic match\n",
      "\n",
      "Using parameters for domain 'technology':\n",
      "  - base_threshold: 0.090\n",
      "  - quality_threshold: 0.420\n",
      "  - percentile: 5\n",
      "  - target_count: 10\n",
      "Using domain 'technology' with base threshold: 0.090, quality threshold: 0.420\n",
      "Using 5th percentile for domain 'technology'\n",
      "- Using 5th percentile from top: 0.657\n",
      "\n",
      "Quality Filtering Stats:\n",
      "- Domain: technology\n",
      "- Text Length: 382 words\n",
      "- Content Density: 0.69\n",
      "- Initial Candidates: 13\n",
      "- Base Threshold: 0.090\n",
      "- Quality Threshold: 0.350\n",
      "- After Quality Filter: 7\n",
      "Using target count 10 for domain 'technology'\n",
      "- Quality filter too strict, falling back to top 10 keyphrases\n",
      "- Using lenient threshold 0.175: 11 keyphrases\n",
      "- Final keyphrase count: 11\n",
      "- Semantic diversity score: 0.81\n",
      "Using domain 'technology' with quality threshold: 0.55\n",
      "Title: Autonomous Driving Systems Progress Amidst Regulatory and Safety Hurdles\n",
      "Domain: automotive\n",
      "Extracted 8 keyphrases:\n",
      "- lidar: 0.66\n",
      "- sensor fusion: 0.65\n",
      "- vehicle: 0.58\n",
      "- ongoing safety validation requirement: 0.55\n",
      "- perception algorithm: 0.42\n",
      "- cybersecurity: 0.38\n",
      "- radar: 0.35\n",
      "- navigating significant regulatory hurdle: 0.34\n",
      "\n",
      "\n",
      "Processing article 7/21 (cybersecurity)...\n",
      "\n",
      "Zero-Shot Domain Detection Results:\n",
      "----------------------------------------\n",
      "Top domain: technology (Score: 0.4462)\n",
      "  1. technology: 0.4462\n",
      "  2. cybersecurity: 0.4210\n",
      "  3. business: 0.1233\n",
      "  4. artificial intelligence: 0.0055\n",
      "  5. education: 0.0024\n",
      "----------------------------------------\n",
      "Threshold for 'technology': 0.45\n",
      "Using secondary domain 'cybersecurity' (Score: 0.4210, Threshold: 0.32)\n",
      "Detected domain: cybersecurity\n",
      "\n",
      "Zero-Shot Domain Detection Results:\n",
      "----------------------------------------\n",
      "Top domain: technology (Score: 0.4449)\n",
      "  1. technology: 0.4449\n",
      "  2. cybersecurity: 0.4072\n",
      "  3. business: 0.1289\n",
      "  4. education: 0.0083\n",
      "  5. artificial intelligence: 0.0064\n",
      "----------------------------------------\n",
      "Threshold for 'technology': 0.45\n",
      "Using secondary domain 'cybersecurity' (Score: 0.4072, Threshold: 0.32)\n",
      "Extracted 3 named entities\n",
      "Generated 9 candidate keyphrases\n",
      "Semantic diversity: 0.745 → 0.745\n",
      "Keyphrase count: 9 → 9\n",
      "\n",
      "Using parameters for domain 'cybersecurity':\n",
      "  - base_threshold: 0.060\n",
      "  - quality_threshold: 0.390\n",
      "  - percentile: 4\n",
      "  - target_count: 10\n",
      "Using domain 'cybersecurity' with base threshold: 0.060, quality threshold: 0.390\n",
      "Using 4th percentile for domain 'cybersecurity'\n",
      "- Using 5th percentile from top: 0.849\n",
      "\n",
      "Quality Filtering Stats:\n",
      "- Domain: cybersecurity\n",
      "- Text Length: 380 words\n",
      "- Content Density: 0.71\n",
      "- Initial Candidates: 7\n",
      "- Base Threshold: 0.060\n",
      "- Quality Threshold: 0.350\n",
      "- After Quality Filter: 5\n",
      "Using target count 10 for domain 'cybersecurity'\n",
      "- Final keyphrase count: 5\n",
      "- Semantic diversity score: 0.61\n",
      "DEBUG: Extracting keyphrases with use_position_weight=True, use_tfidf_weight=True, use_ensemble=True\n",
      "DEBUG: KeyBERT extracted 89 keyphrases\n",
      "DEBUG: MultipartiteRank position weight for 'attack methodologies': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'disruptive cyberthreat': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'Ransomware': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'extortion': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'enterprise defenses': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'pressure': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'rise': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'overall ransomware activity': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'data theft': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'disruptions': 2.00\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'attack methodologies': 0.59\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'disruptive cyberthreat': 0.59\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'Ransomware': 2.00\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'extortion': 1.52\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'enterprise defenses': 0.95\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'pressure': 1.15\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'rise': 1.15\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'overall ransomware activity': 1.43\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'data theft': 1.56\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'disruptions': 0.50\n",
      "DEBUG: MultipartiteRank extracted 10 keyphrases\n",
      "DEBUG: YAKE position weight for 'remote access trojans': 2.00\n",
      "DEBUG: YAKE position weight for 'access trojans rats': 2.00\n",
      "DEBUG: YAKE position weight for 'vulnerable driver byovd': 2.00\n",
      "DEBUG: YAKE position weight for 'legitimate remote monitoring': 2.00\n",
      "DEBUG: YAKE position weight for 'management rmm tools': 2.00\n",
      "DEBUG: YAKE position weight for 'like connectwise screenconnect': 2.00\n",
      "DEBUG: YAKE position weight for 'service raas platforms': 2.00\n",
      "DEBUG: YAKE position weight for 'response edr solutions': 2.00\n",
      "DEBUG: YAKE position weight for 'driver byovd technique': 2.00\n",
      "DEBUG: YAKE position weight for 'disabling security defenses': 2.00\n",
      "DEBUG: YAKE TF-IDF weight for 'remote access trojans': 1.13\n",
      "DEBUG: YAKE TF-IDF weight for 'access trojans rats': 0.76\n",
      "DEBUG: YAKE TF-IDF weight for 'vulnerable driver byovd': 1.13\n",
      "DEBUG: YAKE TF-IDF weight for 'legitimate remote monitoring': 1.46\n",
      "DEBUG: YAKE TF-IDF weight for 'management rmm tools': 1.28\n",
      "DEBUG: YAKE TF-IDF weight for 'like connectwise screenconnect': 0.76\n",
      "DEBUG: YAKE TF-IDF weight for 'service raas platforms': 0.50\n",
      "DEBUG: YAKE TF-IDF weight for 'response edr solutions': 0.50\n",
      "DEBUG: YAKE TF-IDF weight for 'driver byovd technique': 0.76\n",
      "DEBUG: YAKE TF-IDF weight for 'disabling security defenses': 2.00\n",
      "DEBUG: YAKE extracted 10 keyphrases\n",
      "DEBUG: TextRank position weight for 'tactic attackers actively': 2.00\n",
      "DEBUG: TextRank position weight for 'disabling security defenses': 2.00\n",
      "DEBUG: TextRank position weight for 'robust endpoint security': 2.00\n",
      "DEBUG: TextRank position weight for 'ransomware remains': 2.00\n",
      "DEBUG: TextRank position weight for 'attack': 2.00\n",
      "DEBUG: TextRank position weight for 'ransomware activity': 2.00\n",
      "DEBUG: TextRank position weight for 'government sectors remain': 2.00\n",
      "DEBUG: TextRank position weight for 'continually adapt': 2.00\n",
      "DEBUG: TextRank position weight for 'continued rise': 2.00\n",
      "DEBUG: TextRank position weight for 'legitimate remote': 2.00\n",
      "DEBUG: TextRank TF-IDF weight for 'tactic attackers actively': 1.03\n",
      "DEBUG: TextRank TF-IDF weight for 'disabling security defenses': 1.53\n",
      "DEBUG: TextRank TF-IDF weight for 'robust endpoint security': 2.00\n",
      "DEBUG: TextRank TF-IDF weight for 'ransomware remains': 1.67\n",
      "DEBUG: TextRank TF-IDF weight for 'attack': 0.50\n",
      "DEBUG: TextRank TF-IDF weight for 'ransomware activity': 1.67\n",
      "DEBUG: TextRank TF-IDF weight for 'government sectors remain': 0.85\n",
      "DEBUG: TextRank TF-IDF weight for 'continually adapt': 0.91\n",
      "DEBUG: TextRank TF-IDF weight for 'continued rise': 0.91\n",
      "DEBUG: TextRank TF-IDF weight for 'legitimate remote': 1.43\n",
      "DEBUG: TextRank extracted 10 keyphrases\n",
      "DEBUG: Raw domain score for sports: 4.00\n",
      "DEBUG: Top domains detected: politics: 0.3243, technology: 0.2928, business: 0.1622\n",
      "DEBUG: All domain scores: {'technology': 0.2927927927927928, 'business': 0.16216216216216217, 'health': 0.04054054054054054, 'science': 0.0, 'news': 0.0990990990990991, 'academic': 0.0, 'politics': 0.32432432432432434, 'environment': 0.0, 'entertainment': 0.04504504504504504, 'sports': 0.036036036036036036}\n",
      "DEBUG: Adaptive method weights:\n",
      "DEBUG:   - keybert: 0.3140\n",
      "DEBUG:   - multipartiterank: 0.3304\n",
      "DEBUG:   - yake: 0.0925\n",
      "DEBUG:   - textrank: 0.2630\n",
      "DEBUG: Length bonus for 'overall ransomware activity' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'data theft' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'law enforcement' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'recent report' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'major gang' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'threat 's resilience' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'key trend' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'core component attacker' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'double extortion' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'sensitive data' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'ransomware remain' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'ransomware activity' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'enterprise defense' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'disruptive cyberthreat' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'additional pressure' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'reputational damage' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'robust backup' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'triple extortion' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'ddos attack' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'victim 's customer' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'security measure' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'lotl technique' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'malicious purpose' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'normal operation' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'Remote Access Trojans' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'Remote Monitoring' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'Endpoint Detection' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'connectwise screenconnect' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'initial access' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'security defense' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'ransomware payload' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'popular method' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'government sector' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'attack methodology' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'enhance enterprise defense' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'continue rise' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'steal information' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'regulatory penalty' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'mark increase' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'prime target' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'high potential impact' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'ransom demand' (2 words): 1.15x\n",
      "DEBUG: Length bonus for '- service raas' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'less sophisticated actor' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'potent attack' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'defense strategy' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'evolve threat' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'remote access trojan' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'access trojans rat' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'vulnerable driver byovd' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'legitimate remote monitoring' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'management rmm tool' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'service raas platform' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'response edr solution' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'driver byovd technique' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'disable security defense' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'tactic attacker actively' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'robust endpoint security' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'government sector remain' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'continually adapt' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'legitimate remote' (2 words): 1.15x\n",
      "DEBUG: Ensemble combined into 104 keyphrases\n",
      "DEBUG: After redundancy removal: 72 keyphrases\n",
      "DEBUG: Before post-filtering: 72 keyphrases\n",
      "DEBUG: Multi-word phrases: 34\n",
      "DEBUG: Filtered out single word 'rise' as it's part of multi-word phrase(s)\n",
      "DEBUG: Filtered out single word 'attack' as it's part of multi-word phrase(s)\n",
      "DEBUG: After post-filtering: 70 keyphrases\n",
      "DEBUG: Final top 10 keyphrases: ['ransomware', 'data', 'network', 'extortion', 'critical', 'service', 'activity', 'management', 'disable', 'response']\n",
      "\n",
      "Zero-Shot Domain Detection Results:\n",
      "----------------------------------------\n",
      "Top domain: technology (Score: 0.4462)\n",
      "  1. technology: 0.4462\n",
      "  2. cybersecurity: 0.4210\n",
      "  3. business: 0.1233\n",
      "  4. artificial intelligence: 0.0055\n",
      "  5. education: 0.0024\n",
      "----------------------------------------\n",
      "Threshold for 'technology': 0.45\n",
      "Using secondary domain 'cybersecurity' (Score: 0.4210, Threshold: 0.32)\n",
      "Semantic diversity: 0.746 → 0.746\n",
      "Keyphrase count: 14 → 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing articles:  33%|███▎      | 7/21 [07:39<14:39, 62.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generic term filtering: 14 -> 13 keyphrases (1 removed)\n",
      "\n",
      "Filtered out generic terms (examples):\n",
      "- critical (0.11): exact generic match\n",
      "\n",
      "Using parameters for domain 'cybersecurity':\n",
      "  - base_threshold: 0.060\n",
      "  - quality_threshold: 0.390\n",
      "  - percentile: 4\n",
      "  - target_count: 10\n",
      "Using domain 'cybersecurity' with base threshold: 0.060, quality threshold: 0.390\n",
      "Using 4th percentile for domain 'cybersecurity'\n",
      "- Using 5th percentile from top: 0.831\n",
      "\n",
      "Quality Filtering Stats:\n",
      "- Domain: cybersecurity\n",
      "- Text Length: 380 words\n",
      "- Content Density: 0.71\n",
      "- Initial Candidates: 13\n",
      "- Base Threshold: 0.060\n",
      "- Quality Threshold: 0.350\n",
      "- After Quality Filter: 6\n",
      "Using target count 10 for domain 'cybersecurity'\n",
      "- Quality filter too strict, falling back to top 10 keyphrases\n",
      "- Falling back to top 10 keyphrases by score\n",
      "- Final keyphrase count: 10\n",
      "- Semantic diversity score: 0.71\n",
      "Using domain 'cybersecurity' with quality threshold: 0.55\n",
      "Title: Ransomware Evolves: Attackers Shift Tactics Amidst Improved Defenses\n",
      "Domain: cybersecurity\n",
      "Extracted 8 keyphrases:\n",
      "- ransomware: 0.86\n",
      "- cybercriminals: 0.81\n",
      "- cyber threat: 0.73\n",
      "- DDoS: 0.66\n",
      "- LockBit: 0.66\n",
      "- extortion: 0.55\n",
      "- activity: 0.34\n",
      "- data: 0.28\n",
      "\n",
      "\n",
      "Processing article 8/21 (cybersecurity)...\n",
      "Domain detected using BART: cybersecurity (confidence: 0.5717)\n",
      "Detected domain: cybersecurity\n",
      "Domain detected using BART: cybersecurity (confidence: 0.7444)\n",
      "Extracted 1 named entities\n",
      "Generated 10 candidate keyphrases\n",
      "Semantic diversity: 0.802 → 0.802\n",
      "Keyphrase count: 10 → 10\n",
      "\n",
      "Using parameters for domain 'cybersecurity':\n",
      "  - base_threshold: 0.080\n",
      "  - quality_threshold: 0.420\n",
      "  - percentile: 5\n",
      "  - target_count: 10\n",
      "Using domain 'cybersecurity' with base threshold: 0.080, quality threshold: 0.420\n",
      "Using 5th percentile for domain 'cybersecurity'\n",
      "- Using 5th percentile from top: 0.741\n",
      "\n",
      "Quality Filtering Stats:\n",
      "- Domain: cybersecurity\n",
      "- Text Length: 376 words\n",
      "- Content Density: 0.68\n",
      "- Initial Candidates: 8\n",
      "- Base Threshold: 0.080\n",
      "- Quality Threshold: 0.350\n",
      "- After Quality Filter: 3\n",
      "Using target count 10 for domain 'cybersecurity'\n",
      "- Too few keyphrases (3), ensuring minimum of 8\n",
      "- Final keyphrase count: 8\n",
      "- Semantic diversity score: 0.81\n",
      "DEBUG: Extracting keyphrases with use_position_weight=True, use_tfidf_weight=True, use_ensemble=True\n",
      "DEBUG: KeyBERT extracted 86 keyphrases\n",
      "DEBUG: MultipartiteRank position weight for 'organizations': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'gap': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'AI artificial intelligence': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'demand': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'critical skills': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'available talent pool': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'cybersecurity': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'skilled cybersecurity professionals': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'significant challenge': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'severity': 2.00\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'organizations': 1.86\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'gap': 1.35\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'AI artificial intelligence': 2.00\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'demand': 1.02\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'critical skills': 1.14\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'available talent pool': 0.90\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'cybersecurity': 1.27\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'skilled cybersecurity professionals': 1.12\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'significant challenge': 0.83\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'severity': 0.50\n",
      "DEBUG: MultipartiteRank extracted 10 keyphrases\n",
      "DEBUG: YAKE position weight for 'world economic forum': 2.00\n",
      "DEBUG: YAKE position weight for 'active directory security': 2.00\n",
      "DEBUG: YAKE position weight for 'like active directory': 2.00\n",
      "DEBUG: YAKE position weight for 'economic forum highlight': 2.00\n",
      "DEBUG: YAKE position weight for 'directory security cloud': 2.00\n",
      "DEBUG: YAKE position weight for 'artificial intelligence continuously': 2.00\n",
      "DEBUG: YAKE position weight for 'artificial intelligence threat': 2.00\n",
      "DEBUG: YAKE position weight for 'artificial intelligence -powered': 2.00\n",
      "DEBUG: YAKE position weight for 'artificial intelligence technologies': 2.00\n",
      "DEBUG: YAKE position weight for 'artificial intelligence effectively': 2.00\n",
      "DEBUG: YAKE TF-IDF weight for 'world economic forum': 0.55\n",
      "DEBUG: YAKE TF-IDF weight for 'active directory security': 1.32\n",
      "DEBUG: YAKE TF-IDF weight for 'like active directory': 0.50\n",
      "DEBUG: YAKE TF-IDF weight for 'economic forum highlight': 0.57\n",
      "DEBUG: YAKE TF-IDF weight for 'directory security cloud': 1.27\n",
      "DEBUG: YAKE TF-IDF weight for 'artificial intelligence continuously': 2.00\n",
      "DEBUG: YAKE TF-IDF weight for 'artificial intelligence threat': 2.00\n",
      "DEBUG: YAKE TF-IDF weight for 'artificial intelligence -powered': 2.00\n",
      "DEBUG: YAKE TF-IDF weight for 'artificial intelligence technologies': 2.00\n",
      "DEBUG: YAKE TF-IDF weight for 'artificial intelligence effectively': 2.00\n",
      "DEBUG: YAKE extracted 10 keyphrases\n",
      "DEBUG: TextRank position weight for 'existing blue teams defensive security professionals': 2.00\n",
      "DEBUG: TextRank position weight for 'cybersecurity skills gap': 2.00\n",
      "DEBUG: TextRank position weight for 'skills gap requires': 2.00\n",
      "DEBUG: TextRank position weight for 'skills gaps several': 2.00\n",
      "DEBUG: TextRank position weight for 'skilled cybersecurity professionals': 2.00\n",
      "DEBUG: TextRank position weight for 'ai artificial intelligence technologies': 2.00\n",
      "DEBUG: TextRank position weight for 'ai artificial intelligence': 2.00\n",
      "DEBUG: TextRank position weight for 'many organizations report': 2.00\n",
      "DEBUG: TextRank position weight for 'defensive skills': 2.00\n",
      "DEBUG: TextRank position weight for 'security cloud security': 2.00\n",
      "DEBUG: TextRank TF-IDF weight for 'existing blue teams defensive security professionals': 1.19\n",
      "DEBUG: TextRank TF-IDF weight for 'cybersecurity skills gap': 1.40\n",
      "DEBUG: TextRank TF-IDF weight for 'skills gap requires': 0.89\n",
      "DEBUG: TextRank TF-IDF weight for 'skills gaps several': 0.50\n",
      "DEBUG: TextRank TF-IDF weight for 'skilled cybersecurity professionals': 0.67\n",
      "DEBUG: TextRank TF-IDF weight for 'ai artificial intelligence technologies': 1.52\n",
      "DEBUG: TextRank TF-IDF weight for 'ai artificial intelligence': 2.00\n",
      "DEBUG: TextRank TF-IDF weight for 'many organizations report': 0.94\n",
      "DEBUG: TextRank TF-IDF weight for 'defensive skills': 0.84\n",
      "DEBUG: TextRank TF-IDF weight for 'security cloud security': 1.71\n",
      "DEBUG: TextRank extracted 10 keyphrases\n",
      "DEBUG: Raw domain score for sports: 12.50\n",
      "DEBUG: Top domains detected: technology: 0.3419, politics: 0.1830, business: 0.1454\n",
      "DEBUG: All domain scores: {'technology': 0.34185733512786004, 'business': 0.14535666218034993, 'health': 0.03768506056527591, 'science': 0.040376850605652756, 'news': 0.0847913862718708, 'academic': 0.03230148048452221, 'politics': 0.18304172274562583, 'environment': 0.0, 'entertainment': 0.06729475100942127, 'sports': 0.06729475100942127}\n",
      "DEBUG: Adaptive method weights:\n",
      "DEBUG:   - keybert: 0.2981\n",
      "DEBUG:   - multipartiterank: 0.3227\n",
      "DEBUG:   - yake: 0.0934\n",
      "DEBUG:   - textrank: 0.2857\n",
      "DEBUG: Length bonus for 'world economic forum' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'skilled cybersecurity professional' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'available talent pool' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'significant challenge' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'increasingly sophisticated cyberthreats' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'numerous initiative' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'cybersecurity 's importance' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'workforce growth' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'significant increase' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'previous year' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'talent pool' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'economic forum highlight' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'ai artificial intelligence' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'public sector' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'private organization' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'several factor' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'rapid evolution' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'attack surface' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'new specialized skill' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'critical skill' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'artificial intelligence continuously' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'Active Directory' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'educational pipeline' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'qualified candidate' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'cybersecurity career' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'high school' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'traditional degree' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'demonstrable skill' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'recent study' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'exist workforce' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'great difficulty' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'widen gap' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'thing device' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'furthermore adversary' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'great expertise' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'specific rise demand' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'require scale' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'just academic qualification' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'many organization' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'adequate defensive skill' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'current staff organization' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'human team' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'automate task' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'personnel shortage' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'report knowledge gap' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'many professional' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'security stack' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'skill gap' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'attractive career path' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'cybersecurity field failure' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'active directory security' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'directory security cloud' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'artificial intelligence threat' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'artificial intelligence -powered' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'artificial intelligence technology' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'artificial intelligence effectively' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'exist blue team defensive security professional' (6 words): 1.60x\n",
      "DEBUG: Length bonus for 'cybersecurity skill gap' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'skill gap require' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'skill gap several' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'ai artificial intelligence technology' (4 words): 1.45x\n",
      "DEBUG: Length bonus for 'many organization report' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'defensive skill' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'security cloud security' (3 words): 1.30x\n",
      "DEBUG: Ensemble combined into 102 keyphrases\n",
      "DEBUG: After redundancy removal: 79 keyphrases\n",
      "DEBUG: Before post-filtering: 79 keyphrases\n",
      "DEBUG: Multi-word phrases: 44\n",
      "DEBUG: Filtered out single word 'ai' as it's part of multi-word phrase(s)\n",
      "DEBUG: Filtered out single word 'expert' as it's part of multi-word phrase(s)\n",
      "DEBUG: After post-filtering: 77 keyphrases\n",
      "DEBUG: Final top 10 keyphrases: ['gap', 'talent', 'demand', 'shortage', 'report', 'automation', 'however', 'several', 'furthermore', 'indeed']\n",
      "Domain detected using BART: cybersecurity (confidence: 0.5717)\n",
      "Semantic diversity: 0.803 → 0.803\n",
      "Keyphrase count: 15 → 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing articles:  38%|███▊      | 8/21 [08:37<13:15, 61.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generic term filtering: 14 -> 11 keyphrases (3 removed)\n",
      "\n",
      "Filtered out generic terms (examples):\n",
      "- report (0.19): exact generic match\n",
      "- furthermore (0.08): exact generic match\n",
      "- several (0.08): exact generic match\n",
      "\n",
      "Using parameters for domain 'cybersecurity':\n",
      "  - base_threshold: 0.080\n",
      "  - quality_threshold: 0.420\n",
      "  - percentile: 5\n",
      "  - target_count: 10\n",
      "Using domain 'cybersecurity' with base threshold: 0.080, quality threshold: 0.420\n",
      "Using 5th percentile for domain 'cybersecurity'\n",
      "- Using 5th percentile from top: 0.693\n",
      "\n",
      "Quality Filtering Stats:\n",
      "- Domain: cybersecurity\n",
      "- Text Length: 376 words\n",
      "- Content Density: 0.68\n",
      "- Initial Candidates: 11\n",
      "- Base Threshold: 0.080\n",
      "- Quality Threshold: 0.350\n",
      "- After Quality Filter: 4\n",
      "Using target count 10 for domain 'cybersecurity'\n",
      "- Quality filter too strict, falling back to top 10 keyphrases\n",
      "- Falling back to top 10 keyphrases by score\n",
      "- Final keyphrase count: 10\n",
      "- Semantic diversity score: 0.79\n",
      "Using domain 'cybersecurity' with quality threshold: 0.55\n",
      "Title: Cybersecurity Skills Gap Widens in 2025 Despite Increased Awareness\n",
      "Domain: cybersecurity\n",
      "Extracted 8 keyphrases:\n",
      "- cybersecurity professional: 0.85\n",
      "- posing a significant challenge: 0.54\n",
      "- talent pool: 0.47\n",
      "- shortage: 0.35\n",
      "- organization: 0.35\n",
      "- the World Economic Forum: 0.34\n",
      "- demand: 0.31\n",
      "- gap: 0.29\n",
      "\n",
      "\n",
      "Processing article 9/21 (cybersecurity)...\n",
      "Domain detected using BART: cybersecurity (confidence: 0.6476)\n",
      "Detected domain: cybersecurity\n",
      "Domain detected using BART: cybersecurity (confidence: 0.5720)\n",
      "Extracted 4 named entities\n",
      "Generated 11 candidate keyphrases\n",
      "Semantic diversity: 0.694 → 0.694\n",
      "Keyphrase count: 11 → 11\n",
      "\n",
      "Generic term filtering: 11 -> 10 keyphrases (1 removed)\n",
      "\n",
      "Filtered out generic terms (examples):\n",
      "- paramount concern (0.45): vague pattern\n",
      "\n",
      "Using parameters for domain 'cybersecurity':\n",
      "  - base_threshold: 0.060\n",
      "  - quality_threshold: 0.390\n",
      "  - percentile: 4\n",
      "  - target_count: 10\n",
      "Using domain 'cybersecurity' with base threshold: 0.060, quality threshold: 0.390\n",
      "Using 4th percentile for domain 'cybersecurity'\n",
      "- Using 5th percentile from top: 0.806\n",
      "\n",
      "Quality Filtering Stats:\n",
      "- Domain: cybersecurity\n",
      "- Text Length: 359 words\n",
      "- Content Density: 0.71\n",
      "- Initial Candidates: 10\n",
      "- Base Threshold: 0.060\n",
      "- Quality Threshold: 0.350\n",
      "- After Quality Filter: 9\n",
      "Using target count 10 for domain 'cybersecurity'\n",
      "- Quality filter too strict, falling back to top 10 keyphrases\n",
      "- Using lenient threshold 0.175: 10 keyphrases\n",
      "- Final keyphrase count: 10\n",
      "- Semantic diversity score: 0.69\n",
      "DEBUG: Extracting keyphrases with use_position_weight=True, use_tfidf_weight=True, use_ensemble=True\n",
      "DEBUG: KeyBERT extracted 82 keyphrases\n",
      "DEBUG: MultipartiteRank position weight for 'critical infrastructure sectors': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'water systems': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'governments': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'private organizations': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'service disruptions': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'cyberthreats': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'public safety': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'paramount concern': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'potential threats': 2.00\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'critical infrastructure sectors': 2.00\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'water systems': 1.47\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'governments': 0.93\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'private organizations': 1.26\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'service disruptions': 0.56\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'cyberthreats': 0.50\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'public safety': 0.80\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'paramount concern': 0.56\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'potential threats': 0.69\n",
      "DEBUG: MultipartiteRank extracted 9 keyphrases\n",
      "DEBUG: YAKE position weight for 'securing critical infrastructure': 2.00\n",
      "DEBUG: YAKE position weight for 'national cybersecurity strategy': 2.00\n",
      "DEBUG: YAKE position weight for 'initiatives national cybersecurity': 2.00\n",
      "DEBUG: YAKE position weight for 'national security ransomware': 2.00\n",
      "DEBUG: YAKE position weight for 'initiatives like italy': 2.00\n",
      "DEBUG: YAKE position weight for 'collaborative initiatives national': 2.00\n",
      "DEBUG: YAKE position weight for 'cybersecurity strategy 2022': 2.00\n",
      "DEBUG: YAKE position weight for '2025 securing critical': 2.00\n",
      "DEBUG: YAKE position weight for 'cyberincidents targeting critical': 2.00\n",
      "DEBUG: YAKE position weight for 'essential services cyberincidents': 2.00\n",
      "DEBUG: YAKE TF-IDF weight for 'securing critical infrastructure': 2.00\n",
      "DEBUG: YAKE TF-IDF weight for 'national cybersecurity strategy': 0.78\n",
      "DEBUG: YAKE TF-IDF weight for 'initiatives national cybersecurity': 1.16\n",
      "DEBUG: YAKE TF-IDF weight for 'national security ransomware': 1.16\n",
      "DEBUG: YAKE TF-IDF weight for 'initiatives like italy': 0.74\n",
      "DEBUG: YAKE TF-IDF weight for 'collaborative initiatives national': 0.75\n",
      "DEBUG: YAKE TF-IDF weight for 'cybersecurity strategy 2022': 0.50\n",
      "DEBUG: YAKE TF-IDF weight for '2025 securing critical': 1.69\n",
      "DEBUG: YAKE TF-IDF weight for 'cyberincidents targeting critical': 1.49\n",
      "DEBUG: YAKE TF-IDF weight for 'essential services cyberincidents': 0.77\n",
      "DEBUG: YAKE extracted 10 keyphrases\n",
      "DEBUG: TextRank position weight for 'securing critical infrastructure sectors including': 2.00\n",
      "DEBUG: TextRank position weight for 'critical infrastructure protection mandating stricter security': 2.00\n",
      "DEBUG: TextRank position weight for 'securing critical infrastructure': 2.00\n",
      "DEBUG: TextRank position weight for 'operational technology ot systems': 2.00\n",
      "DEBUG: TextRank position weight for 'critical systems continuous': 2.00\n",
      "DEBUG: TextRank position weight for 'challenges ot systems': 2.00\n",
      "DEBUG: TextRank position weight for 'critical systems': 2.00\n",
      "DEBUG: TextRank position weight for 'targeting critical infrastructure': 2.00\n",
      "DEBUG: TextRank position weight for 'critical infrastructure': 2.00\n",
      "DEBUG: TextRank position weight for 'national security': 2.00\n",
      "DEBUG: TextRank TF-IDF weight for 'securing critical infrastructure sectors including': 1.42\n",
      "DEBUG: TextRank TF-IDF weight for 'critical infrastructure protection mandating stricter security': 1.97\n",
      "DEBUG: TextRank TF-IDF weight for 'securing critical infrastructure': 1.60\n",
      "DEBUG: TextRank TF-IDF weight for 'operational technology ot systems': 1.04\n",
      "DEBUG: TextRank TF-IDF weight for 'critical systems continuous': 1.46\n",
      "DEBUG: TextRank TF-IDF weight for 'challenges ot systems': 0.50\n",
      "DEBUG: TextRank TF-IDF weight for 'critical systems': 1.91\n",
      "DEBUG: TextRank TF-IDF weight for 'targeting critical infrastructure': 1.74\n",
      "DEBUG: TextRank TF-IDF weight for 'critical infrastructure': 2.00\n",
      "DEBUG: TextRank TF-IDF weight for 'national security': 1.18\n",
      "DEBUG: TextRank extracted 10 keyphrases\n",
      "DEBUG: Raw domain score for sports: 2.50\n",
      "DEBUG: Top domains detected: business: 0.2644, politics: 0.2614, news: 0.1641\n",
      "DEBUG: All domain scores: {'technology': 0.1337386018237082, 'business': 0.26443768996960487, 'health': 0.02735562310030395, 'science': 0.05167173252279635, 'news': 0.1641337386018237, 'academic': 0.0121580547112462, 'politics': 0.2613981762917933, 'environment': 0.04559270516717325, 'entertainment': 0.0243161094224924, 'sports': 0.015197568389057751}\n",
      "DEBUG: Adaptive method weights:\n",
      "DEBUG:   - keybert: 0.3135\n",
      "DEBUG:   - multipartiterank: 0.3209\n",
      "DEBUG:   - yake: 0.1032\n",
      "DEBUG:   - textrank: 0.2623\n",
      "DEBUG: Length bonus for 'critical infrastructure' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'national security' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'private organization' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'public safety' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'potential threat' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'service disruption' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'paramount concern' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'risk profile' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'economic damage' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'ransomware operator' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'operational disruption' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'national security ransomware' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'critical infrastructure sector' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'water system' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'Information Technology' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'Operational Technology' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'severe repercussion' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'significant impact' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'recent trend' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'vital sector' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'information technology it' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'critical infrastructure environment' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'unique challenge' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'ot system' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'physical process' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'power generation' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'water treatment' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'modern security consideration' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'it network' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'it ot divide' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'resilience government' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'operational technology ot system' (4 words): 1.45x\n",
      "DEBUG: Length bonus for 'National Cybersecurity Strategy' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'critical system' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'critical infrastructure protection' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'training technology deployment' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'however regulatory fragmentation' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'different jurisdiction' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'multinational organization' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'increase sophistication' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'essential service cyberincidents' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'increase likelihood' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'mark increase' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'interconnect system' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'strict security standard' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'key defensive strategy' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'robust network segmentation' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'comprehensive incident response' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'supply chain' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'legacy system' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'skilled cybersecurity professional' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'ot expertise' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'persistent threat' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'essential service society' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'secure critical infrastructure' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'initiatives national cybersecurity' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'initiatives italy' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'collaborative initiative national' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'cybersecurity strategy 2022' (3 words): 1.30x\n",
      "DEBUG: Length bonus for '2025 secure critical' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'cyberincidents target critical' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'secure critical infrastructure sector include' (5 words): 1.60x\n",
      "DEBUG: Length bonus for 'critical infrastructure protection mandate strict security' (6 words): 1.60x\n",
      "DEBUG: Length bonus for 'critical system continuous' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'challenge ot system' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'target critical infrastructure' (3 words): 1.30x\n",
      "DEBUG: Ensemble combined into 96 keyphrases\n",
      "DEBUG: After redundancy removal: 72 keyphrases\n",
      "DEBUG: Before post-filtering: 72 keyphrases\n",
      "DEBUG: Multi-word phrases: 45\n",
      "DEBUG: Filtered out single word 'national' as it's part of multi-word phrase(s)\n",
      "DEBUG: After post-filtering: 71 keyphrases\n",
      "DEBUG: Final top 10 keyphrases: ['continuous', 'key', 'however', 'recent', 'cyberincidents', 'ransomware', 'challenge', 'industry', 'recovery', 'vulnerability']\n",
      "Domain detected using BART: cybersecurity (confidence: 0.6476)\n",
      "Semantic diversity: 0.751 → 0.751\n",
      "Keyphrase count: 16 → 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing articles:  43%|████▎     | 9/21 [09:28<11:38, 58.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generic term filtering: 15 -> 13 keyphrases (2 removed)\n",
      "\n",
      "Filtered out generic terms (examples):\n",
      "- key (0.10): exact generic match\n",
      "- recent (0.09): exact generic match\n",
      "\n",
      "Using parameters for domain 'cybersecurity':\n",
      "  - base_threshold: 0.060\n",
      "  - quality_threshold: 0.390\n",
      "  - percentile: 4\n",
      "  - target_count: 10\n",
      "Using domain 'cybersecurity' with base threshold: 0.060, quality threshold: 0.390\n",
      "Using 4th percentile for domain 'cybersecurity'\n",
      "- Using 5th percentile from top: 0.790\n",
      "\n",
      "Quality Filtering Stats:\n",
      "- Domain: cybersecurity\n",
      "- Text Length: 359 words\n",
      "- Content Density: 0.71\n",
      "- Initial Candidates: 13\n",
      "- Base Threshold: 0.060\n",
      "- Quality Threshold: 0.350\n",
      "- After Quality Filter: 8\n",
      "Using target count 10 for domain 'cybersecurity'\n",
      "- Quality filter too strict, falling back to top 10 keyphrases\n",
      "- Falling back to top 10 keyphrases by score\n",
      "- Final keyphrase count: 10\n",
      "- Semantic diversity score: 0.68\n",
      "Using domain 'cybersecurity' with quality threshold: 0.55\n",
      "Title: Protecting Critical Infrastructure: A Top Priority Amidst Rising Geopolitical Tensions\n",
      "Domain: cybersecurity\n",
      "Extracted 8 keyphrases:\n",
      "- cyber threat: 0.83\n",
      "- infrastructure sector: 0.76\n",
      "- Securing: 0.73\n",
      "- ransomware: 0.72\n",
      "- infrastructure: 0.70\n",
      "- government: 0.49\n",
      "- Information Technology: 0.43\n",
      "- industry: 0.36\n",
      "\n",
      "\n",
      "Processing article 10/21 (food)...\n",
      "\n",
      "Zero-Shot Domain Detection Results:\n",
      "----------------------------------------\n",
      "Top domain: food (Score: 0.2430)\n",
      "  1. food: 0.2430\n",
      "  2. agriculture: 0.2269\n",
      "  3. technology: 0.2074\n",
      "  4. science: 0.1477\n",
      "  5. business: 0.1192\n",
      "----------------------------------------\n",
      "Threshold for 'food': 0.28\n",
      "Low confidence (0.2430) for domain 'food'.\n",
      "Initialized domain fallback classifier from global namespace\n",
      "Domain detected using fallback classifier: technology (confidence: 0.4000)\n",
      "Detected domain: technology\n",
      "\n",
      "Zero-Shot Domain Detection Results:\n",
      "----------------------------------------\n",
      "Top domain: food (Score: 0.2538)\n",
      "  1. food: 0.2538\n",
      "  2. agriculture: 0.2530\n",
      "  3. technology: 0.1875\n",
      "  4. science: 0.1382\n",
      "  5. business: 0.1007\n",
      "----------------------------------------\n",
      "Threshold for 'food': 0.28\n",
      "Using secondary domain 'agriculture' (Score: 0.2530, Threshold: 0.25)\n",
      "Extracted 0 named entities\n",
      "Generated 5 candidate keyphrases\n",
      "Semantic diversity: 0.527 → 0.527\n",
      "Keyphrase count: 5 → 5\n",
      "Recovered keyphrase: cultivated meat (score: 0.8909)\n",
      "\n",
      "Using parameters for domain 'technology':\n",
      "  - base_threshold: 0.070\n",
      "  - quality_threshold: 0.390\n",
      "  - percentile: 4\n",
      "  - target_count: 10\n",
      "Using domain 'technology' with base threshold: 0.070, quality threshold: 0.390\n",
      "Using 4th percentile for domain 'technology'\n",
      "- Using 5th percentile from top: 0.891\n",
      "\n",
      "Quality Filtering Stats:\n",
      "- Domain: technology\n",
      "- Text Length: 359 words\n",
      "- Content Density: 0.70\n",
      "- Initial Candidates: 3\n",
      "- Base Threshold: 0.070\n",
      "- Quality Threshold: 0.350\n",
      "- After Quality Filter: 3\n",
      "Using target count 10 for domain 'technology'\n",
      "- Final keyphrase count: 3\n",
      "- Semantic diversity score: 0.32\n",
      "DEBUG: Extracting keyphrases with use_position_weight=True, use_tfidf_weight=True, use_ensemble=True\n",
      "DEBUG: KeyBERT extracted 79 keyphrases\n",
      "DEBUG: MultipartiteRank position weight for 'production': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'meat': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'traditional animal agriculture': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'key markets': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'cell': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'cost': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'potential': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'nutrient': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'regulatory approvals': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'texture': 2.00\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'production': 2.00\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'meat': 1.90\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'traditional animal agriculture': 0.94\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'key markets': 1.04\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'cell': 1.29\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'cost': 1.29\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'potential': 1.86\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'nutrient': 1.29\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'regulatory approvals': 1.10\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'texture': 0.50\n",
      "DEBUG: MultipartiteRank extracted 10 keyphrases\n",
      "DEBUG: YAKE position weight for 'development consumer acceptance': 2.00\n",
      "DEBUG: YAKE position weight for 'efficiency hybrid products': 2.00\n",
      "DEBUG: YAKE position weight for 'hybrid products blending': 2.00\n",
      "DEBUG: YAKE position weight for 'labeling early market': 2.00\n",
      "DEBUG: YAKE position weight for 'early market entries': 2.00\n",
      "DEBUG: YAKE position weight for 'retail shelves startups': 2.00\n",
      "DEBUG: YAKE position weight for 'bioreactor efficiency hybrid': 2.00\n",
      "DEBUG: YAKE position weight for 'retail adoption proponents': 2.00\n",
      "DEBUG: YAKE position weight for 'hurdles remain recent': 2.00\n",
      "DEBUG: YAKE position weight for 'remain recent months': 2.00\n",
      "DEBUG: YAKE TF-IDF weight for 'development consumer acceptance': 1.67\n",
      "DEBUG: YAKE TF-IDF weight for 'efficiency hybrid products': 1.46\n",
      "DEBUG: YAKE TF-IDF weight for 'hybrid products blending': 1.14\n",
      "DEBUG: YAKE TF-IDF weight for 'labeling early market': 2.00\n",
      "DEBUG: YAKE TF-IDF weight for 'early market entries': 1.50\n",
      "DEBUG: YAKE TF-IDF weight for 'retail shelves startups': 1.10\n",
      "DEBUG: YAKE TF-IDF weight for 'bioreactor efficiency hybrid': 1.10\n",
      "DEBUG: YAKE TF-IDF weight for 'retail adoption proponents': 1.10\n",
      "DEBUG: YAKE TF-IDF weight for 'hurdles remain recent': 0.50\n",
      "DEBUG: YAKE TF-IDF weight for 'remain recent months': 0.50\n",
      "DEBUG: YAKE extracted 10 keyphrases\n",
      "DEBUG: TextRank position weight for 'products blending cultivated cells': 2.00\n",
      "DEBUG: TextRank position weight for 'meat production': 2.00\n",
      "DEBUG: TextRank position weight for 'cell growth remains': 2.00\n",
      "DEBUG: TextRank position weight for 'potentially lowering costs': 2.00\n",
      "DEBUG: TextRank position weight for 'cultivated meat': 2.00\n",
      "DEBUG: TextRank position weight for 'market availability remains': 2.00\n",
      "DEBUG: TextRank position weight for 'scaling production': 2.00\n",
      "DEBUG: TextRank position weight for 'conventional meat': 2.00\n",
      "DEBUG: TextRank position weight for 'based meat': 2.00\n",
      "DEBUG: TextRank position weight for 'blended products': 2.00\n",
      "DEBUG: TextRank TF-IDF weight for 'products blending cultivated cells': 1.11\n",
      "DEBUG: TextRank TF-IDF weight for 'meat production': 2.00\n",
      "DEBUG: TextRank TF-IDF weight for 'cell growth remains': 1.31\n",
      "DEBUG: TextRank TF-IDF weight for 'potentially lowering costs': 0.61\n",
      "DEBUG: TextRank TF-IDF weight for 'cultivated meat': 1.64\n",
      "DEBUG: TextRank TF-IDF weight for 'market availability remains': 1.23\n",
      "DEBUG: TextRank TF-IDF weight for 'scaling production': 1.36\n",
      "DEBUG: TextRank TF-IDF weight for 'conventional meat': 1.29\n",
      "DEBUG: TextRank TF-IDF weight for 'based meat': 1.49\n",
      "DEBUG: TextRank TF-IDF weight for 'blended products': 0.50\n",
      "DEBUG: TextRank extracted 10 keyphrases\n",
      "DEBUG: Raw domain score for sports: 5.25\n",
      "DEBUG: Top domains detected: science: 0.2745, business: 0.1889, news: 0.1604\n",
      "DEBUG: All domain scores: {'technology': 0.1532976827094474, 'business': 0.1889483065953654, 'health': 0.0213903743315508, 'science': 0.27450980392156865, 'news': 0.16042780748663102, 'academic': 0.0142602495543672, 'politics': 0.024955436720142603, 'environment': 0.07130124777183601, 'entertainment': 0.053475935828877004, 'sports': 0.0374331550802139}\n",
      "DEBUG: Adaptive method weights:\n",
      "DEBUG:   - keybert: 0.2595\n",
      "DEBUG:   - multipartiterank: 0.2809\n",
      "DEBUG:   - yake: 0.2257\n",
      "DEBUG:   - textrank: 0.2339\n",
      "DEBUG: Length bonus for 'key market' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'regulatory approval' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'zoonotic disease' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'significant hurdle' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'pilot program' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'animal cell' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'traditional animal agriculture' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'remain recent month' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'development consumer acceptance' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'tissue growth' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'key technological challenge' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'industrial level' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'cell growth' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'optimal condition' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'tissue development' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'cell growth remain' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'crucial factor' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'taste parity affordability' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'clear communication' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'production process' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'cell line' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'potential pathway' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'regulatory pathway' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'safety assessment' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'efficiency hybrid product' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'cultivate meat' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'broad commercial availability' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'several company' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'increase investment' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'bioreactor technology' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'nutrient medium' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'structure cut' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'sophisticated scaffold technique' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'challenge progress' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'fast growth' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'high yield' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'cultivate cell' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'exist technology' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'various country' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'early market entry' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'blend product' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'price parity' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'conventional meat' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'widespread retail adoption' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'potential benefit' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'significantly reduce land' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'conventional livestock farming' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'antibiotic use' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'meat production' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'significant role' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'future food system' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'hybrid product blend' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'label early market' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'retail shelf startup' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'bioreactor efficiency hybrid' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'retail adoption proponent' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'hurdle remain recent' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'product blend cultivate cell' (4 words): 1.45x\n",
      "DEBUG: Length bonus for 'potentially lower cost' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'market availability remain' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'scale production' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'base meat' (2 words): 1.15x\n",
      "DEBUG: Ensemble combined into 96 keyphrases\n",
      "DEBUG: After redundancy removal: 62 keyphrases\n",
      "DEBUG: Before post-filtering: 62 keyphrases\n",
      "DEBUG: Multi-word phrases: 30\n",
      "DEBUG: After post-filtering: 62 keyphrases\n",
      "DEBUG: Final top 10 keyphrases: ['production', 'regulatory', 'meat', 'key', 'market', 'potential', 'animal', 'consumer', 'cost', 'safety']\n",
      "\n",
      "Zero-Shot Domain Detection Results:\n",
      "----------------------------------------\n",
      "Top domain: food (Score: 0.2430)\n",
      "  1. food: 0.2430\n",
      "  2. agriculture: 0.2269\n",
      "  3. technology: 0.2074\n",
      "  4. science: 0.1477\n",
      "  5. business: 0.1192\n",
      "----------------------------------------\n",
      "Threshold for 'food': 0.28\n",
      "Low confidence (0.2430) for domain 'food'.\n",
      "Domain detected using fallback classifier: technology (confidence: 0.4000)\n",
      "Semantic diversity: 0.734 → 0.734\n",
      "Keyphrase count: 13 → 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing articles:  48%|████▊     | 10/21 [10:24<10:32, 57.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generic term filtering: 11 -> 9 keyphrases (2 removed)\n",
      "\n",
      "Filtered out generic terms (examples):\n",
      "- market (0.41): exact generic match\n",
      "- key (0.07): exact generic match\n",
      "\n",
      "Using parameters for domain 'technology':\n",
      "  - base_threshold: 0.070\n",
      "  - quality_threshold: 0.390\n",
      "  - percentile: 4\n",
      "  - target_count: 10\n",
      "Using domain 'technology' with base threshold: 0.070, quality threshold: 0.390\n",
      "Using 4th percentile for domain 'technology'\n",
      "- Using 5th percentile from top: 0.756\n",
      "\n",
      "Quality Filtering Stats:\n",
      "- Domain: technology\n",
      "- Text Length: 359 words\n",
      "- Content Density: 0.70\n",
      "- Initial Candidates: 9\n",
      "- Base Threshold: 0.070\n",
      "- Quality Threshold: 0.350\n",
      "- After Quality Filter: 5\n",
      "Using target count 10 for domain 'technology'\n",
      "- Too few keyphrases (5), ensuring minimum of 8\n",
      "- Final keyphrase count: 8\n",
      "- Semantic diversity score: 0.76\n",
      "Using domain 'technology' with quality threshold: 0.55\n",
      "Title: Lab-Grown Meat Nears Commercial Reality Amidst Production Hurdles\n",
      "Domain: food\n",
      "Extracted 8 keyphrases:\n",
      "- field of cultivated meat: 0.86\n",
      "- labgrown or cellbased meat: 0.59\n",
      "- production: 0.44\n",
      "- animal: 0.38\n",
      "- regulatory: 0.36\n",
      "- consumer: 0.33\n",
      "- cost: 0.16\n",
      "- safety: 0.14\n",
      "\n",
      "\n",
      "Processing article 11/21 (food)...\n",
      "\n",
      "Zero-Shot Domain Detection Results:\n",
      "----------------------------------------\n",
      "Top domain: food (Score: 0.2012)\n",
      "  1. food: 0.2012\n",
      "  2. agriculture: 0.2011\n",
      "  3. technology: 0.1547\n",
      "  4. environment: 0.1399\n",
      "  5. science: 0.1274\n",
      "----------------------------------------\n",
      "Threshold for 'food': 0.28\n",
      "Low confidence (0.2012) for domain 'food'.\n",
      "Domain detected using fallback classifier: agriculture (confidence: 1.0000)\n",
      "Detected domain: agriculture\n",
      "\n",
      "Zero-Shot Domain Detection Results:\n",
      "----------------------------------------\n",
      "Top domain: agriculture (Score: 0.2726)\n",
      "  1. agriculture: 0.2726\n",
      "  2. food: 0.2193\n",
      "  3. technology: 0.1739\n",
      "  4. science: 0.1451\n",
      "  5. environment: 0.1112\n",
      "----------------------------------------\n",
      "Threshold for 'agriculture': 0.25\n",
      "Extracted 3 named entities\n",
      "Generated 12 candidate keyphrases\n",
      "Semantic diversity: 0.793 → 0.793\n",
      "Keyphrase count: 12 → 12\n",
      "\n",
      "Using parameters for domain 'agriculture':\n",
      "  - base_threshold: 0.050\n",
      "  - quality_threshold: 0.350\n",
      "  - percentile: 2\n",
      "  - target_count: 8\n",
      "Using domain 'agriculture' with base threshold: 0.050, quality threshold: 0.350\n",
      "Using 2th percentile for domain 'agriculture'\n",
      "- Using 5th percentile from top: 0.785\n",
      "\n",
      "Quality Filtering Stats:\n",
      "- Domain: agriculture\n",
      "- Text Length: 381 words\n",
      "- Content Density: 0.70\n",
      "- Initial Candidates: 6\n",
      "- Base Threshold: 0.050\n",
      "- Quality Threshold: 0.350\n",
      "- After Quality Filter: 5\n",
      "Using target count 8 for domain 'agriculture'\n",
      "- Final keyphrase count: 5\n",
      "- Semantic diversity score: 0.73\n",
      "DEBUG: Extracting keyphrases with use_position_weight=True, use_tfidf_weight=True, use_ensemble=True\n",
      "DEBUG: KeyBERT extracted 96 keyphrases\n",
      "DEBUG: MultipartiteRank position weight for 'sustainable agriculture': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'water fertilizer': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'global population innovation': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'farming': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'climate change': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'crops': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'precision techniques': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'practices': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'pesticides': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'solutions': 2.00\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'sustainable agriculture': 1.34\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'water fertilizer': 0.71\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'global population innovation': 0.56\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'farming': 1.36\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'climate change': 0.94\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'crops': 1.01\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'precision techniques': 1.27\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'practices': 1.01\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'pesticides': 0.50\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'solutions': 2.00\n",
      "DEBUG: MultipartiteRank extracted 10 keyphrases\n",
      "DEBUG: YAKE position weight for 'environment agriculture cea': 2.00\n",
      "DEBUG: YAKE position weight for 'precision breeding act': 2.00\n",
      "DEBUG: YAKE position weight for 'controlled environment agriculture': 2.00\n",
      "DEBUG: YAKE position weight for 'like ch4 global': 2.00\n",
      "DEBUG: YAKE position weight for 'agriculture cea including': 2.00\n",
      "DEBUG: YAKE position weight for 'breeding techniques like': 2.00\n",
      "DEBUG: YAKE position weight for 'sound precision agriculture': 2.00\n",
      "DEBUG: YAKE position weight for 'precision agriculture remains': 2.00\n",
      "DEBUG: YAKE position weight for 'fertility controlled environment': 2.00\n",
      "DEBUG: YAKE position weight for 'companies like ch4': 2.00\n",
      "DEBUG: YAKE TF-IDF weight for 'environment agriculture cea': 1.29\n",
      "DEBUG: YAKE TF-IDF weight for 'precision breeding act': 1.19\n",
      "DEBUG: YAKE TF-IDF weight for 'controlled environment agriculture': 1.22\n",
      "DEBUG: YAKE TF-IDF weight for 'like ch4 global': 1.90\n",
      "DEBUG: YAKE TF-IDF weight for 'agriculture cea including': 1.42\n",
      "DEBUG: YAKE TF-IDF weight for 'breeding techniques like': 2.00\n",
      "DEBUG: YAKE TF-IDF weight for 'sound precision agriculture': 1.54\n",
      "DEBUG: YAKE TF-IDF weight for 'precision agriculture remains': 1.47\n",
      "DEBUG: YAKE TF-IDF weight for 'fertility controlled environment': 0.50\n",
      "DEBUG: YAKE TF-IDF weight for 'companies like ch4': 1.08\n",
      "DEBUG: YAKE extracted 10 keyphrases\n",
      "DEBUG: TextRank position weight for 'reduce agriculture': 2.00\n",
      "DEBUG: TextRank position weight for 'enhancing precision techniques': 2.00\n",
      "DEBUG: TextRank position weight for 'key driver utilizing gps sensors drones': 2.00\n",
      "DEBUG: TextRank position weight for 'weather conditions reducing': 2.00\n",
      "DEBUG: TextRank position weight for 'precision agriculture': 2.00\n",
      "DEBUG: TextRank position weight for 'agriculture practices': 2.00\n",
      "DEBUG: TextRank position weight for 'sustainable agriculture': 2.00\n",
      "DEBUG: TextRank position weight for 'agriculture cea': 2.00\n",
      "DEBUG: TextRank position weight for 'cropping systems': 2.00\n",
      "DEBUG: TextRank position weight for 'make farming more': 2.00\n",
      "DEBUG: TextRank TF-IDF weight for 'reduce agriculture': 1.66\n",
      "DEBUG: TextRank TF-IDF weight for 'enhancing precision techniques': 1.15\n",
      "DEBUG: TextRank TF-IDF weight for 'key driver utilizing gps sensors drones': 0.50\n",
      "DEBUG: TextRank TF-IDF weight for 'weather conditions reducing': 1.42\n",
      "DEBUG: TextRank TF-IDF weight for 'precision agriculture': 2.00\n",
      "DEBUG: TextRank TF-IDF weight for 'agriculture practices': 2.00\n",
      "DEBUG: TextRank TF-IDF weight for 'sustainable agriculture': 1.66\n",
      "DEBUG: TextRank TF-IDF weight for 'agriculture cea': 1.66\n",
      "DEBUG: TextRank TF-IDF weight for 'cropping systems': 0.90\n",
      "DEBUG: TextRank TF-IDF weight for 'make farming more': 1.28\n",
      "DEBUG: TextRank extracted 10 keyphrases\n",
      "DEBUG: Raw domain score for sports: 4.50\n",
      "DEBUG: Top domains detected: technology: 0.2636, environment: 0.2078, science: 0.1798\n",
      "DEBUG: All domain scores: {'technology': 0.26356589147286824, 'business': 0.031007751937984496, 'health': 0.034108527131782945, 'science': 0.17984496124031008, 'news': 0.10077519379844961, 'academic': 0.012403100775193798, 'politics': 0.09612403100775194, 'environment': 0.20775193798449612, 'entertainment': 0.046511627906976744, 'sports': 0.027906976744186046}\n",
      "DEBUG: Adaptive method weights:\n",
      "DEBUG:   - keybert: 0.3001\n",
      "DEBUG:   - multipartiterank: 0.3349\n",
      "DEBUG:   - yake: 0.0930\n",
      "DEBUG:   - textrank: 0.2719\n",
      "DEBUG: Length bonus for 'sustainable agriculture' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'climate change' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'water fertilizer' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'dual pressure' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'food security' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'environmental impact' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'key driver' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'data analytics' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'lessen environmental runoff' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'ai artificial intelligence' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'machine learning' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'sound precision agriculture' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'precision agriculture remain' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'precision agriculture' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'make farming more' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'global population innovation' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'Precision Breeding Act' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'precision technique' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'new breeding technique' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'gene edit' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'crucial role' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'nutritional value' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'chemical input' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'climate condition' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'public acceptance' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'regulatory framework' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'regenerative agriculture practice' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'soil health' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'agriculture practice' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'Controlled Environment Agriculture' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'cropping system' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'water retention' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'climate mitigation' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'vertical farming' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'urban area' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'significantly less water' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'traditional farming' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'promise solution' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'gps sensor drone' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'waste low cost' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'enhance trait' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'high yield' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'limit arable land' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'weather condition' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'transportation distance' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'associate emission innovation' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'lead lighting' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'asparagopsis seaweed' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'ch4 global' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'bio - pesticide' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'agricultural phage' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'target way' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'recent fluctuation' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'government initiative' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'sustainable practice' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'collective goal' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'food system' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'environment agriculture cea' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'precision breed act' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'control environment agriculture' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'like ch4 global' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'agriculture cea include' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'breed technique' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'fertility control environment' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'company ch4' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'reduce agriculture' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'enhance precision technique' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'key driver utilize gps sensor drone' (6 words): 1.60x\n",
      "DEBUG: Length bonus for 'weather condition reduce' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'agriculture cea' (2 words): 1.15x\n",
      "DEBUG: Ensemble combined into 112 keyphrases\n",
      "DEBUG: After redundancy removal: 81 keyphrases\n",
      "DEBUG: Before post-filtering: 81 keyphrases\n",
      "DEBUG: Multi-word phrases: 44\n",
      "DEBUG: After post-filtering: 81 keyphrases\n",
      "DEBUG: Final top 10 keyphrases: ['precision', 'global', 'water', 'cea', 'genetic', 'livestock', 'asparagopsis', 'use', 'furthermore', 'investment']\n",
      "\n",
      "Zero-Shot Domain Detection Results:\n",
      "----------------------------------------\n",
      "Top domain: food (Score: 0.2012)\n",
      "  1. food: 0.2012\n",
      "  2. agriculture: 0.2011\n",
      "  3. technology: 0.1547\n",
      "  4. environment: 0.1399\n",
      "  5. science: 0.1274\n",
      "----------------------------------------\n",
      "Threshold for 'food': 0.28\n",
      "Low confidence (0.2012) for domain 'food'.\n",
      "Domain detected using fallback classifier: agriculture (confidence: 1.0000)\n",
      "Semantic diversity: 0.793 → 0.793\n",
      "Keyphrase count: 15 → 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing articles:  52%|█████▏    | 11/21 [11:28<09:54, 59.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generic term filtering: 13 -> 11 keyphrases (2 removed)\n",
      "\n",
      "Filtered out generic terms (examples):\n",
      "- use (0.07): exact generic match\n",
      "- furthermore (0.03): exact generic match\n",
      "\n",
      "Using parameters for domain 'agriculture':\n",
      "  - base_threshold: 0.050\n",
      "  - quality_threshold: 0.350\n",
      "  - percentile: 2\n",
      "  - target_count: 8\n",
      "Using domain 'agriculture' with base threshold: 0.050, quality threshold: 0.350\n",
      "Using 2th percentile for domain 'agriculture'\n",
      "- Using 5th percentile from top: 0.734\n",
      "\n",
      "Quality Filtering Stats:\n",
      "- Domain: agriculture\n",
      "- Text Length: 381 words\n",
      "- Content Density: 0.70\n",
      "- Initial Candidates: 11\n",
      "- Base Threshold: 0.050\n",
      "- Quality Threshold: 0.350\n",
      "- After Quality Filter: 5\n",
      "Using target count 8 for domain 'agriculture'\n",
      "- Quality filter too strict, falling back to top 8 keyphrases\n",
      "- Falling back to top 8 keyphrases by score\n",
      "- Final keyphrase count: 8\n",
      "- Semantic diversity score: 0.75\n",
      "Using domain 'agriculture' with quality threshold: 0.40\n",
      "Title: Sustainable Agriculture Innovations Tackle Climate Change and Food Security\n",
      "Domain: food\n",
      "Extracted 8 keyphrases:\n",
      "- agriculture: 0.84\n",
      "- food security: 0.63\n",
      "- Precision Breeding Act: 0.55\n",
      "- climate change: 0.42\n",
      "- AI: 0.41\n",
      "- genetic: 0.26\n",
      "- investment: 0.18\n",
      "- cea: 0.16\n",
      "\n",
      "\n",
      "Processing article 12/21 (food)...\n",
      "\n",
      "Zero-Shot Domain Detection Results:\n",
      "----------------------------------------\n",
      "Top domain: agriculture (Score: 0.2197)\n",
      "  1. agriculture: 0.2197\n",
      "  2. technology: 0.2180\n",
      "  3. food: 0.1962\n",
      "  4. business: 0.1949\n",
      "  5. artificial intelligence: 0.0989\n",
      "----------------------------------------\n",
      "Threshold for 'agriculture': 0.25\n",
      "Low confidence (0.2197) for domain 'agriculture'.\n",
      "Domain detected using fallback classifier: technology (confidence: 0.4000)\n",
      "Detected domain: technology\n",
      "\n",
      "Zero-Shot Domain Detection Results:\n",
      "----------------------------------------\n",
      "Top domain: agriculture (Score: 0.2664)\n",
      "  1. agriculture: 0.2664\n",
      "  2. food: 0.2253\n",
      "  3. technology: 0.2240\n",
      "  4. business: 0.1742\n",
      "  5. world: 0.0406\n",
      "----------------------------------------\n",
      "Threshold for 'agriculture': 0.25\n",
      "Extracted 1 named entities\n",
      "Generated 9 candidate keyphrases\n",
      "Semantic diversity: 0.707 → 0.707\n",
      "Keyphrase count: 9 → 9\n",
      "\n",
      "Using parameters for domain 'technology':\n",
      "  - base_threshold: 0.070\n",
      "  - quality_threshold: 0.390\n",
      "  - percentile: 4\n",
      "  - target_count: 10\n",
      "Using domain 'technology' with base threshold: 0.070, quality threshold: 0.390\n",
      "Using 4th percentile for domain 'technology'\n",
      "- Using 5th percentile from top: 0.790\n",
      "\n",
      "Quality Filtering Stats:\n",
      "- Domain: technology\n",
      "- Text Length: 376 words\n",
      "- Content Density: 0.71\n",
      "- Initial Candidates: 8\n",
      "- Base Threshold: 0.070\n",
      "- Quality Threshold: 0.350\n",
      "- After Quality Filter: 4\n",
      "Using target count 10 for domain 'technology'\n",
      "- Too few keyphrases (4), ensuring minimum of 8\n",
      "- Final keyphrase count: 8\n",
      "- Semantic diversity score: 0.74\n",
      "DEBUG: Extracting keyphrases with use_position_weight=True, use_tfidf_weight=True, use_ensemble=True\n",
      "DEBUG: KeyBERT extracted 102 keyphrases\n",
      "DEBUG: MultipartiteRank position weight for 'food products': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'farm': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'Recent global disruptions': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'response': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'transport': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'resilience': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'transparency efficiency': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'real': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'time data': 2.00\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'food products': 2.00\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'farm': 0.50\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'Recent global disruptions': 1.27\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'response': 1.18\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'transport': 1.58\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'resilience': 1.58\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'transparency efficiency': 0.97\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'real': 1.18\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'time data': 1.18\n",
      "DEBUG: MultipartiteRank extracted 9 keyphrases\n",
      "DEBUG: YAKE position weight for 'food supply chains': 2.00\n",
      "DEBUG: YAKE position weight for 'supply chain automated': 2.00\n",
      "DEBUG: YAKE position weight for 'shorter supply chains': 2.00\n",
      "DEBUG: YAKE position weight for 'technology shorter supply': 2.00\n",
      "DEBUG: YAKE position weight for 'resilience technologies like': 2.00\n",
      "DEBUG: YAKE position weight for 'chain automated systems': 2.00\n",
      "DEBUG: YAKE position weight for 'complex food supply': 2.00\n",
      "DEBUG: YAKE position weight for 'transparent food supply': 2.00\n",
      "DEBUG: YAKE position weight for 'furthermore platforms enhancing': 2.00\n",
      "DEBUG: YAKE position weight for 'recent global disruptions': 2.00\n",
      "DEBUG: YAKE TF-IDF weight for 'food supply chains': 2.00\n",
      "DEBUG: YAKE TF-IDF weight for 'supply chain automated': 1.52\n",
      "DEBUG: YAKE TF-IDF weight for 'shorter supply chains': 1.12\n",
      "DEBUG: YAKE TF-IDF weight for 'technology shorter supply': 1.64\n",
      "DEBUG: YAKE TF-IDF weight for 'resilience technologies like': 0.80\n",
      "DEBUG: YAKE TF-IDF weight for 'chain automated systems': 0.94\n",
      "DEBUG: YAKE TF-IDF weight for 'complex food supply': 1.92\n",
      "DEBUG: YAKE TF-IDF weight for 'transparent food supply': 1.92\n",
      "DEBUG: YAKE TF-IDF weight for 'furthermore platforms enhancing': 0.60\n",
      "DEBUG: YAKE TF-IDF weight for 'recent global disruptions': 0.50\n",
      "DEBUG: YAKE extracted 10 keyphrases\n",
      "DEBUG: TextRank position weight for 'transparent food supply chains capable': 2.00\n",
      "DEBUG: TextRank position weight for 'complex food supply chains': 2.00\n",
      "DEBUG: TextRank position weight for 'supply chain automated systems': 2.00\n",
      "DEBUG: TextRank position weight for 'vast datasets including weather patterns historical demand': 2.00\n",
      "DEBUG: TextRank position weight for 'systems help coordinate complex logistics': 2.00\n",
      "DEBUG: TextRank position weight for 'supply chains supported': 2.00\n",
      "DEBUG: TextRank position weight for 'disruptions optimize inventory levels predict': 2.00\n",
      "DEBUG: TextRank position weight for 'supply chain': 2.00\n",
      "DEBUG: TextRank position weight for 'logistics information ai artificial intelligence': 2.00\n",
      "DEBUG: TextRank position weight for 'artificial intelligence ai artificial intelligence': 2.00\n",
      "DEBUG: TextRank TF-IDF weight for 'transparent food supply chains capable': 1.55\n",
      "DEBUG: TextRank TF-IDF weight for 'complex food supply chains': 1.74\n",
      "DEBUG: TextRank TF-IDF weight for 'supply chain automated systems': 1.58\n",
      "DEBUG: TextRank TF-IDF weight for 'vast datasets including weather patterns historical demand': 0.62\n",
      "DEBUG: TextRank TF-IDF weight for 'systems help coordinate complex logistics': 0.69\n",
      "DEBUG: TextRank TF-IDF weight for 'supply chains supported': 1.33\n",
      "DEBUG: TextRank TF-IDF weight for 'disruptions optimize inventory levels predict': 0.50\n",
      "DEBUG: TextRank TF-IDF weight for 'supply chain': 2.00\n",
      "DEBUG: TextRank TF-IDF weight for 'logistics information ai artificial intelligence': 1.44\n",
      "DEBUG: TextRank TF-IDF weight for 'artificial intelligence ai artificial intelligence': 1.96\n",
      "DEBUG: TextRank extracted 10 keyphrases\n",
      "DEBUG: Raw domain score for sports: 8.00\n",
      "DEBUG: Top domains detected: technology: 0.3464, business: 0.3418, environment: 0.0970\n",
      "DEBUG: All domain scores: {'technology': 0.3464203233256351, 'business': 0.3418013856812933, 'health': 0.016166281755196306, 'science': 0.011547344110854504, 'news': 0.06466512702078522, 'academic': 0.004618937644341801, 'politics': 0.050808314087759814, 'environment': 0.09699769053117784, 'entertainment': 0.03002309468822171, 'sports': 0.03695150115473441}\n",
      "DEBUG: Adaptive method weights:\n",
      "DEBUG:   - keybert: 0.3162\n",
      "DEBUG:   - multipartiterank: 0.3205\n",
      "DEBUG:   - yake: 0.0960\n",
      "DEBUG:   - textrank: 0.2674\n",
      "DEBUG: Length bonus for 'food supply chain' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'food product' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'supply chain' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'transparency efficiency' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'geopolitical conflict' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'extreme weather event' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'top priority' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'food industry' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'crucial role' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'transparent immutable record' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'product authenticity' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'complex food supply' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'complex food supply chain' (4 words): 1.45x\n",
      "DEBUG: Length bonus for 'recent global disruption' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'Artificial Intelligence' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'temperature humidity' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'optimal quality' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'proactive intervention' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'predictive analytics' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'supply chain management' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'vast datasets' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'logistics information' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'ai artificial intelligence' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'potential disruption' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'inventory level' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'consumer demand' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'real - time' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'artificial intelligence ai artificial intelligence' (5 words): 1.60x\n",
      "DEBUG: Length bonus for 'logistics information ai artificial intelligence' (5 words): 1.60x\n",
      "DEBUG: Length bonus for 'time data' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'supply automation' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'labor dependency' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'various part' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'processing plant' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'vertical farm' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'response build resilience' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'enhance visibility' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'resilience technology' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'thing sensor' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'storage facility' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'automate system' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'warehouse robotics' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'autonomous vehicle' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'labor shortage' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'complex logistics network' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'more effectively diversification' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'key resilience strategy' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'local source platform' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'control environment agriculture' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'urban center' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'associate risk' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'alternative supplier' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'primary region' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'more coordinated response' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'strategic implementation' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'future shock' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'consistent access' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'supply chain automate' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'short supply chain' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'technology short supply' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'chain automate system' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'transparent food supply' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'furthermore platform enhance' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'transparent food supply chain capable' (5 words): 1.60x\n",
      "DEBUG: Length bonus for 'supply chain automate system' (4 words): 1.45x\n",
      "DEBUG: Length bonus for 'vast datasets include weather pattern historical demand' (7 words): 1.60x\n",
      "DEBUG: Length bonus for 'system help coordinate complex logistics' (5 words): 1.60x\n",
      "DEBUG: Length bonus for 'supply chain support' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'disruption optimize inventory level predict' (5 words): 1.60x\n",
      "DEBUG: Ensemble combined into 119 keyphrases\n",
      "DEBUG: After redundancy removal: 88 keyphrases\n",
      "DEBUG: Before post-filtering: 88 keyphrases\n",
      "DEBUG: Multi-word phrases: 41\n",
      "DEBUG: After post-filtering: 88 keyphrases\n",
      "DEBUG: Final top 10 keyphrases: ['technology', 'automation', 'transport', 'information', 'efficiency', 'furthermore', 'diversification', 'predictive', 'advanced', 'blockchain']\n",
      "\n",
      "Zero-Shot Domain Detection Results:\n",
      "----------------------------------------\n",
      "Top domain: agriculture (Score: 0.2197)\n",
      "  1. agriculture: 0.2197\n",
      "  2. technology: 0.2180\n",
      "  3. food: 0.1962\n",
      "  4. business: 0.1949\n",
      "  5. artificial intelligence: 0.0989\n",
      "----------------------------------------\n",
      "Threshold for 'agriculture': 0.25\n",
      "Low confidence (0.2197) for domain 'agriculture'.\n",
      "Domain detected using fallback classifier: technology (confidence: 0.4000)\n",
      "Semantic diversity: 0.770 → 0.770\n",
      "Keyphrase count: 17 → 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing articles:  57%|█████▋    | 12/21 [12:41<09:33, 63.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generic term filtering: 17 -> 14 keyphrases (3 removed)\n",
      "\n",
      "Filtered out generic terms (examples):\n",
      "- information (0.22): exact generic match\n",
      "- advanced (0.14): exact generic match\n",
      "- furthermore (0.06): exact generic match\n",
      "\n",
      "Using parameters for domain 'technology':\n",
      "  - base_threshold: 0.070\n",
      "  - quality_threshold: 0.390\n",
      "  - percentile: 4\n",
      "  - target_count: 10\n",
      "Using domain 'technology' with base threshold: 0.070, quality threshold: 0.390\n",
      "Using 4th percentile for domain 'technology'\n",
      "- Using 5th percentile from top: 0.734\n",
      "\n",
      "Quality Filtering Stats:\n",
      "- Domain: technology\n",
      "- Text Length: 376 words\n",
      "- Content Density: 0.71\n",
      "- Initial Candidates: 14\n",
      "- Base Threshold: 0.070\n",
      "- Quality Threshold: 0.350\n",
      "- After Quality Filter: 7\n",
      "Using target count 10 for domain 'technology'\n",
      "- Quality filter too strict, falling back to top 10 keyphrases\n",
      "- Using lenient threshold 0.175: 14 keyphrases\n",
      "- Too many keyphrases (14), selecting 12 diverse ones\n",
      "- Final keyphrase count: 12\n",
      "- Semantic diversity score: 0.75\n",
      "Using domain 'technology' with quality threshold: 0.55\n",
      "Title: Food Supply Chain Resilience Becomes Critical Focus with New Technologies\n",
      "Domain: food\n",
      "Extracted 8 keyphrases:\n",
      "- food supply chain: 0.85\n",
      "- recent global disruption: 0.67\n",
      "- IoT: 0.66\n",
      "- global disruption: 0.65\n",
      "- blockchain: 0.57\n",
      "- automation: 0.51\n",
      "- technology: 0.49\n",
      "- transport: 0.34\n",
      "\n",
      "\n",
      "Processing article 13/21 (environment)...\n",
      "\n",
      "Zero-Shot Domain Detection Results:\n",
      "----------------------------------------\n",
      "Top domain: technology (Score: 0.4326)\n",
      "  1. technology: 0.4326\n",
      "  2. environment: 0.2927\n",
      "  3. world: 0.1892\n",
      "  4. business: 0.0530\n",
      "  5. science: 0.0307\n",
      "----------------------------------------\n",
      "Threshold for 'technology': 0.45\n",
      "Using secondary domain 'environment' (Score: 0.2927, Threshold: 0.28)\n",
      "Detected domain: environment\n",
      "\n",
      "Zero-Shot Domain Detection Results:\n",
      "----------------------------------------\n",
      "Top domain: technology (Score: 0.4670)\n",
      "  1. technology: 0.4670\n",
      "  2. environment: 0.3170\n",
      "  3. world: 0.1324\n",
      "  4. business: 0.0615\n",
      "  5. science: 0.0177\n",
      "----------------------------------------\n",
      "Threshold for 'technology': 0.45\n",
      "Extracted 0 named entities\n",
      "Generated 15 candidate keyphrases\n",
      "Semantic diversity: 0.796 → 0.796\n",
      "Keyphrase count: 15 → 15\n",
      "\n",
      "Using parameters for domain 'environment':\n",
      "  - base_threshold: 0.080\n",
      "  - quality_threshold: 0.400\n",
      "  - percentile: 4\n",
      "  - target_count: 9\n",
      "Using domain 'environment' with base threshold: 0.080, quality threshold: 0.400\n",
      "Using 4th percentile for domain 'environment'\n",
      "- Using 4th percentile from top: 0.792\n",
      "\n",
      "Quality Filtering Stats:\n",
      "- Domain: environment\n",
      "- Text Length: 372 words\n",
      "- Content Density: 0.67\n",
      "- Initial Candidates: 10\n",
      "- Base Threshold: 0.080\n",
      "- Quality Threshold: 0.350\n",
      "- After Quality Filter: 5\n",
      "Using target count 9 for domain 'environment'\n",
      "- Quality filter too strict, falling back to top 9 keyphrases\n",
      "- Falling back to top 9 keyphrases by score\n",
      "- Final keyphrase count: 9\n",
      "- Semantic diversity score: 0.74\n",
      "DEBUG: Extracting keyphrases with use_position_weight=True, use_tfidf_weight=True, use_ensemble=True\n",
      "DEBUG: KeyBERT extracted 84 keyphrases\n",
      "DEBUG: MultipartiteRank position weight for 'solar wind': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'renewable energy sources': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'wind power installations': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'integration energy storage': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'large companies': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'cost': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'electricity grids': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'hydropower': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'term': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'global transition': 2.00\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'solar wind': 1.52\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'renewable energy sources': 2.00\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'wind power installations': 1.43\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'integration energy storage': 1.77\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'large companies': 1.34\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'cost': 0.88\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'electricity grids': 0.97\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'hydropower': 0.50\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'term': 0.88\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'global transition': 0.64\n",
      "DEBUG: MultipartiteRank extracted 10 keyphrases\n",
      "DEBUG: YAKE position weight for 'purchase agreements ppas': 2.00\n",
      "DEBUG: YAKE position weight for 'corporate power purchase': 2.00\n",
      "DEBUG: YAKE position weight for 'power purchase agreements': 2.00\n",
      "DEBUG: YAKE position weight for 'vre like solar': 2.00\n",
      "DEBUG: YAKE position weight for 'renewable energy vre': 2.00\n",
      "DEBUG: YAKE position weight for 'energy vre like': 2.00\n",
      "DEBUG: YAKE position weight for 'like solar wind': 2.00\n",
      "DEBUG: YAKE position weight for 'large renewable energy': 2.00\n",
      "DEBUG: YAKE position weight for 'scale projects despite': 2.00\n",
      "DEBUG: YAKE position weight for 'regions corporate power': 2.00\n",
      "DEBUG: YAKE TF-IDF weight for 'purchase agreements ppas': 0.50\n",
      "DEBUG: YAKE TF-IDF weight for 'corporate power purchase': 0.78\n",
      "DEBUG: YAKE TF-IDF weight for 'power purchase agreements': 0.91\n",
      "DEBUG: YAKE TF-IDF weight for 'vre like solar': 0.92\n",
      "DEBUG: YAKE TF-IDF weight for 'renewable energy vre': 1.51\n",
      "DEBUG: YAKE TF-IDF weight for 'energy vre like': 1.39\n",
      "DEBUG: YAKE TF-IDF weight for 'like solar wind': 1.24\n",
      "DEBUG: YAKE TF-IDF weight for 'large renewable energy': 2.00\n",
      "DEBUG: YAKE TF-IDF weight for 'scale projects despite': 0.87\n",
      "DEBUG: YAKE TF-IDF weight for 'regions corporate power': 0.97\n",
      "DEBUG: YAKE extracted 10 keyphrases\n",
      "DEBUG: TextRank position weight for 'energy storage solutions battery storage technology': 2.00\n",
      "DEBUG: TextRank position weight for 'large renewable energy projects': 2.00\n",
      "DEBUG: TextRank position weight for 'grid integration energy storage': 2.00\n",
      "DEBUG: TextRank position weight for 'renewable energy long': 2.00\n",
      "DEBUG: TextRank position weight for 'renewable energy': 2.00\n",
      "DEBUG: TextRank position weight for 'renewable generation large': 2.00\n",
      "DEBUG: TextRank position weight for 'using renewable electricity': 2.00\n",
      "DEBUG: TextRank position weight for 'energy storage': 2.00\n",
      "DEBUG: TextRank position weight for 'renewable generation sites': 2.00\n",
      "DEBUG: TextRank position weight for 'wind power': 2.00\n",
      "DEBUG: TextRank TF-IDF weight for 'energy storage solutions battery storage technology': 1.58\n",
      "DEBUG: TextRank TF-IDF weight for 'large renewable energy projects': 1.79\n",
      "DEBUG: TextRank TF-IDF weight for 'grid integration energy storage': 1.22\n",
      "DEBUG: TextRank TF-IDF weight for 'renewable energy long': 1.49\n",
      "DEBUG: TextRank TF-IDF weight for 'renewable energy': 2.00\n",
      "DEBUG: TextRank TF-IDF weight for 'renewable generation large': 1.49\n",
      "DEBUG: TextRank TF-IDF weight for 'using renewable electricity': 0.50\n",
      "DEBUG: TextRank TF-IDF weight for 'energy storage': 1.54\n",
      "DEBUG: TextRank TF-IDF weight for 'renewable generation sites': 0.74\n",
      "DEBUG: TextRank TF-IDF weight for 'wind power': 1.05\n",
      "DEBUG: TextRank extracted 10 keyphrases\n",
      "DEBUG: Raw domain score for sports: 16.00\n",
      "DEBUG: Top domains detected: environment: 0.5560, business: 0.1101, technology: 0.0826\n",
      "DEBUG: All domain scores: {'technology': 0.08256880733944955, 'business': 0.11009174311926606, 'health': 0.0, 'science': 0.04954128440366973, 'news': 0.03486238532110092, 'academic': 0.025688073394495414, 'politics': 0.04220183486238532, 'environment': 0.5559633027522936, 'entertainment': 0.04036697247706422, 'sports': 0.05871559633027523}\n",
      "DEBUG: Adaptive method weights:\n",
      "DEBUG:   - keybert: 0.3075\n",
      "DEBUG:   - multipartiterank: 0.2934\n",
      "DEBUG:   - yake: 0.1371\n",
      "DEBUG:   - textrank: 0.2620\n",
      "DEBUG: Length bonus for 'solar wind' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'renewable energy source' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'energy storage' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'integration energy storage' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'wind power installation' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'global transition' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'climate change' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'however significant challenge' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'persist solar' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'record level' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'many part' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'photovoltaic cell efficiency' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'wind turbine design' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'fossil fuel' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'power purchase agreement' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'corporate power purchase' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'renewable energy' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'grid integration energy storage' (4 words): 1.45x\n",
      "DEBUG: Length bonus for 'wind power' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'major driver' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'new capacity addition' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'key role' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'renewable portfolio standard' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'competitive auction' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'large amount' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'substantial upgrade' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'renewable energy vre' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'energy vre' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'renewable energy long' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'renewable generation site' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'electricity grid' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'flexible grid operation' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'significant investment' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'other storage solution' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'green hydrogen production' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'renewable electricity' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'renewable generation large' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'fall technology' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'supportive government policy' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'mount urgency' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'large company' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'exist electricity grid' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'fluctuate power flow' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'manage intermittency' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'high cost' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'supply chain constraint' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'key material' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'lithium cobalt' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'rare earth element' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'deployment rate' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'public acceptance' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'further consideration' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'decarbonize energy system' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'regulatory process' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'critical step' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'international climate goal' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'purchase agreement ppas' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'vre solar' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'large renewable energy' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'scale project despite' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'region corporate power' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'energy storage solution battery storage technology' (6 words): 1.60x\n",
      "DEBUG: Length bonus for 'large renewable energy project' (4 words): 1.45x\n",
      "DEBUG: Length bonus for 'use renewable electricity' (3 words): 1.30x\n",
      "DEBUG: Ensemble combined into 100 keyphrases\n",
      "DEBUG: After redundancy removal: 65 keyphrases\n",
      "DEBUG: Before post-filtering: 65 keyphrases\n",
      "DEBUG: Multi-word phrases: 32\n",
      "DEBUG: Filtered out single word 'term' as it's part of multi-word phrase(s)\n",
      "DEBUG: After post-filtering: 64 keyphrases\n",
      "DEBUG: Final top 10 keyphrases: ['large', 'power', 'solar', 'wind', 'battery', 'deployment', 'public', 'other', 'grid', 'cost']\n",
      "\n",
      "Zero-Shot Domain Detection Results:\n",
      "----------------------------------------\n",
      "Top domain: technology (Score: 0.4326)\n",
      "  1. technology: 0.4326\n",
      "  2. environment: 0.2927\n",
      "  3. world: 0.1892\n",
      "  4. business: 0.0530\n",
      "  5. science: 0.0307\n",
      "----------------------------------------\n",
      "Threshold for 'technology': 0.45\n",
      "Using secondary domain 'environment' (Score: 0.2927, Threshold: 0.28)\n",
      "Semantic diversity: 0.776 → 0.776\n",
      "Keyphrase count: 16 → 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing articles:  62%|██████▏   | 13/21 [13:37<08:09, 61.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generic term filtering: 14 -> 13 keyphrases (1 removed)\n",
      "\n",
      "Filtered out generic terms (examples):\n",
      "- large (0.03): exact generic match\n",
      "\n",
      "Using parameters for domain 'environment':\n",
      "  - base_threshold: 0.080\n",
      "  - quality_threshold: 0.400\n",
      "  - percentile: 4\n",
      "  - target_count: 9\n",
      "Using domain 'environment' with base threshold: 0.080, quality threshold: 0.400\n",
      "Using 4th percentile for domain 'environment'\n",
      "- Using 4th percentile from top: 0.792\n",
      "\n",
      "Quality Filtering Stats:\n",
      "- Domain: environment\n",
      "- Text Length: 372 words\n",
      "- Content Density: 0.67\n",
      "- Initial Candidates: 13\n",
      "- Base Threshold: 0.080\n",
      "- Quality Threshold: 0.350\n",
      "- After Quality Filter: 6\n",
      "Using target count 9 for domain 'environment'\n",
      "- Quality filter too strict, falling back to top 9 keyphrases\n",
      "- Using lenient threshold 0.175: 10 keyphrases\n",
      "- Final keyphrase count: 10\n",
      "- Semantic diversity score: 0.72\n",
      "Using domain 'environment' with quality threshold: 0.45\n",
      "Title: Global Renewable Energy Adoption Accelerates, But Challenges Remain\n",
      "Domain: environment\n",
      "Extracted 8 keyphrases:\n",
      "- solar: 0.81\n",
      "- energy source: 0.78\n",
      "- hydropower: 0.77\n",
      "- Wind power installation: 0.62\n",
      "- photovoltaic cell efficiency: 0.47\n",
      "- grid: 0.43\n",
      "- battery: 0.34\n",
      "- fuel: 0.25\n",
      "\n",
      "\n",
      "Processing article 14/21 (environment)...\n",
      "\n",
      "Zero-Shot Domain Detection Results:\n",
      "----------------------------------------\n",
      "Top domain: environment (Score: 0.5726)\n",
      "  1. environment: 0.5726\n",
      "  2. science: 0.3425\n",
      "  3. politics: 0.0614\n",
      "  4. world: 0.0206\n",
      "  5. technology: 0.0009\n",
      "----------------------------------------\n",
      "Threshold for 'environment': 0.28\n",
      "Detected domain: environment\n",
      "\n",
      "Zero-Shot Domain Detection Results:\n",
      "----------------------------------------\n",
      "Top domain: environment (Score: 0.5221)\n",
      "  1. environment: 0.5221\n",
      "  2. science: 0.2335\n",
      "  3. world: 0.1444\n",
      "  4. politics: 0.0953\n",
      "  5. health: 0.0021\n",
      "----------------------------------------\n",
      "Threshold for 'environment': 0.28\n",
      "Extracted 2 named entities\n",
      "Generated 8 candidate keyphrases\n",
      "Semantic diversity: 0.754 → 0.754\n",
      "Keyphrase count: 8 → 8\n",
      "\n",
      "Using parameters for domain 'environment':\n",
      "  - base_threshold: 0.080\n",
      "  - quality_threshold: 0.400\n",
      "  - percentile: 4\n",
      "  - target_count: 9\n",
      "Using domain 'environment' with base threshold: 0.080, quality threshold: 0.400\n",
      "Using 4th percentile for domain 'environment'\n",
      "- Using 4th percentile from top: 0.859\n",
      "\n",
      "Quality Filtering Stats:\n",
      "- Domain: environment\n",
      "- Text Length: 382 words\n",
      "- Content Density: 0.69\n",
      "- Initial Candidates: 4\n",
      "- Base Threshold: 0.080\n",
      "- Quality Threshold: 0.350\n",
      "- After Quality Filter: 3\n",
      "Using target count 9 for domain 'environment'\n",
      "- Final keyphrase count: 3\n",
      "- Semantic diversity score: 0.73\n",
      "DEBUG: Extracting keyphrases with use_position_weight=True, use_tfidf_weight=True, use_ensemble=True\n",
      "DEBUG: KeyBERT extracted 97 keyphrases\n",
      "DEBUG: MultipartiteRank position weight for 'Paris Agreement': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'climate action': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'sufficient global cooperation': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'concrete domestic policies': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'climate change': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'international goals': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'commitments': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'countries': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'nations': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'economic backdrop': 2.00\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'Paris Agreement': 1.67\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'climate action': 2.00\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'sufficient global cooperation': 0.97\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'concrete domestic policies': 0.65\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'climate change': 1.44\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'international goals': 1.12\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'commitments': 0.80\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'countries': 1.01\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'nations': 0.80\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'economic backdrop': 0.50\n",
      "DEBUG: MultipartiteRank extracted 10 keyphrases\n",
      "DEBUG: YAKE position weight for 'determined contributions ndcs': 2.00\n",
      "DEBUG: YAKE position weight for 'nationally determined contributions': 2.00\n",
      "DEBUG: YAKE position weight for 'paris agreement face': 2.00\n",
      "DEBUG: YAKE position weight for 'paris agreement framework': 2.00\n",
      "DEBUG: YAKE position weight for 'paris agreement ultimately': 2.00\n",
      "DEBUG: YAKE position weight for 'agreement framework relies': 2.00\n",
      "DEBUG: YAKE position weight for 'agreement ultimately depends': 2.00\n",
      "DEBUG: YAKE position weight for 'action trade disputes': 2.00\n",
      "DEBUG: YAKE position weight for 'trade disputes particularly': 2.00\n",
      "DEBUG: YAKE position weight for 'policies globally despite': 2.00\n",
      "DEBUG: YAKE TF-IDF weight for 'determined contributions ndcs': 0.83\n",
      "DEBUG: YAKE TF-IDF weight for 'nationally determined contributions': 0.50\n",
      "DEBUG: YAKE TF-IDF weight for 'paris agreement face': 2.00\n",
      "DEBUG: YAKE TF-IDF weight for 'paris agreement framework': 2.00\n",
      "DEBUG: YAKE TF-IDF weight for 'paris agreement ultimately': 2.00\n",
      "DEBUG: YAKE TF-IDF weight for 'agreement framework relies': 1.75\n",
      "DEBUG: YAKE TF-IDF weight for 'agreement ultimately depends': 1.75\n",
      "DEBUG: YAKE TF-IDF weight for 'action trade disputes': 1.17\n",
      "DEBUG: YAKE TF-IDF weight for 'trade disputes particularly': 0.83\n",
      "DEBUG: YAKE TF-IDF weight for 'policies globally despite': 1.25\n",
      "DEBUG: YAKE extracted 10 keyphrases\n",
      "DEBUG: TextRank position weight for 'international climate cooperation ensuring': 2.00\n",
      "DEBUG: TextRank position weight for 'include financing climate action': 2.00\n",
      "DEBUG: TextRank position weight for 'international climate': 2.00\n",
      "DEBUG: TextRank position weight for 'climate policies globally': 2.00\n",
      "DEBUG: TextRank position weight for 'emissions reductions globally requires stronger political': 2.00\n",
      "DEBUG: TextRank position weight for 'achieving sufficient global cooperation remains challenging': 2.00\n",
      "DEBUG: TextRank position weight for 'provide climate finance': 2.00\n",
      "DEBUG: TextRank position weight for 'greenhouse gas emissions periodic stocktakes assess': 2.00\n",
      "DEBUG: TextRank position weight for 'climate action': 2.00\n",
      "DEBUG: TextRank position weight for 'domestic climate': 2.00\n",
      "DEBUG: TextRank TF-IDF weight for 'international climate cooperation ensuring': 1.52\n",
      "DEBUG: TextRank TF-IDF weight for 'include financing climate action': 1.03\n",
      "DEBUG: TextRank TF-IDF weight for 'international climate': 2.00\n",
      "DEBUG: TextRank TF-IDF weight for 'climate policies globally': 1.40\n",
      "DEBUG: TextRank TF-IDF weight for 'emissions reductions globally requires stronger political': 0.97\n",
      "DEBUG: TextRank TF-IDF weight for 'achieving sufficient global cooperation remains challenging': 0.71\n",
      "DEBUG: TextRank TF-IDF weight for 'provide climate finance': 0.92\n",
      "DEBUG: TextRank TF-IDF weight for 'greenhouse gas emissions periodic stocktakes assess': 0.50\n",
      "DEBUG: TextRank TF-IDF weight for 'climate action': 1.61\n",
      "DEBUG: TextRank TF-IDF weight for 'domestic climate': 1.39\n",
      "DEBUG: TextRank extracted 10 keyphrases\n",
      "DEBUG: Raw domain score for sports: 9.00\n",
      "DEBUG: Top domains detected: politics: 0.2861, environment: 0.1959, business: 0.1907\n",
      "DEBUG: All domain scores: {'technology': 0.06443298969072164, 'business': 0.19072164948453607, 'health': 0.0, 'science': 0.023195876288659795, 'news': 0.14175257731958762, 'academic': 0.03865979381443299, 'politics': 0.2860824742268041, 'environment': 0.1958762886597938, 'entertainment': 0.01288659793814433, 'sports': 0.04639175257731959}\n",
      "DEBUG: Adaptive method weights:\n",
      "DEBUG:   - keybert: 0.2963\n",
      "DEBUG:   - multipartiterank: 0.3232\n",
      "DEBUG:   - yake: 0.1021\n",
      "DEBUG:   - textrank: 0.2784\n",
      "DEBUG: Length bonus for 'paris agreement' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'paris agreement framework' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'paris agreement face' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'climate action' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'nationally Determined contribution' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'climate change' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'sufficient global cooperation' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'international goal' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'critical period' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'scientific consensus' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'own target' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'greenhouse gas emission' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'periodic stocktake' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'collective progress' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'economic backdrop' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'recent assessment' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'current ndcs' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'global warming' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'agreement 's target' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'ambition gap' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'central focus' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'key issue' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'climate impact' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'climate finance' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'carbon market' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'international cooperation' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'international climate' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'provide climate finance' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'robust rule' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'economic pressure' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'energy security' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'carbon border adjustment' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'international climate cooperation' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'fossil fuel' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'concrete domestic policy' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'emission reduction' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'green technology' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'political support' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'climate policy' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'positive signal' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'many country' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'renewable energy' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'private sector' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'climate risk' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'grow corporate commitment' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'international platform' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'specialized un body' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'international collaboration' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'increase financial flow' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'effective implementation' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'major economy' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'critical decade' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'determine contribution ndcs' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'nationally determine contribution' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'paris agreement ultimately' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'agreement framework rely' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'agreement ultimately depend' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'action trade dispute' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'trade dispute particularly' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'policy globally despite' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'international climate cooperation ensure' (4 words): 1.45x\n",
      "DEBUG: Length bonus for 'include finance climate action' (4 words): 1.45x\n",
      "DEBUG: Length bonus for 'climate policy globally' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'emission reduction globally require strong political' (6 words): 1.60x\n",
      "DEBUG: Length bonus for 'achieve sufficient global cooperation remain challenge' (6 words): 1.60x\n",
      "DEBUG: Length bonus for 'greenhouse gas emission periodic stocktake ass' (6 words): 1.60x\n",
      "DEBUG: Length bonus for 'domestic climate' (2 words): 1.15x\n",
      "DEBUG: Ensemble combined into 114 keyphrases\n",
      "DEBUG: After redundancy removal: 82 keyphrases\n",
      "DEBUG: Before post-filtering: 82 keyphrases\n",
      "DEBUG: Multi-word phrases: 39\n",
      "DEBUG: After post-filtering: 82 keyphrases\n",
      "DEBUG: Final top 10 keyphrases: ['international', 'global', 'action', 'cooperation', 'geopolitical', 'key', 'ambition', 'particularly', 'many', 'however']\n",
      "\n",
      "Zero-Shot Domain Detection Results:\n",
      "----------------------------------------\n",
      "Top domain: environment (Score: 0.5726)\n",
      "  1. environment: 0.5726\n",
      "  2. science: 0.3425\n",
      "  3. politics: 0.0614\n",
      "  4. world: 0.0206\n",
      "  5. technology: 0.0009\n",
      "----------------------------------------\n",
      "Threshold for 'environment': 0.28\n",
      "Semantic diversity: 0.787 → 0.787\n",
      "Keyphrase count: 13 → 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing articles:  67%|██████▋   | 14/21 [14:45<07:23, 63.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generic term filtering: 13 -> 11 keyphrases (2 removed)\n",
      "\n",
      "Filtered out generic terms (examples):\n",
      "- key (0.10): exact generic match\n",
      "- many (0.05): exact generic match\n",
      "\n",
      "Using parameters for domain 'environment':\n",
      "  - base_threshold: 0.080\n",
      "  - quality_threshold: 0.400\n",
      "  - percentile: 4\n",
      "  - target_count: 9\n",
      "Using domain 'environment' with base threshold: 0.080, quality threshold: 0.400\n",
      "Using 4th percentile for domain 'environment'\n",
      "- Using 4th percentile from top: 0.841\n",
      "\n",
      "Quality Filtering Stats:\n",
      "- Domain: environment\n",
      "- Text Length: 382 words\n",
      "- Content Density: 0.69\n",
      "- Initial Candidates: 11\n",
      "- Base Threshold: 0.080\n",
      "- Quality Threshold: 0.350\n",
      "- After Quality Filter: 5\n",
      "Using target count 9 for domain 'environment'\n",
      "- Quality filter too strict, falling back to top 9 keyphrases\n",
      "- Using lenient threshold 0.175: 9 keyphrases\n",
      "- Final keyphrase count: 9\n",
      "- Semantic diversity score: 0.73\n",
      "Using domain 'environment' with quality threshold: 0.45\n",
      "Title: International Climate Agreements Face Crucial Test Amidst Geopolitical Landscape\n",
      "Domain: environment\n",
      "Extracted 8 keyphrases:\n",
      "- the Paris Agreement: 0.86\n",
      "- climate change: 0.82\n",
      "- geopolitical: 0.55\n",
      "- Nationally Determined Contributions: 0.53\n",
      "- cooperation: 0.39\n",
      "- action: 0.27\n",
      "- global: 0.26\n",
      "- international: 0.22\n",
      "\n",
      "\n",
      "Processing article 15/21 (environment)...\n",
      "\n",
      "Zero-Shot Domain Detection Results:\n",
      "----------------------------------------\n",
      "Top domain: science (Score: 0.3489)\n",
      "  1. science: 0.3489\n",
      "  2. environment: 0.3314\n",
      "  3. world: 0.2821\n",
      "  4. space: 0.0164\n",
      "  5. health: 0.0151\n",
      "----------------------------------------\n",
      "Threshold for 'science': 0.40\n",
      "Using secondary domain 'environment' (Score: 0.3314, Threshold: 0.28)\n",
      "Detected domain: environment\n",
      "\n",
      "Zero-Shot Domain Detection Results:\n",
      "----------------------------------------\n",
      "Top domain: science (Score: 0.3814)\n",
      "  1. science: 0.3814\n",
      "  2. environment: 0.3534\n",
      "  3. world: 0.2427\n",
      "  4. agriculture: 0.0080\n",
      "  5. technology: 0.0069\n",
      "----------------------------------------\n",
      "Threshold for 'science': 0.40\n",
      "Using secondary domain 'environment' (Score: 0.3534, Threshold: 0.28)\n",
      "Extracted 1 named entities\n",
      "Generated 16 candidate keyphrases\n",
      "Semantic diversity: 0.757 → 0.757\n",
      "Keyphrase count: 16 → 16\n",
      "Recovered keyphrase: habitat destruction (score: 0.8555)\n",
      "Recovered keyphrase: Habitat (score: 0.7790)\n",
      "Recovered keyphrase: resource (score: 0.3069)\n",
      "\n",
      "Generic term filtering: 11 -> 10 keyphrases (1 removed)\n",
      "\n",
      "Filtered out generic terms (examples):\n",
      "- resource (0.31): exact generic match\n",
      "\n",
      "Using parameters for domain 'environment':\n",
      "  - base_threshold: 0.060\n",
      "  - quality_threshold: 0.370\n",
      "  - percentile: 3\n",
      "  - target_count: 9\n",
      "Using domain 'environment' with base threshold: 0.060, quality threshold: 0.370\n",
      "Using 3th percentile for domain 'environment'\n",
      "- Using 5th percentile from top: 0.852\n",
      "\n",
      "Quality Filtering Stats:\n",
      "- Domain: environment\n",
      "- Text Length: 384 words\n",
      "- Content Density: 0.71\n",
      "- Initial Candidates: 10\n",
      "- Base Threshold: 0.060\n",
      "- Quality Threshold: 0.350\n",
      "- After Quality Filter: 8\n",
      "Using target count 9 for domain 'environment'\n",
      "- Quality filter too strict, falling back to top 9 keyphrases\n",
      "- Using lenient threshold 0.175: 9 keyphrases\n",
      "- Final keyphrase count: 9\n",
      "- Semantic diversity score: 0.64\n",
      "DEBUG: Extracting keyphrases with use_position_weight=True, use_tfidf_weight=True, use_ensemble=True\n",
      "DEBUG: KeyBERT extracted 82 keyphrases\n",
      "DEBUG: MultipartiteRank position weight for 'biodiversity': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'wildlife climate change': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'degradation': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'resources': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'pollution': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'ecosystems': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'ranges': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'habitats': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'industrial waste': 2.00\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'biodiversity': 2.00\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'wildlife climate change': 1.67\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'degradation': 0.50\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'resources': 1.03\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'pollution': 1.57\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'ecosystems': 1.74\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'ranges': 0.50\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'habitats': 1.35\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'industrial waste': 0.87\n",
      "DEBUG: MultipartiteRank extracted 9 keyphrases\n",
      "DEBUG: YAKE position weight for 'montreal global biodiversity': 2.00\n",
      "DEBUG: YAKE position weight for 'species integrating biodiversity': 2.00\n",
      "DEBUG: YAKE position weight for 'global biodiversity framework': 2.00\n",
      "DEBUG: YAKE position weight for 'invasive species scientific': 2.00\n",
      "DEBUG: YAKE position weight for 'invasive species integrating': 2.00\n",
      "DEBUG: YAKE position weight for 'climate change pollution': 2.00\n",
      "DEBUG: YAKE position weight for 'lost biodiversity tackling': 2.00\n",
      "DEBUG: YAKE position weight for 'integrating biodiversity considerations': 2.00\n",
      "DEBUG: YAKE position weight for 'biodiversity conservation strategies': 2.00\n",
      "DEBUG: YAKE position weight for 'conservation outcomes despite': 2.00\n",
      "DEBUG: YAKE TF-IDF weight for 'montreal global biodiversity': 1.33\n",
      "DEBUG: YAKE TF-IDF weight for 'species integrating biodiversity': 2.00\n",
      "DEBUG: YAKE TF-IDF weight for 'global biodiversity framework': 1.61\n",
      "DEBUG: YAKE TF-IDF weight for 'invasive species scientific': 1.02\n",
      "DEBUG: YAKE TF-IDF weight for 'invasive species integrating': 1.02\n",
      "DEBUG: YAKE TF-IDF weight for 'climate change pollution': 1.55\n",
      "DEBUG: YAKE TF-IDF weight for 'lost biodiversity tackling': 0.50\n",
      "DEBUG: YAKE TF-IDF weight for 'integrating biodiversity considerations': 0.50\n",
      "DEBUG: YAKE TF-IDF weight for 'biodiversity conservation strategies': 1.60\n",
      "DEBUG: YAKE TF-IDF weight for 'conservation outcomes despite': 1.22\n",
      "DEBUG: YAKE extracted 10 keyphrases\n",
      "DEBUG: TextRank position weight for 'actions expanding protected areas national parks marine reserves remains': 2.00\n",
      "DEBUG: TextRank position weight for 'restoring degraded ecosystems reducing pollution': 2.00\n",
      "DEBUG: TextRank position weight for 'biodiversity conservation': 2.00\n",
      "DEBUG: TextRank position weight for 'global biodiversity framework': 2.00\n",
      "DEBUG: TextRank position weight for 'habitats e g coral': 2.00\n",
      "DEBUG: TextRank position weight for 'managing invasive species': 2.00\n",
      "DEBUG: TextRank position weight for 'ambitious targets including protecting': 2.00\n",
      "DEBUG: TextRank position weight for 'biodiversity loss': 2.00\n",
      "DEBUG: TextRank position weight for 'species extinction rates': 2.00\n",
      "DEBUG: TextRank position weight for 'ecosystems pollution': 2.00\n",
      "DEBUG: TextRank TF-IDF weight for 'actions expanding protected areas national parks marine reserves remains': 0.99\n",
      "DEBUG: TextRank TF-IDF weight for 'restoring degraded ecosystems reducing pollution': 1.12\n",
      "DEBUG: TextRank TF-IDF weight for 'biodiversity conservation': 2.00\n",
      "DEBUG: TextRank TF-IDF weight for 'global biodiversity framework': 1.41\n",
      "DEBUG: TextRank TF-IDF weight for 'habitats e g coral': 0.99\n",
      "DEBUG: TextRank TF-IDF weight for 'managing invasive species': 1.17\n",
      "DEBUG: TextRank TF-IDF weight for 'ambitious targets including protecting': 0.50\n",
      "DEBUG: TextRank TF-IDF weight for 'biodiversity loss': 1.33\n",
      "DEBUG: TextRank TF-IDF weight for 'species extinction rates': 1.31\n",
      "DEBUG: TextRank TF-IDF weight for 'ecosystems pollution': 1.31\n",
      "DEBUG: TextRank extracted 10 keyphrases\n",
      "DEBUG: Raw domain score for sports: 26.00\n",
      "DEBUG: Top domains detected: environment: 0.4009, science: 0.1705, business: 0.0941\n",
      "DEBUG: All domain scores: {'technology': 0.06090808416389812, 'business': 0.09413067552602436, 'health': 0.0, 'science': 0.17054263565891473, 'news': 0.08748615725359911, 'academic': 0.03875968992248062, 'politics': 0.06423034330011074, 'environment': 0.4008859357696567, 'entertainment': 0.02547065337763012, 'sports': 0.05758582502768549}\n",
      "DEBUG: Adaptive method weights:\n",
      "DEBUG:   - keybert: 0.3033\n",
      "DEBUG:   - multipartiterank: 0.3151\n",
      "DEBUG:   - yake: 0.1024\n",
      "DEBUG:   - textrank: 0.2792\n",
      "DEBUG: Length bonus for 'species extinction rate' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'global decline' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'alarming rate' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'not only nature' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'invasive specie' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'climate change pollution' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'invasive species scientific' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'wildlife climate change' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'industrial waste' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'extreme weather event' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'ambitious target' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'planet 's land' (3 words): 1.30x\n",
      "DEBUG: Length bonus for '30x30 goal' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'degraded ecosystem' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'financial resource' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'global biodiversity framework' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'montreal global biodiversity' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'biodiversity conservation' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'ecosystems pollution' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'biodiversity loss' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'wide range' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'degraded habitat' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'circular economy model' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'illegal wildlife trade' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'biodiversity consideration' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'economic planning' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'essential service ecosystem' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'specie ' range' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'plastic pesticide' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'biodiversity conservation strategy' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'protect area' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'lose biodiversity' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'decision - make' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'vital technological advancement' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'species detection' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'ai artificial intelligence' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'ecological data' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'powerful new tool' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'conservation monitoring' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'management community involvement' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'indigenous people' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'local community' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'grow toolkit' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'conservation approach' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'specie integrate biodiversity' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'invasive specie integrate' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'lose biodiversity tackling' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'integrate biodiversity consideration' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'conservation outcome despite' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'action expand protect area national park marine reserve remain' (9 words): 1.60x\n",
      "DEBUG: Length bonus for 'restore degraded ecosystem reduce pollution' (5 words): 1.60x\n",
      "DEBUG: Length bonus for 'habitat e g coral' (4 words): 1.45x\n",
      "DEBUG: Length bonus for 'manage invasive specie' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'ambitious target include protect' (4 words): 1.45x\n",
      "DEBUG: Ensemble combined into 93 keyphrases\n",
      "DEBUG: After redundancy removal: 59 keyphrases\n",
      "DEBUG: Before post-filtering: 59 keyphrases\n",
      "DEBUG: Multi-word phrases: 23\n",
      "DEBUG: After post-filtering: 59 keyphrases\n",
      "DEBUG: Final top 10 keyphrases: ['conservation', 'climate', 'biodiversity', 'pollution', 'wildlife', 'natural', 'community', 'ecosystem', 'waste', 'framework']\n",
      "\n",
      "Zero-Shot Domain Detection Results:\n",
      "----------------------------------------\n",
      "Top domain: science (Score: 0.3489)\n",
      "  1. science: 0.3489\n",
      "  2. environment: 0.3314\n",
      "  3. world: 0.2821\n",
      "  4. space: 0.0164\n",
      "  5. health: 0.0151\n",
      "----------------------------------------\n",
      "Threshold for 'science': 0.40\n",
      "Using secondary domain 'environment' (Score: 0.3314, Threshold: 0.28)\n",
      "Semantic diversity: 0.639 → 0.639\n",
      "Keyphrase count: 16 → 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing articles:  71%|███████▏  | 15/21 [15:40<06:04, 60.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generic term filtering: 13 -> 11 keyphrases (2 removed)\n",
      "\n",
      "Filtered out generic terms (examples):\n",
      "- ecosystem (0.84): exact generic match\n",
      "- wildlife (0.81): exact generic match\n",
      "\n",
      "Using parameters for domain 'environment':\n",
      "  - base_threshold: 0.060\n",
      "  - quality_threshold: 0.370\n",
      "  - percentile: 3\n",
      "  - target_count: 9\n",
      "Using domain 'environment' with base threshold: 0.060, quality threshold: 0.370\n",
      "Using 3th percentile for domain 'environment'\n",
      "- Using 5th percentile from top: 0.848\n",
      "\n",
      "Quality Filtering Stats:\n",
      "- Domain: environment\n",
      "- Text Length: 384 words\n",
      "- Content Density: 0.71\n",
      "- Initial Candidates: 11\n",
      "- Base Threshold: 0.060\n",
      "- Quality Threshold: 0.350\n",
      "- After Quality Filter: 7\n",
      "Using target count 9 for domain 'environment'\n",
      "- Quality filter too strict, falling back to top 9 keyphrases\n",
      "- Falling back to top 9 keyphrases by score\n",
      "- Final keyphrase count: 9\n",
      "- Semantic diversity score: 0.66\n",
      "Using domain 'environment' with quality threshold: 0.45\n",
      "Title: Biodiversity Crisis Deepens, Conservation Efforts Scale Up Response\n",
      "Domain: environment\n",
      "Extracted 8 keyphrases:\n",
      "- driven primarily by habitat destruction: 0.85\n",
      "- biodiversity: 0.84\n",
      "- mass extinction event: 0.84\n",
      "- climate change: 0.83\n",
      "- pollution: 0.80\n",
      "- conservation: 0.77\n",
      "- global decline: 0.73\n",
      "- waste: 0.33\n",
      "\n",
      "\n",
      "Processing article 16/21 (real estate)...\n",
      "\n",
      "Zero-Shot Domain Detection Results:\n",
      "----------------------------------------\n",
      "Top domain: real estate (Score: 0.3867)\n",
      "  1. real estate: 0.3867\n",
      "  2. world: 0.2615\n",
      "  3. business: 0.2118\n",
      "  4. space: 0.1295\n",
      "  5. education: 0.0039\n",
      "----------------------------------------\n",
      "Threshold for 'real estate': 0.32\n",
      "Detected domain: real estate\n",
      "\n",
      "Zero-Shot Domain Detection Results:\n",
      "----------------------------------------\n",
      "Top domain: real estate (Score: 0.4963)\n",
      "  1. real estate: 0.4963\n",
      "  2. world: 0.4046\n",
      "  3. business: 0.0936\n",
      "  4. politics: 0.0010\n",
      "  5. education: 0.0007\n",
      "----------------------------------------\n",
      "Threshold for 'real estate': 0.32\n",
      "Extracted 0 named entities\n",
      "Generated 7 candidate keyphrases\n",
      "Semantic diversity: 0.743 → 0.743\n",
      "Keyphrase count: 7 → 7\n",
      "\n",
      "Using parameters for domain 'real estate':\n",
      "  - base_threshold: 0.080\n",
      "  - quality_threshold: 0.400\n",
      "  - percentile: 4\n",
      "  - target_count: 9\n",
      "Using domain 'real estate' with base threshold: 0.080, quality threshold: 0.400\n",
      "Using 4th percentile for domain 'real estate'\n",
      "- Using 4th percentile from top: 0.820\n",
      "\n",
      "Quality Filtering Stats:\n",
      "- Domain: real estate\n",
      "- Text Length: 376 words\n",
      "- Content Density: 0.65\n",
      "- Initial Candidates: 4\n",
      "- Base Threshold: 0.080\n",
      "- Quality Threshold: 0.350\n",
      "- After Quality Filter: 2\n",
      "Using target count 9 for domain 'real estate'\n",
      "- Final keyphrase count: 2\n",
      "- Semantic diversity score: 0.73\n",
      "DEBUG: Extracting keyphrases with use_position_weight=True, use_tfidf_weight=True, use_ensemble=True\n",
      "DEBUG: KeyBERT extracted 91 keyphrases\n",
      "DEBUG: MultipartiteRank position weight for 'Housing markets': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'price growth': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'persistent affordability challenges': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'markets': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'many': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'housing demand': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'inflation': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'mortgages': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'tentative signs': 2.00\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'Housing markets': 1.86\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'price growth': 2.00\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'persistent affordability challenges': 1.01\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'markets': 1.87\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'many': 1.48\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'housing demand': 1.65\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'inflation': 0.99\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'mortgages': 0.50\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'tentative signs': 0.84\n",
      "DEBUG: MultipartiteRank extracted 9 keyphrases\n",
      "DEBUG: YAKE position weight for 'demand building activity': 2.00\n",
      "DEBUG: YAKE position weight for 'weaker demand building': 2.00\n",
      "DEBUG: YAKE position weight for 'developed economies transaction': 2.00\n",
      "DEBUG: YAKE position weight for 'economies transaction volumes': 2.00\n",
      "DEBUG: YAKE position weight for 'transaction volumes slowed': 2.00\n",
      "DEBUG: YAKE position weight for 'critical issue decades': 2.00\n",
      "DEBUG: YAKE position weight for 'regulatory hurdles regional': 2.00\n",
      "DEBUG: YAKE position weight for 'hurdles regional variations': 2.00\n",
      "DEBUG: YAKE position weight for 'tenants looking ahead': 2.00\n",
      "DEBUG: YAKE position weight for 'downturns conversely markets': 2.00\n",
      "DEBUG: YAKE TF-IDF weight for 'demand building activity': 2.00\n",
      "DEBUG: YAKE TF-IDF weight for 'weaker demand building': 1.17\n",
      "DEBUG: YAKE TF-IDF weight for 'developed economies transaction': 1.44\n",
      "DEBUG: YAKE TF-IDF weight for 'economies transaction volumes': 1.50\n",
      "DEBUG: YAKE TF-IDF weight for 'transaction volumes slowed': 0.50\n",
      "DEBUG: YAKE TF-IDF weight for 'critical issue decades': 0.71\n",
      "DEBUG: YAKE TF-IDF weight for 'regulatory hurdles regional': 0.77\n",
      "DEBUG: YAKE TF-IDF weight for 'hurdles regional variations': 0.50\n",
      "DEBUG: YAKE TF-IDF weight for 'tenants looking ahead': 0.50\n",
      "DEBUG: YAKE TF-IDF weight for 'downturns conversely markets': 1.90\n",
      "DEBUG: YAKE extracted 10 keyphrases\n",
      "DEBUG: TextRank position weight for 'relative price performance rental markets': 2.00\n",
      "DEBUG: TextRank position weight for 'face pressure high homeownership costs': 2.00\n",
      "DEBUG: TextRank position weight for 'housing affordability remains': 2.00\n",
      "DEBUG: TextRank position weight for 'potential interest rate': 2.00\n",
      "DEBUG: TextRank position weight for 'price growth': 2.00\n",
      "DEBUG: TextRank position weight for 'housing markets': 2.00\n",
      "DEBUG: TextRank position weight for 'moderate price': 2.00\n",
      "DEBUG: TextRank position weight for 'price corrections': 2.00\n",
      "DEBUG: TextRank position weight for 'elevated borrowing costs': 2.00\n",
      "DEBUG: TextRank position weight for 'price surges': 2.00\n",
      "DEBUG: TextRank TF-IDF weight for 'relative price performance rental markets': 1.69\n",
      "DEBUG: TextRank TF-IDF weight for 'face pressure high homeownership costs': 0.91\n",
      "DEBUG: TextRank TF-IDF weight for 'housing affordability remains': 0.72\n",
      "DEBUG: TextRank TF-IDF weight for 'potential interest rate': 1.14\n",
      "DEBUG: TextRank TF-IDF weight for 'price growth': 2.00\n",
      "DEBUG: TextRank TF-IDF weight for 'housing markets': 1.72\n",
      "DEBUG: TextRank TF-IDF weight for 'moderate price': 0.50\n",
      "DEBUG: TextRank TF-IDF weight for 'price corrections': 1.05\n",
      "DEBUG: TextRank TF-IDF weight for 'elevated borrowing costs': 0.87\n",
      "DEBUG: TextRank TF-IDF weight for 'price surges': 0.50\n",
      "DEBUG: TextRank extracted 10 keyphrases\n",
      "DEBUG: Raw domain score for sports: 6.00\n",
      "DEBUG: Top domains detected: business: 0.4751, entertainment: 0.1510, technology: 0.0994\n",
      "DEBUG: All domain scores: {'technology': 0.09944751381215469, 'business': 0.47513812154696133, 'health': 0.04419889502762431, 'science': 0.02578268876611418, 'news': 0.06077348066298342, 'academic': 0.0, 'politics': 0.09944751381215469, 'environment': 0.0, 'entertainment': 0.15101289134438306, 'sports': 0.04419889502762431}\n",
      "DEBUG: Adaptive method weights:\n",
      "DEBUG:   - keybert: 0.2768\n",
      "DEBUG:   - multipartiterank: 0.2839\n",
      "DEBUG:   - yake: 0.2089\n",
      "DEBUG:   - textrank: 0.2305\n",
      "DEBUG: Length bonus for 'housing market' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'price growth' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'persistent affordability challenge' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'price correction' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'tentative sign' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'transaction volume' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'interest rate' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'central bank' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'housing demand' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'cautious optimism' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'borrowing cost' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'housing affordability' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'critical issue' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'wage increase' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'recent surge' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'mortgage rate' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'many potential buyer' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'desirable location' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'many area' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'demand building activity' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'potential interest rate' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'critical issue decade' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'housing affordability remain' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'supply shortage' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'regulatory hurdle' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'regional variation' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'strong population growth' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'conversely market' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'pandemic boom' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'suburban rural area' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'migration pattern' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'high homeownership cost' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'more people' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'rental sector' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'high interest rate' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'many developed economy' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'steep decline' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'significant market' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'resilient economy' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'less expensive city' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'many city' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'national economy' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'major crash' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'most stable economy' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'tight supply' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'homeowner equity' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'time affordability constraint' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'market activity' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'precede boom year' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'weak demand building' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'develop economy transaction' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'economy transaction volume' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'transaction volume slow' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'regulatory hurdle regional' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'hurdle regional variation' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'tenant look ahead' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'downturn conversely market' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'relative price performance rental market' (5 words): 1.60x\n",
      "DEBUG: Length bonus for 'face pressure high homeownership cost' (5 words): 1.60x\n",
      "DEBUG: Length bonus for 'moderate price' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'elevate borrowing cost' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'price surge' (2 words): 1.15x\n",
      "DEBUG: Ensemble combined into 107 keyphrases\n",
      "DEBUG: After redundancy removal: 66 keyphrases\n",
      "DEBUG: Before post-filtering: 66 keyphrases\n",
      "DEBUG: Multi-word phrases: 27\n",
      "DEBUG: After post-filtering: 66 keyphrases\n",
      "DEBUG: Final top 10 keyphrases: ['supply', 'housing', 'price', 'rental', 'high', 'affordability', 'demand', 'performance', 'market', 'regional']\n",
      "\n",
      "Zero-Shot Domain Detection Results:\n",
      "----------------------------------------\n",
      "Top domain: real estate (Score: 0.3867)\n",
      "  1. real estate: 0.3867\n",
      "  2. world: 0.2615\n",
      "  3. business: 0.2118\n",
      "  4. space: 0.1295\n",
      "  5. education: 0.0039\n",
      "----------------------------------------\n",
      "Threshold for 'real estate': 0.32\n",
      "Semantic diversity: 0.717 → 0.717\n",
      "Keyphrase count: 12 → 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing articles:  76%|███████▌  | 16/21 [16:41<05:04, 60.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generic term filtering: 10 -> 9 keyphrases (1 removed)\n",
      "\n",
      "Filtered out generic terms (examples):\n",
      "- high (0.12): exact generic match\n",
      "\n",
      "Using parameters for domain 'real estate':\n",
      "  - base_threshold: 0.080\n",
      "  - quality_threshold: 0.400\n",
      "  - percentile: 4\n",
      "  - target_count: 9\n",
      "Using domain 'real estate' with base threshold: 0.080, quality threshold: 0.400\n",
      "Using 4th percentile for domain 'real estate'\n",
      "- Using 4th percentile from top: 0.820\n",
      "\n",
      "Quality Filtering Stats:\n",
      "- Domain: real estate\n",
      "- Text Length: 376 words\n",
      "- Content Density: 0.65\n",
      "- Initial Candidates: 9\n",
      "- Base Threshold: 0.080\n",
      "- Quality Threshold: 0.350\n",
      "- After Quality Filter: 4\n",
      "Using target count 9 for domain 'real estate'\n",
      "- Quality filter too strict, falling back to top 9 keyphrases\n",
      "- Falling back to top 9 keyphrases by score\n",
      "- Final keyphrase count: 9\n",
      "- Semantic diversity score: 0.72\n",
      "Using domain 'real estate' with quality threshold: 0.40\n",
      "Title: Global Housing Markets Navigate Shifting Sands of Interest Rates and Affordability\n",
      "Domain: real estate\n",
      "Extracted 8 keyphrases:\n",
      "- housing market: 0.88\n",
      "- affordability: 0.70\n",
      "- demand: 0.43\n",
      "- volatility: 0.38\n",
      "- supply: 0.33\n",
      "- price: 0.28\n",
      "- rental: 0.25\n",
      "- regional: 0.24\n",
      "\n",
      "\n",
      "Processing article 17/21 (real estate)...\n",
      "Domain detected using BART: real estate (confidence: 0.6136)\n",
      "Detected domain: real estate\n",
      "Domain detected using BART: real estate (confidence: 0.7561)\n",
      "Extracted 3 named entities\n",
      "Generated 15 candidate keyphrases\n",
      "Semantic diversity: 0.814 → 0.814\n",
      "Keyphrase count: 15 → 15\n",
      "\n",
      "Using parameters for domain 'real estate':\n",
      "  - base_threshold: 0.080\n",
      "  - quality_threshold: 0.400\n",
      "  - percentile: 4\n",
      "  - target_count: 9\n",
      "Using domain 'real estate' with base threshold: 0.080, quality threshold: 0.400\n",
      "Using 4th percentile for domain 'real estate'\n",
      "- Using 4th percentile from top: 0.779\n",
      "\n",
      "Quality Filtering Stats:\n",
      "- Domain: real estate\n",
      "- Text Length: 403 words\n",
      "- Content Density: 0.66\n",
      "- Initial Candidates: 13\n",
      "- Base Threshold: 0.080\n",
      "- Quality Threshold: 0.350\n",
      "- After Quality Filter: 7\n",
      "Using target count 9 for domain 'real estate'\n",
      "- Quality filter too strict, falling back to top 9 keyphrases\n",
      "- Using lenient threshold 0.175: 11 keyphrases\n",
      "- Final keyphrase count: 11\n",
      "- Semantic diversity score: 0.84\n",
      "DEBUG: Extracting keyphrases with use_position_weight=True, use_tfidf_weight=True, use_ensemble=True\n",
      "DEBUG: KeyBERT extracted 86 keyphrases\n",
      "DEBUG: MultipartiteRank position weight for 'office market': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'remote': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'tenant': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'hybrid work models': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'employees': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'significant transformation': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'COVID-19 pandemic': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'modern amenities': 2.00\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'office market': 2.00\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'remote': 0.94\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'tenant': 0.50\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'hybrid work models': 1.60\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'employees': 1.25\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'significant transformation': 0.50\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'COVID-19 pandemic': 0.51\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'modern amenities': 0.68\n",
      "DEBUG: MultipartiteRank extracted 8 keyphrases\n",
      "DEBUG: YAKE position weight for 'new york city': 2.00\n",
      "DEBUG: YAKE position weight for 'like new york': 2.00\n",
      "DEBUG: YAKE position weight for 'estate cre sector': 2.00\n",
      "DEBUG: YAKE position weight for 'cre sector particularly': 2.00\n",
      "DEBUG: YAKE position weight for 'real estate cre': 2.00\n",
      "DEBUG: YAKE position weight for 'work era innovation': 2.00\n",
      "DEBUG: YAKE position weight for 'numerous companies research': 2.00\n",
      "DEBUG: YAKE position weight for 'companies research indicates': 2.00\n",
      "DEBUG: YAKE position weight for 'hubs like new': 2.00\n",
      "DEBUG: YAKE position weight for 'commerce fulfillment industrial': 2.00\n",
      "DEBUG: YAKE TF-IDF weight for 'new york city': 0.97\n",
      "DEBUG: YAKE TF-IDF weight for 'like new york': 0.53\n",
      "DEBUG: YAKE TF-IDF weight for 'estate cre sector': 2.00\n",
      "DEBUG: YAKE TF-IDF weight for 'cre sector particularly': 1.95\n",
      "DEBUG: YAKE TF-IDF weight for 'real estate cre': 1.34\n",
      "DEBUG: YAKE TF-IDF weight for 'work era innovation': 1.54\n",
      "DEBUG: YAKE TF-IDF weight for 'numerous companies research': 1.11\n",
      "DEBUG: YAKE TF-IDF weight for 'companies research indicates': 1.20\n",
      "DEBUG: YAKE TF-IDF weight for 'hubs like new': 0.50\n",
      "DEBUG: YAKE TF-IDF weight for 'commerce fulfillment industrial': 0.60\n",
      "DEBUG: YAKE extracted 10 keyphrases\n",
      "DEBUG: TextRank position weight for 'event capabilities fitness centers improved air quality systems communal lounges soundproof': 2.00\n",
      "DEBUG: TextRank position weight for 'office demand': 2.00\n",
      "DEBUG: TextRank position weight for 'office vacancy rates': 2.00\n",
      "DEBUG: TextRank position weight for 'office market': 2.00\n",
      "DEBUG: TextRank position weight for '- office': 2.00\n",
      "DEBUG: TextRank position weight for 'overall office': 2.00\n",
      "DEBUG: TextRank position weight for 'shared offices': 2.00\n",
      "DEBUG: TextRank position weight for 'office': 2.00\n",
      "DEBUG: TextRank position weight for 'logistics tenant demands': 2.00\n",
      "DEBUG: TextRank position weight for 'hybrid work models': 2.00\n",
      "DEBUG: TextRank TF-IDF weight for 'event capabilities fitness centers improved air quality systems communal lounges soundproof': 1.00\n",
      "DEBUG: TextRank TF-IDF weight for 'office demand': 1.63\n",
      "DEBUG: TextRank TF-IDF weight for 'office vacancy rates': 1.57\n",
      "DEBUG: TextRank TF-IDF weight for 'office market': 1.56\n",
      "DEBUG: TextRank TF-IDF weight for '- office': 2.00\n",
      "DEBUG: TextRank TF-IDF weight for 'overall office': 1.38\n",
      "DEBUG: TextRank TF-IDF weight for 'shared offices': 0.50\n",
      "DEBUG: TextRank TF-IDF weight for 'office': 1.77\n",
      "DEBUG: TextRank TF-IDF weight for 'logistics tenant demands': 0.78\n",
      "DEBUG: TextRank TF-IDF weight for 'hybrid work models': 1.32\n",
      "DEBUG: TextRank extracted 10 keyphrases\n",
      "DEBUG: Raw domain score for sports: 10.00\n",
      "DEBUG: Top domains detected: business: 0.2852, health: 0.1497, technology: 0.1212\n",
      "DEBUG: All domain scores: {'technology': 0.12121212121212122, 'business': 0.28520499108734404, 'health': 0.1497326203208556, 'science': 0.0748663101604278, 'news': 0.11586452762923351, 'academic': 0.053475935828877004, 'politics': 0.0392156862745098, 'environment': 0.06417112299465241, 'entertainment': 0.024955436720142603, 'sports': 0.07130124777183601}\n",
      "DEBUG: Adaptive method weights:\n",
      "DEBUG:   - keybert: 0.2980\n",
      "DEBUG:   - multipartiterank: 0.3164\n",
      "DEBUG:   - yake: 0.0993\n",
      "DEBUG:   - textrank: 0.2863\n",
      "DEBUG: Length bonus for 'office demand' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'significant transformation' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'initial fear' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'complete office exodus' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'many major market' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'hybrid model' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'office market' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'estate cre sector' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'cre sector particularly' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'hybrid work model' (3 words): 1.30x\n",
      "DEBUG: Length bonus for '- office' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'office vacancy rate' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'real estate cre' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'COVID-19 pandemic' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'new york city' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'overall office occupancy' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'major hub' (2 words): 1.15x\n",
      "DEBUG: Length bonus for '2019 level' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'foreseeable future' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'property manager' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'noticeable flight' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'potential pressure' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'other us' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'new york' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'overall office' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'logistics tenant demand' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'office experience' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'foster collaboration' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'competitive market' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'significant investment' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'remote work' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'geographic demand' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'major city center' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'suburban office market' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'evolve tenant' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'numerous company research' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'project office value' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'modern amenity' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'high vacancy rate' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'increase interest' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'short lease term' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'focus work' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'retain tenant' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'increase activity' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'small satellite office' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'retail segment' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'continue focus' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'experiential retail' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'high demand' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'ongoing growth' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'online shopping' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'recalibration success hinge' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'flexibility adaptability' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'evolve need' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'property technology proptech' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'work era innovation' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'company research indicate' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'hubs new' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'commerce fulfillment industrial' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'event capability fitness center improve air quality system communal lounge soundproof' (11 words): 1.60x\n",
      "DEBUG: Length bonus for 'share office' (2 words): 1.15x\n",
      "DEBUG: Ensemble combined into 99 keyphrases\n",
      "DEBUG: After redundancy removal: 62 keyphrases\n",
      "DEBUG: Before post-filtering: 62 keyphrases\n",
      "DEBUG: Multi-word phrases: 29\n",
      "DEBUG: Filtered out single word 'cre' as it's part of multi-word phrase(s)\n",
      "DEBUG: After post-filtering: 61 keyphrases\n",
      "DEBUG: Final top 10 keyphrases: ['office', 'overall', 'logistics', 'flexibility', 'pandemic', 'focus', 'research', 'property', 'quality', 'industrial']\n",
      "Domain detected using BART: real estate (confidence: 0.6136)\n",
      "Semantic diversity: 0.798 → 0.798\n",
      "Keyphrase count: 18 → 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing articles:  81%|████████  | 17/21 [17:37<03:57, 59.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using parameters for domain 'real estate':\n",
      "  - base_threshold: 0.080\n",
      "  - quality_threshold: 0.400\n",
      "  - percentile: 4\n",
      "  - target_count: 9\n",
      "Using domain 'real estate' with base threshold: 0.080, quality threshold: 0.400\n",
      "Using 4th percentile for domain 'real estate'\n",
      "- Using 4th percentile from top: 0.764\n",
      "\n",
      "Quality Filtering Stats:\n",
      "- Domain: real estate\n",
      "- Text Length: 403 words\n",
      "- Content Density: 0.66\n",
      "- Initial Candidates: 16\n",
      "- Base Threshold: 0.080\n",
      "- Quality Threshold: 0.350\n",
      "- After Quality Filter: 8\n",
      "Using target count 9 for domain 'real estate'\n",
      "- Quality filter too strict, falling back to top 9 keyphrases\n",
      "- Using lenient threshold 0.175: 13 keyphrases\n",
      "- Too many keyphrases (13), selecting 12 diverse ones\n",
      "- Final keyphrase count: 12\n",
      "- Semantic diversity score: 0.80\n",
      "Using domain 'real estate' with quality threshold: 0.40\n",
      "Title: Commercial Real Estate Adapts to Hybrid Work Models and Tenant Demands\n",
      "Domain: real estate\n",
      "Extracted 9 keyphrases:\n",
      "- particularly the office market: 0.84\n",
      "- office exodus: 0.72\n",
      "- CRE: 0.44\n",
      "- logistics: 0.41\n",
      "- work model: 0.40\n",
      "- landscape: 0.37\n",
      "- COVID-19: 0.35\n",
      "- estate sector: 0.35\n",
      "- property: 0.35\n",
      "\n",
      "\n",
      "Processing article 18/21 (real estate)...\n",
      "\n",
      "Zero-Shot Domain Detection Results:\n",
      "----------------------------------------\n",
      "Top domain: environment (Score: 0.3907)\n",
      "  1. environment: 0.3907\n",
      "  2. real estate: 0.2843\n",
      "  3. technology: 0.0960\n",
      "  4. business: 0.0955\n",
      "  5. education: 0.0521\n",
      "----------------------------------------\n",
      "Threshold for 'environment': 0.28\n",
      "Detected domain: environment\n",
      "Domain detected using BART: real estate (confidence: 0.6701)\n",
      "Extracted 2 named entities\n",
      "Generated 15 candidate keyphrases\n",
      "Semantic diversity: 0.789 → 0.789\n",
      "Keyphrase count: 15 → 15\n",
      "\n",
      "Using parameters for domain 'environment':\n",
      "  - base_threshold: 0.080\n",
      "  - quality_threshold: 0.400\n",
      "  - percentile: 4\n",
      "  - target_count: 9\n",
      "Using domain 'environment' with base threshold: 0.080, quality threshold: 0.400\n",
      "Using 4th percentile for domain 'environment'\n",
      "- Using 4th percentile from top: 0.821\n",
      "\n",
      "Quality Filtering Stats:\n",
      "- Domain: environment\n",
      "- Text Length: 382 words\n",
      "- Content Density: 0.70\n",
      "- Initial Candidates: 10\n",
      "- Base Threshold: 0.080\n",
      "- Quality Threshold: 0.350\n",
      "- After Quality Filter: 6\n",
      "Using target count 9 for domain 'environment'\n",
      "- Quality filter too strict, falling back to top 9 keyphrases\n",
      "- Using lenient threshold 0.175: 10 keyphrases\n",
      "- Final keyphrase count: 10\n",
      "- Semantic diversity score: 0.78\n",
      "DEBUG: Extracting keyphrases with use_position_weight=True, use_tfidf_weight=True, use_ensemble=True\n",
      "DEBUG: KeyBERT extracted 86 keyphrases\n",
      "DEBUG: MultipartiteRank position weight for 'Sustainability': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'performance windows energy': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'lighting': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'real estate development': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'building materials': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'ventilation green': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'Environmental Design': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'natural light': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'efficient hvac systems': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'April': 2.00\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'Sustainability': 1.02\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'performance windows energy': 1.89\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'lighting': 0.50\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'real estate development': 1.80\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'building materials': 2.00\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'ventilation green': 1.03\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'Environmental Design': 1.75\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'natural light': 1.41\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'efficient hvac systems': 1.12\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'April': 0.50\n",
      "DEBUG: MultipartiteRank extracted 10 keyphrases\n",
      "DEBUG: YAKE position weight for 'sustainable building practices': 2.00\n",
      "DEBUG: YAKE position weight for 'research establishment environmental': 2.00\n",
      "DEBUG: YAKE position weight for 'establishment environmental assessment': 2.00\n",
      "DEBUG: YAKE position weight for 'environmental assessment method': 2.00\n",
      "DEBUG: YAKE position weight for 'breeam building research': 2.00\n",
      "DEBUG: YAKE position weight for 'building research establishment': 2.00\n",
      "DEBUG: YAKE position weight for 'real estate development': 2.00\n",
      "DEBUG: YAKE position weight for 'like leed leadership': 2.00\n",
      "DEBUG: YAKE position weight for 'hvac systems led': 2.00\n",
      "DEBUG: YAKE position weight for 'governance esg criteria': 2.00\n",
      "DEBUG: YAKE TF-IDF weight for 'sustainable building practices': 2.00\n",
      "DEBUG: YAKE TF-IDF weight for 'research establishment environmental': 1.57\n",
      "DEBUG: YAKE TF-IDF weight for 'establishment environmental assessment': 1.14\n",
      "DEBUG: YAKE TF-IDF weight for 'environmental assessment method': 1.47\n",
      "DEBUG: YAKE TF-IDF weight for 'breeam building research': 1.07\n",
      "DEBUG: YAKE TF-IDF weight for 'building research establishment': 1.27\n",
      "DEBUG: YAKE TF-IDF weight for 'real estate development': 1.66\n",
      "DEBUG: YAKE TF-IDF weight for 'like leed leadership': 0.97\n",
      "DEBUG: YAKE TF-IDF weight for 'hvac systems led': 0.66\n",
      "DEBUG: YAKE TF-IDF weight for 'governance esg criteria': 0.50\n",
      "DEBUG: YAKE extracted 10 keyphrases\n",
      "DEBUG: TextRank position weight for 'awareness grows sustainable building practices': 2.00\n",
      "DEBUG: TextRank position weight for 'growing environmental awareness stricter regulations tenant demand': 2.00\n",
      "DEBUG: TextRank position weight for 'sustainable building practices': 2.00\n",
      "DEBUG: TextRank position weight for 'sustainable buildings': 2.00\n",
      "DEBUG: TextRank position weight for 'stricter building energy': 2.00\n",
      "DEBUG: TextRank position weight for 'building materials': 2.00\n",
      "DEBUG: TextRank position weight for 'reduce operational energy': 2.00\n",
      "DEBUG: TextRank position weight for 'green building': 2.00\n",
      "DEBUG: TextRank position weight for 'incorporating environmental social': 2.00\n",
      "DEBUG: TextRank position weight for 'development practices focus': 2.00\n",
      "DEBUG: TextRank TF-IDF weight for 'awareness grows sustainable building practices': 1.53\n",
      "DEBUG: TextRank TF-IDF weight for 'growing environmental awareness stricter regulations tenant demand': 2.00\n",
      "DEBUG: TextRank TF-IDF weight for 'sustainable building practices': 1.82\n",
      "DEBUG: TextRank TF-IDF weight for 'sustainable buildings': 0.50\n",
      "DEBUG: TextRank TF-IDF weight for 'stricter building energy': 1.96\n",
      "DEBUG: TextRank TF-IDF weight for 'building materials': 1.58\n",
      "DEBUG: TextRank TF-IDF weight for 'reduce operational energy': 1.43\n",
      "DEBUG: TextRank TF-IDF weight for 'green building': 1.37\n",
      "DEBUG: TextRank TF-IDF weight for 'incorporating environmental social': 1.29\n",
      "DEBUG: TextRank TF-IDF weight for 'development practices focus': 0.99\n",
      "DEBUG: TextRank extracted 10 keyphrases\n",
      "DEBUG: Raw domain score for sports: 10.50\n",
      "DEBUG: Top domains detected: environment: 0.4133, business: 0.1891, news: 0.0916\n",
      "DEBUG: All domain scores: {'technology': 0.08187134502923976, 'business': 0.18908382066276802, 'health': 0.009746588693957114, 'science': 0.023391812865497075, 'news': 0.09161793372319688, 'academic': 0.005847953216374269, 'politics': 0.06627680311890838, 'environment': 0.41325536062378165, 'entertainment': 0.07797270955165692, 'sports': 0.04093567251461988}\n",
      "DEBUG: Adaptive method weights:\n",
      "DEBUG:   - keybert: 0.3054\n",
      "DEBUG:   - multipartiterank: 0.3133\n",
      "DEBUG:   - yake: 0.1009\n",
      "DEBUG:   - textrank: 0.2804\n",
      "DEBUG: Length bonus for 'sustainable building practice' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'real estate development' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'natural light' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'ventilation green' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'sustainable building' (2 words): 1.15x\n",
      "DEBUG: Length bonus for '2025 sustainability' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'niche concern' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'core consideration' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'investment sector' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'investor pressure' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'significant momentum' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'energy efficiency' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'primary focus' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'real estate' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'performance windows energy' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'reduce operational energy' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'green building' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'efficient hvac system' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'Environmental Design' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'leed leadership' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'sustainable performance' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'environmental impact' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'construction waste' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'research establishment environmental' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'environmental assessment method' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'establishment environmental assessment' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'breeam building research' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'new development' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'green infrastructure' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'green roof' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'local biodiversity' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'social aspect' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'healthy indoor environment' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'good air quality' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'buyer demand' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'Environmental Social' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'smart building technology' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'passive strategy' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'build certification' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'build material' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'grow emphasis' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'locally source product' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'environmental factor sustainability' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'local community tenant' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'sustainable property' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'low utility bill' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'well - being' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'personal value' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'long run' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'upfront cost' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'sustainable construction' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'reduce operational expense' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'standard procedure' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'optional extra' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'build research establishment' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'hvac system lead' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'governance esg criterion' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'awareness grow sustainable building practice' (5 words): 1.60x\n",
      "DEBUG: Length bonus for 'grow environmental awareness strict regulation tenant demand' (7 words): 1.60x\n",
      "DEBUG: Length bonus for 'strict building energy' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'incorporate environmental social' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'development practice focus' (3 words): 1.30x\n",
      "DEBUG: Ensemble combined into 100 keyphrases\n",
      "DEBUG: After redundancy removal: 72 keyphrases\n",
      "DEBUG: Before post-filtering: 72 keyphrases\n",
      "DEBUG: Multi-word phrases: 36\n",
      "DEBUG: After post-filtering: 72 keyphrases\n",
      "DEBUG: Final top 10 keyphrases: ['energy', 'sustainable', 'sustainability', 'water', 'reduce', 'life', 'awareness', 'governance', 'leadership', 'there']\n",
      "\n",
      "Zero-Shot Domain Detection Results:\n",
      "----------------------------------------\n",
      "Top domain: environment (Score: 0.3907)\n",
      "  1. environment: 0.3907\n",
      "  2. real estate: 0.2843\n",
      "  3. technology: 0.0960\n",
      "  4. business: 0.0955\n",
      "  5. education: 0.0521\n",
      "----------------------------------------\n",
      "Threshold for 'environment': 0.28\n",
      "Semantic diversity: 0.724 → 0.724\n",
      "Keyphrase count: 17 → 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing articles:  86%|████████▌ | 18/21 [18:37<02:58, 59.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generic term filtering: 15 -> 14 keyphrases (1 removed)\n",
      "\n",
      "Filtered out generic terms (examples):\n",
      "- water (0.25): exact generic match\n",
      "\n",
      "Using parameters for domain 'environment':\n",
      "  - base_threshold: 0.080\n",
      "  - quality_threshold: 0.400\n",
      "  - percentile: 4\n",
      "  - target_count: 9\n",
      "Using domain 'environment' with base threshold: 0.080, quality threshold: 0.400\n",
      "Using 4th percentile for domain 'environment'\n",
      "- Using 4th percentile from top: 0.840\n",
      "\n",
      "Quality Filtering Stats:\n",
      "- Domain: environment\n",
      "- Text Length: 382 words\n",
      "- Content Density: 0.70\n",
      "- Initial Candidates: 14\n",
      "- Base Threshold: 0.080\n",
      "- Quality Threshold: 0.350\n",
      "- After Quality Filter: 8\n",
      "Using target count 9 for domain 'environment'\n",
      "- Quality filter too strict, falling back to top 9 keyphrases\n",
      "- Using lenient threshold 0.175: 12 keyphrases\n",
      "- Final keyphrase count: 12\n",
      "- Semantic diversity score: 0.70\n",
      "Using domain 'environment' with quality threshold: 0.45\n",
      "Title: Sustainable Building Practices Gain Momentum in Real Estate Development\n",
      "Domain: real estate\n",
      "Extracted 8 keyphrases:\n",
      "- sustainability: 0.85\n",
      "- environmental awareness: 0.83\n",
      "- sustainable: 0.82\n",
      "- Energy and Environmental Design: 0.82\n",
      "- energy efficiency: 0.82\n",
      "- LEED: 0.72\n",
      "- tenant demand: 0.43\n",
      "- building technology: 0.39\n",
      "\n",
      "\n",
      "Processing article 19/21 (entertainment)...\n",
      "Domain detected using BART: entertainment (confidence: 0.5204)\n",
      "Detected domain: entertainment\n",
      "Domain detected using BART: entertainment (confidence: 0.5203)\n",
      "Extracted 3 named entities\n",
      "Generated 10 candidate keyphrases\n",
      "Semantic diversity: 0.750 → 0.750\n",
      "Keyphrase count: 10 → 10\n",
      "\n",
      "Using parameters for domain 'entertainment':\n",
      "  - base_threshold: 0.070\n",
      "  - quality_threshold: 0.380\n",
      "  - percentile: 3\n",
      "  - target_count: 8\n",
      "Using domain 'entertainment' with base threshold: 0.070, quality threshold: 0.380\n",
      "Using 3th percentile for domain 'entertainment'\n",
      "- Using 3th percentile from top: 0.865\n",
      "\n",
      "Quality Filtering Stats:\n",
      "- Domain: entertainment\n",
      "- Text Length: 388 words\n",
      "- Content Density: 0.69\n",
      "- Initial Candidates: 9\n",
      "- Base Threshold: 0.070\n",
      "- Quality Threshold: 0.350\n",
      "- After Quality Filter: 6\n",
      "Using target count 8 for domain 'entertainment'\n",
      "- Quality filter too strict, falling back to top 8 keyphrases\n",
      "- Falling back to top 8 keyphrases by score\n",
      "- Final keyphrase count: 8\n",
      "- Semantic diversity score: 0.75\n",
      "DEBUG: Extracting keyphrases with use_position_weight=True, use_tfidf_weight=True, use_ensemble=True\n",
      "DEBUG: KeyBERT extracted 85 keyphrases\n",
      "DEBUG: MultipartiteRank position weight for 'music content': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'strategies': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'services': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'sustainable profitability': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'success': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'rapid growth': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'video': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'streaming': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'subscriber loyalty': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'cost ad': 2.00\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'music content': 1.57\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'strategies': 0.93\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'services': 2.00\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'sustainable profitability': 0.77\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'success': 0.93\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'rapid growth': 0.96\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'video': 0.65\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'streaming': 1.42\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'subscriber loyalty': 0.50\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'cost ad': 0.50\n",
      "DEBUG: MultipartiteRank extracted 10 keyphrases\n",
      "DEBUG: YAKE position weight for 'netflix disney max': 2.00\n",
      "DEBUG: YAKE position weight for 'services netflix disney': 2.00\n",
      "DEBUG: YAKE position weight for 'disney max etc': 2.00\n",
      "DEBUG: YAKE position weight for 'streaming services netflix': 2.00\n",
      "DEBUG: YAKE position weight for 'content libraries competition': 2.00\n",
      "DEBUG: YAKE position weight for 'video music streaming': 2.00\n",
      "DEBUG: YAKE position weight for 'licensing popular library': 2.00\n",
      "DEBUG: YAKE position weight for 'platforms like spotify': 2.00\n",
      "DEBUG: YAKE position weight for 'music streaming platforms': 2.00\n",
      "DEBUG: YAKE position weight for 'volume licensing popular': 2.00\n",
      "DEBUG: YAKE TF-IDF weight for 'netflix disney max': 0.50\n",
      "DEBUG: YAKE TF-IDF weight for 'services netflix disney': 1.61\n",
      "DEBUG: YAKE TF-IDF weight for 'disney max etc': 0.50\n",
      "DEBUG: YAKE TF-IDF weight for 'streaming services netflix': 2.00\n",
      "DEBUG: YAKE TF-IDF weight for 'content libraries competition': 1.38\n",
      "DEBUG: YAKE TF-IDF weight for 'video music streaming': 1.94\n",
      "DEBUG: YAKE TF-IDF weight for 'licensing popular library': 0.79\n",
      "DEBUG: YAKE TF-IDF weight for 'platforms like spotify': 0.86\n",
      "DEBUG: YAKE TF-IDF weight for 'music streaming platforms': 1.75\n",
      "DEBUG: YAKE TF-IDF weight for 'volume licensing popular': 0.50\n",
      "DEBUG: YAKE extracted 10 keyphrases\n",
      "DEBUG: TextRank position weight for 'ai artificial intelligence podcast integration higher audio quality options': 2.00\n",
      "DEBUG: TextRank position weight for 'sales operations content remains': 2.00\n",
      "DEBUG: TextRank position weight for 'content libraries competition': 2.00\n",
      "DEBUG: TextRank position weight for 'streaming services': 2.00\n",
      "DEBUG: TextRank position weight for 'video music streaming': 2.00\n",
      "DEBUG: TextRank position weight for 'personalization competitive pricing': 2.00\n",
      "DEBUG: TextRank position weight for 'music content': 2.00\n",
      "DEBUG: TextRank position weight for 'content library': 2.00\n",
      "DEBUG: TextRank position weight for 'library content': 2.00\n",
      "DEBUG: TextRank position weight for 'content investment': 2.00\n",
      "DEBUG: TextRank TF-IDF weight for 'ai artificial intelligence podcast integration higher audio quality options': 0.77\n",
      "DEBUG: TextRank TF-IDF weight for 'sales operations content remains': 0.97\n",
      "DEBUG: TextRank TF-IDF weight for 'content libraries competition': 0.72\n",
      "DEBUG: TextRank TF-IDF weight for 'streaming services': 2.00\n",
      "DEBUG: TextRank TF-IDF weight for 'video music streaming': 1.27\n",
      "DEBUG: TextRank TF-IDF weight for 'personalization competitive pricing': 0.50\n",
      "DEBUG: TextRank TF-IDF weight for 'music content': 1.52\n",
      "DEBUG: TextRank TF-IDF weight for 'content library': 1.17\n",
      "DEBUG: TextRank TF-IDF weight for 'library content': 1.17\n",
      "DEBUG: TextRank TF-IDF weight for 'content investment': 1.17\n",
      "DEBUG: TextRank extracted 10 keyphrases\n",
      "DEBUG: Raw domain score for sports: 17.00\n",
      "DEBUG: Top domains detected: business: 0.3535, technology: 0.2284, entertainment: 0.2026\n",
      "DEBUG: All domain scores: {'technology': 0.22840119165839126, 'business': 0.3535253227408143, 'health': 0.005958291956305859, 'science': 0.025819265143992055, 'news': 0.030784508440913606, 'academic': 0.023833167825223437, 'politics': 0.03773584905660377, 'environment': 0.023833167825223437, 'entertainment': 0.2025819265143992, 'sports': 0.06752730883813307}\n",
      "DEBUG: Adaptive method weights:\n",
      "DEBUG:   - keybert: 0.2993\n",
      "DEBUG:   - multipartiterank: 0.3220\n",
      "DEBUG:   - yake: 0.1093\n",
      "DEBUG:   - textrank: 0.2694\n",
      "DEBUG: Length bonus for 'netflix disney max' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'rapid growth' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'sustainable profitability' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'subscriber loyalty' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'dominant mode' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'heavy content investment' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'low introductory pricing' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'password sharing' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'ad tier' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'music content' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'disney max etc' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'content investment' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'cost ad' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'significant shift' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'new revenue stream' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'sophisticated advertising technology' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'more calculated service' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'subscriber acquisition' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'traditional studio' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'key tactic' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'major battleground' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'significant investment' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'library content' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'north america' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'latin america' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'apple music' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'international expansion' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'future growth' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'local taste' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'establish giant' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'new entrant battle' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'increasingly saturate market' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'many stream service' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'sale operation' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'arm race' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'economic condition consolidation' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'large player' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'personalize recommendation' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'interactive experience' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'artist compensation' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'stream royalty' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'music industry' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'stream service' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'numerous option' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'available consumer' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'few service' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'compelling content library' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'various tier' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'strong brand identity' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'easy growth' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'sustainable success' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'strategic focus' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'operational efficiency' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'service netflix disney' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'stream service netflix' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'content libraries competition' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'video music streaming' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'license popular library' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'platform spotify' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'music stream platform' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'volume license popular' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'ai artificial intelligence podcast integration high audio quality option' (9 words): 1.60x\n",
      "DEBUG: Length bonus for 'sale operation content remain' (4 words): 1.45x\n",
      "DEBUG: Length bonus for 'personalization competitive pricing' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'content library' (2 words): 1.15x\n",
      "DEBUG: Ensemble combined into 97 keyphrases\n",
      "DEBUG: After redundancy removal: 79 keyphrases\n",
      "DEBUG: Before post-filtering: 79 keyphrases\n",
      "DEBUG: Multi-word phrases: 50\n",
      "DEBUG: Filtered out single word 'era' as it's part of multi-word phrase(s)\n",
      "DEBUG: After post-filtering: 78 keyphrases\n",
      "DEBUG: Final top 10 keyphrases: ['market', 'content', 'success', 'industry', 'profitability', 'economics', 'consolidation', 'spotify', 'however', 'licensing']\n",
      "Domain detected using BART: entertainment (confidence: 0.5204)\n",
      "Semantic diversity: 0.757 → 0.757\n",
      "Keyphrase count: 15 → 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing articles:  90%|█████████ | 19/21 [19:29<01:54, 57.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generic term filtering: 15 -> 13 keyphrases (2 removed)\n",
      "\n",
      "Filtered out generic terms (examples):\n",
      "- industry (0.41): exact generic match\n",
      "- success (0.31): exact generic match\n",
      "\n",
      "Using parameters for domain 'entertainment':\n",
      "  - base_threshold: 0.070\n",
      "  - quality_threshold: 0.380\n",
      "  - percentile: 3\n",
      "  - target_count: 8\n",
      "Using domain 'entertainment' with base threshold: 0.070, quality threshold: 0.380\n",
      "Using 3th percentile for domain 'entertainment'\n",
      "- Using 3th percentile from top: 0.847\n",
      "\n",
      "Quality Filtering Stats:\n",
      "- Domain: entertainment\n",
      "- Text Length: 388 words\n",
      "- Content Density: 0.69\n",
      "- Initial Candidates: 13\n",
      "- Base Threshold: 0.070\n",
      "- Quality Threshold: 0.350\n",
      "- After Quality Filter: 10\n",
      "Using target count 8 for domain 'entertainment'\n",
      "- Final keyphrase count: 10\n",
      "- Semantic diversity score: 0.74\n",
      "Using domain 'artificial intelligence' with quality threshold: 0.60\n",
      "Title: Streaming Wars Intensify: Services Battle for Subscribers Amidst Market Saturation\n",
      "Domain: entertainment\n",
      "Extracted 8 keyphrases:\n",
      "- global streaming entertainment landscape: 0.89\n",
      "- subscriber loyalty: 0.78\n",
      "- Netflix: 0.70\n",
      "- profitability: 0.68\n",
      "- market: 0.55\n",
      "- content: 0.54\n",
      "- spotify: 0.45\n",
      "- Disney: 0.39\n",
      "\n",
      "\n",
      "Processing article 20/21 (entertainment)...\n",
      "\n",
      "Zero-Shot Domain Detection Results:\n",
      "----------------------------------------\n",
      "Top domain: technology (Score: 0.5442)\n",
      "  1. technology: 0.5442\n",
      "  2. entertainment: 0.3939\n",
      "  3. business: 0.0305\n",
      "  4. education: 0.0141\n",
      "  5. artificial intelligence: 0.0071\n",
      "----------------------------------------\n",
      "Threshold for 'technology': 0.45\n",
      "Detected domain: technology\n",
      "Domain detected using BART: entertainment (confidence: 0.5920)\n",
      "Extracted 4 named entities\n",
      "Generated 21 candidate keyphrases\n",
      "Semantic diversity: 0.746 → 0.746\n",
      "Keyphrase count: 21 → 21\n",
      "\n",
      "Generic term filtering: 17 -> 15 keyphrases (2 removed)\n",
      "\n",
      "Filtered out generic terms (examples):\n",
      "- experience (0.38): exact generic match\n",
      "- market (0.28): exact generic match\n",
      "\n",
      "Using parameters for domain 'technology':\n",
      "  - base_threshold: 0.090\n",
      "  - quality_threshold: 0.420\n",
      "  - percentile: 5\n",
      "  - target_count: 10\n",
      "Using domain 'technology' with base threshold: 0.090, quality threshold: 0.420\n",
      "Using 5th percentile for domain 'technology'\n",
      "- Using 5th percentile from top: 0.857\n",
      "\n",
      "Quality Filtering Stats:\n",
      "- Domain: technology\n",
      "- Text Length: 383 words\n",
      "- Content Density: 0.66\n",
      "- Initial Candidates: 15\n",
      "- Base Threshold: 0.090\n",
      "- Quality Threshold: 0.350\n",
      "- After Quality Filter: 7\n",
      "Using target count 10 for domain 'technology'\n",
      "- Quality filter too strict, falling back to top 10 keyphrases\n",
      "- Using lenient threshold 0.175: 14 keyphrases\n",
      "- Too many keyphrases (14), selecting 12 diverse ones\n",
      "- Final keyphrase count: 12\n",
      "- Semantic diversity score: 0.72\n",
      "DEBUG: Extracting keyphrases with use_position_weight=True, use_tfidf_weight=True, use_ensemble=True\n",
      "DEBUG: KeyBERT extracted 95 keyphrases\n",
      "DEBUG: MultipartiteRank position weight for 'VR virtual reality': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'interactive experiences': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'immersive': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'virtual spaces': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'innovative content creation': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'hardware affordability': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'users': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'interactive exhibits': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'consumers advances': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'digital worlds': 2.00\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'VR virtual reality': 2.00\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'interactive experiences': 1.01\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'immersive': 0.59\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'virtual spaces': 1.58\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'innovative content creation': 0.58\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'hardware affordability': 0.59\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'users': 0.59\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'interactive exhibits': 0.59\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'consumers advances': 0.50\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'digital worlds': 0.59\n",
      "DEBUG: MultipartiteRank extracted 10 keyphrases\n",
      "DEBUG: YAKE position weight for 'like meta quest': 2.00\n",
      "DEBUG: YAKE position weight for 'like china accessories': 2.00\n",
      "DEBUG: YAKE position weight for 'china accessories like': 2.00\n",
      "DEBUG: YAKE position weight for 'reality creating mixed': 2.00\n",
      "DEBUG: YAKE position weight for 'creating mixed reality': 2.00\n",
      "DEBUG: YAKE position weight for 'virtual reality': 2.00\n",
      "DEBUG: YAKE position weight for 'virtual reality market': 2.00\n",
      "DEBUG: YAKE position weight for 'virtual reality experiences': 2.00\n",
      "DEBUG: YAKE position weight for 'augmented reality creating': 2.00\n",
      "DEBUG: YAKE position weight for 'virtual reality hardware': 2.00\n",
      "DEBUG: YAKE TF-IDF weight for 'like meta quest': 0.50\n",
      "DEBUG: YAKE TF-IDF weight for 'like china accessories': 0.50\n",
      "DEBUG: YAKE TF-IDF weight for 'china accessories like': 0.50\n",
      "DEBUG: YAKE TF-IDF weight for 'reality creating mixed': 0.79\n",
      "DEBUG: YAKE TF-IDF weight for 'creating mixed reality': 0.79\n",
      "DEBUG: YAKE TF-IDF weight for 'virtual reality': 2.00\n",
      "DEBUG: YAKE TF-IDF weight for 'virtual reality market': 1.87\n",
      "DEBUG: YAKE TF-IDF weight for 'virtual reality experiences': 1.88\n",
      "DEBUG: YAKE TF-IDF weight for 'augmented reality creating': 0.74\n",
      "DEBUG: YAKE TF-IDF weight for 'virtual reality hardware': 1.74\n",
      "DEBUG: YAKE extracted 10 keyphrases\n",
      "DEBUG: TextRank position weight for 'virtual reality vr virtual reality market applications': 2.00\n",
      "DEBUG: TextRank position weight for 'vr virtual reality entertainment ai artificial intelligence': 2.00\n",
      "DEBUG: TextRank position weight for 'social vr virtual reality experiences': 2.00\n",
      "DEBUG: TextRank position weight for 'vr virtual reality hardware market': 2.00\n",
      "DEBUG: TextRank position weight for 'making vr virtual reality': 2.00\n",
      "DEBUG: TextRank position weight for 'vr virtual reality films': 2.00\n",
      "DEBUG: TextRank position weight for 'gaming vr virtual reality': 2.00\n",
      "DEBUG: TextRank position weight for 'virtual spaces attend virtual': 2.00\n",
      "DEBUG: TextRank position weight for 'vr virtual reality': 2.00\n",
      "DEBUG: TextRank position weight for 'virtual experience': 2.00\n",
      "DEBUG: TextRank TF-IDF weight for 'virtual reality vr virtual reality market applications': 2.00\n",
      "DEBUG: TextRank TF-IDF weight for 'vr virtual reality entertainment ai artificial intelligence': 1.53\n",
      "DEBUG: TextRank TF-IDF weight for 'social vr virtual reality experiences': 1.27\n",
      "DEBUG: TextRank TF-IDF weight for 'vr virtual reality hardware market': 1.30\n",
      "DEBUG: TextRank TF-IDF weight for 'making vr virtual reality': 1.07\n",
      "DEBUG: TextRank TF-IDF weight for 'vr virtual reality films': 0.88\n",
      "DEBUG: TextRank TF-IDF weight for 'gaming vr virtual reality': 1.07\n",
      "DEBUG: TextRank TF-IDF weight for 'virtual spaces attend virtual': 1.54\n",
      "DEBUG: TextRank TF-IDF weight for 'vr virtual reality': 1.37\n",
      "DEBUG: TextRank TF-IDF weight for 'virtual experience': 0.50\n",
      "DEBUG: TextRank extracted 10 keyphrases\n",
      "DEBUG: Raw domain score for sports: 5.50\n",
      "DEBUG: Top domains detected: technology: 0.7141, entertainment: 0.1086, business: 0.0495\n",
      "DEBUG: All domain scores: {'technology': 0.7140575079872205, 'business': 0.04952076677316294, 'health': 0.006389776357827476, 'science': 0.03354632587859425, 'news': 0.025559105431309903, 'academic': 0.022364217252396165, 'politics': 0.012779552715654952, 'environment': 0.009584664536741214, 'entertainment': 0.10862619808306709, 'sports': 0.01757188498402556}\n",
      "DEBUG: Adaptive method weights:\n",
      "DEBUG:   - keybert: 0.2873\n",
      "DEBUG:   - multipartiterank: 0.2968\n",
      "DEBUG:   - yake: 0.2384\n",
      "DEBUG:   - textrank: 0.1775\n",
      "DEBUG: Length bonus for 'vr virtual reality' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'virtual reality' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'meta quest' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'innovative content creation' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'hardware affordability' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'virtual reality vr virtual reality market application' (7 words): 1.60x\n",
      "DEBUG: Length bonus for 'dominant driver' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'niche origin' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'standalone headset' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'hardware sale' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'vr virtual reality hardware market' (5 words): 1.60x\n",
      "DEBUG: Length bonus for 'virtual reality market' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'interactive experience' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'virtual reality hardware' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'virtual space' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'virtual experience' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'significant market presence' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'haptic feedback suit' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'physical sensation' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'widespread adoption' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'cinematic experience' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'live event' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'unparalleled presence' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'virtual concert' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'global artist' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'unique vantage point' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'virtual movie screening' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'social vr virtual reality experience' (5 words): 1.60x\n",
      "DEBUG: Length bonus for 'virtual reality experience' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'vr virtual reality film' (4 words): 1.45x\n",
      "DEBUG: Length bonus for 'Artificial Intelligence' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'Augmented reality' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'mixed reality' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'interactive exhibit' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'digital world' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'cultural institution' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'virtual tour' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'procedural generation' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'vast virtual environment' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'user choice' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'digital element' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'real world' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'entertainment training' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'broad entertainment sector' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'consumer advance' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'improve resolution' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'wide field' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'continue growth' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'china accessory' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'even deep immersion' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'share virtual space' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'augment reality ar' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'new possibility' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'social interaction' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'progress challenge' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'specialize skill' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'significant investment issue' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'technology matures' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'more significant part' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'entertainment landscape' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'novel interactive experience' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'futuristic concept' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'traditional format' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'reality create mix' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'create mixed reality' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'augment reality create' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'vr virtual reality entertainment ai artificial intelligence' (7 words): 1.60x\n",
      "DEBUG: Length bonus for 'make vr virtual reality' (4 words): 1.45x\n",
      "DEBUG: Length bonus for 'game vr virtual reality' (4 words): 1.45x\n",
      "DEBUG: Length bonus for 'virtual space attend virtual' (4 words): 1.45x\n",
      "DEBUG: Ensemble combined into 108 keyphrases\n",
      "DEBUG: After redundancy removal: 76 keyphrases\n",
      "DEBUG: Before post-filtering: 76 keyphrases\n",
      "DEBUG: Multi-word phrases: 40\n",
      "DEBUG: After post-filtering: 76 keyphrases\n",
      "DEBUG: Final top 10 keyphrases: ['virtual', 'reality', 'content', 'narrative', 'however', 'potential', 'integration', 'untethered', 'cost', 'barrier']\n",
      "\n",
      "Zero-Shot Domain Detection Results:\n",
      "----------------------------------------\n",
      "Top domain: technology (Score: 0.5442)\n",
      "  1. technology: 0.5442\n",
      "  2. entertainment: 0.3939\n",
      "  3. business: 0.0305\n",
      "  4. education: 0.0141\n",
      "  5. artificial intelligence: 0.0071\n",
      "----------------------------------------\n",
      "Threshold for 'technology': 0.45\n",
      "Semantic diversity: 0.756 → 0.756\n",
      "Keyphrase count: 17 → 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing articles:  95%|█████████▌| 20/21 [20:35<00:59, 59.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using parameters for domain 'technology':\n",
      "  - base_threshold: 0.090\n",
      "  - quality_threshold: 0.420\n",
      "  - percentile: 5\n",
      "  - target_count: 10\n",
      "Using domain 'technology' with base threshold: 0.090, quality threshold: 0.420\n",
      "Using 5th percentile for domain 'technology'\n",
      "- Using 5th percentile from top: 0.858\n",
      "\n",
      "Quality Filtering Stats:\n",
      "- Domain: technology\n",
      "- Text Length: 383 words\n",
      "- Content Density: 0.66\n",
      "- Initial Candidates: 14\n",
      "- Base Threshold: 0.090\n",
      "- Quality Threshold: 0.350\n",
      "- After Quality Filter: 7\n",
      "Using target count 10 for domain 'technology'\n",
      "- Quality filter too strict, falling back to top 10 keyphrases\n",
      "- Using lenient threshold 0.175: 11 keyphrases\n",
      "- Final keyphrase count: 11\n",
      "- Semantic diversity score: 0.74\n",
      "Using domain 'technology' with quality threshold: 0.55\n",
      "Title: Virtual Reality Gains Traction in Entertainment Beyond Gaming\n",
      "Domain: entertainment\n",
      "Extracted 8 keyphrases:\n",
      "- VR: 0.86\n",
      "- Virtual Reality: 0.86\n",
      "- the Virtual Reality: 0.84\n",
      "- hardware affordability: 0.74\n",
      "- gaming: 0.70\n",
      "- combined with innovative content creation: 0.60\n",
      "- entertainment sector: 0.59\n",
      "- untethered: 0.30\n",
      "\n",
      "\n",
      "Processing article 21/21 (entertainment)...\n",
      "\n",
      "Zero-Shot Domain Detection Results:\n",
      "----------------------------------------\n",
      "Top domain: technology (Score: 0.4038)\n",
      "  1. technology: 0.4038\n",
      "  2. artificial intelligence: 0.3885\n",
      "  3. entertainment: 0.1732\n",
      "  4. science: 0.0316\n",
      "  5. education: 0.0015\n",
      "----------------------------------------\n",
      "Threshold for 'technology': 0.45\n",
      "Using secondary domain 'artificial intelligence' (Score: 0.3885, Threshold: 0.35)\n",
      "Detected domain: artificial intelligence\n",
      "\n",
      "Zero-Shot Domain Detection Results:\n",
      "----------------------------------------\n",
      "Top domain: technology (Score: 0.4092)\n",
      "  1. technology: 0.4092\n",
      "  2. artificial intelligence: 0.3452\n",
      "  3. entertainment: 0.1449\n",
      "  4. science: 0.0945\n",
      "  5. education: 0.0031\n",
      "----------------------------------------\n",
      "Threshold for 'technology': 0.45\n",
      "Low confidence (0.4092) for domain 'technology'.\n",
      "Score is above 0.25, using detected domain despite being below threshold\n",
      "Extracted 3 named entities\n",
      "Generated 16 candidate keyphrases\n",
      "Semantic diversity: 0.757 → 0.757\n",
      "Keyphrase count: 16 → 16\n",
      "\n",
      "Using parameters for domain 'artificial intelligence':\n",
      "  - base_threshold: 0.090\n",
      "  - quality_threshold: 0.450\n",
      "  - percentile: 5\n",
      "  - target_count: 10\n",
      "Using domain 'artificial intelligence' with base threshold: 0.090, quality threshold: 0.450\n",
      "Using 5th percentile for domain 'artificial intelligence'\n",
      "- Using 5th percentile from top: 0.797\n",
      "\n",
      "Quality Filtering Stats:\n",
      "- Domain: artificial intelligence\n",
      "- Text Length: 394 words\n",
      "- Content Density: 0.69\n",
      "- Initial Candidates: 13\n",
      "- Base Threshold: 0.090\n",
      "- Quality Threshold: 0.350\n",
      "- After Quality Filter: 10\n",
      "Using target count 10 for domain 'artificial intelligence'\n",
      "- Final keyphrase count: 10\n",
      "- Semantic diversity score: 0.74\n",
      "DEBUG: Extracting keyphrases with use_position_weight=True, use_tfidf_weight=True, use_ensemble=True\n",
      "DEBUG: KeyBERT extracted 98 keyphrases\n",
      "DEBUG: MultipartiteRank position weight for 'AI artificial intelligence': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'powerful new tools': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'human roles': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'music creation': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'creativity copyright': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'film production': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'artists': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'significant debate': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'creative possibilities': 2.00\n",
      "DEBUG: MultipartiteRank position weight for 'Amper Music': 2.00\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'AI artificial intelligence': 2.00\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'powerful new tools': 1.02\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'human roles': 0.81\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'music creation': 0.92\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'creativity copyright': 0.84\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'film production': 1.06\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'artists': 0.50\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'significant debate': 0.57\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'creative possibilities': 0.77\n",
      "DEBUG: MultipartiteRank TF-IDF weight for 'Amper Music': 0.81\n",
      "DEBUG: MultipartiteRank extracted 10 keyphrases\n",
      "DEBUG: YAKE position weight for 'artificial intelligence': 2.00\n",
      "DEBUG: YAKE position weight for 'platforms like amper': 2.00\n",
      "DEBUG: YAKE position weight for 'tools like openai': 2.00\n",
      "DEBUG: YAKE position weight for 'streaming platforms voice': 2.00\n",
      "DEBUG: YAKE position weight for 'platforms voice synthesis': 2.00\n",
      "DEBUG: YAKE position weight for 'predictive analytics platforms': 2.00\n",
      "DEBUG: YAKE position weight for 'artificial intelligence tools': 2.00\n",
      "DEBUG: YAKE position weight for 'like amper music': 2.00\n",
      "DEBUG: YAKE position weight for 'production cycle generative': 2.00\n",
      "DEBUG: YAKE position weight for 'artificial intelligence revolution': 2.00\n",
      "DEBUG: YAKE TF-IDF weight for 'artificial intelligence': 2.00\n",
      "DEBUG: YAKE TF-IDF weight for 'platforms like amper': 0.75\n",
      "DEBUG: YAKE TF-IDF weight for 'tools like openai': 0.81\n",
      "DEBUG: YAKE TF-IDF weight for 'streaming platforms voice': 0.50\n",
      "DEBUG: YAKE TF-IDF weight for 'platforms voice synthesis': 0.60\n",
      "DEBUG: YAKE TF-IDF weight for 'predictive analytics platforms': 0.60\n",
      "DEBUG: YAKE TF-IDF weight for 'artificial intelligence tools': 1.85\n",
      "DEBUG: YAKE TF-IDF weight for 'like amper music': 0.78\n",
      "DEBUG: YAKE TF-IDF weight for 'production cycle generative': 0.68\n",
      "DEBUG: YAKE TF-IDF weight for 'artificial intelligence revolution': 1.48\n",
      "DEBUG: YAKE extracted 10 keyphrases\n",
      "DEBUG: TextRank position weight for 'artificial intelligence ai artificial intelligence': 2.00\n",
      "DEBUG: TextRank position weight for 'animation predictive analytics platforms use ai artificial intelligence': 2.00\n",
      "DEBUG: TextRank position weight for 'music production ai artificial intelligence': 2.00\n",
      "DEBUG: TextRank position weight for 'generative ai artificial intelligence tools': 2.00\n",
      "DEBUG: TextRank position weight for 'visualization ai artificial intelligence enhances': 2.00\n",
      "DEBUG: TextRank position weight for 'production ai artificial intelligence': 2.00\n",
      "DEBUG: TextRank position weight for 'ai artificial intelligence integration': 2.00\n",
      "DEBUG: TextRank position weight for 'training ai artificial intelligence': 2.00\n",
      "DEBUG: TextRank position weight for 'ai artificial intelligence': 2.00\n",
      "DEBUG: TextRank position weight for 'powerful new tools': 2.00\n",
      "DEBUG: TextRank TF-IDF weight for 'artificial intelligence ai artificial intelligence': 2.00\n",
      "DEBUG: TextRank TF-IDF weight for 'animation predictive analytics platforms use ai artificial intelligence': 1.21\n",
      "DEBUG: TextRank TF-IDF weight for 'music production ai artificial intelligence': 1.52\n",
      "DEBUG: TextRank TF-IDF weight for 'generative ai artificial intelligence tools': 1.24\n",
      "DEBUG: TextRank TF-IDF weight for 'visualization ai artificial intelligence enhances': 1.06\n",
      "DEBUG: TextRank TF-IDF weight for 'production ai artificial intelligence': 1.54\n",
      "DEBUG: TextRank TF-IDF weight for 'ai artificial intelligence integration': 1.35\n",
      "DEBUG: TextRank TF-IDF weight for 'training ai artificial intelligence': 1.33\n",
      "DEBUG: TextRank TF-IDF weight for 'ai artificial intelligence': 1.53\n",
      "DEBUG: TextRank TF-IDF weight for 'powerful new tools': 0.50\n",
      "DEBUG: TextRank extracted 10 keyphrases\n",
      "DEBUG: Raw domain score for sports: 8.00\n",
      "DEBUG: Top domains detected: technology: 0.4170, entertainment: 0.3050, news: 0.0579\n",
      "DEBUG: All domain scores: {'technology': 0.416988416988417, 'business': 0.052123552123552123, 'health': 0.0, 'science': 0.04247104247104247, 'news': 0.05791505791505792, 'academic': 0.04826254826254826, 'politics': 0.04633204633204633, 'environment': 0.0, 'entertainment': 0.305019305019305, 'sports': 0.03088803088803089}\n",
      "DEBUG: Adaptive method weights:\n",
      "DEBUG:   - keybert: 0.3301\n",
      "DEBUG:   - multipartiterank: 0.3242\n",
      "DEBUG:   - yake: 0.1039\n",
      "DEBUG:   - textrank: 0.2418\n",
      "DEBUG: Length bonus for 'Artificial Intelligence' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'ai artificial intelligence' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'amper music' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'powerful new tool' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'human role' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'film production' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'music creation' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'creativity copyright' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'significant debate' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'entertainment industry' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'practical integration' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'various way' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'specific parameter' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'music composition' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'artificial intelligence ai artificial intelligence' (5 words): 1.60x\n",
      "DEBUG: Length bonus for 'music production ai artificial intelligence' (5 words): 1.60x\n",
      "DEBUG: Length bonus for 'production ai artificial intelligence' (4 words): 1.45x\n",
      "DEBUG: Length bonus for 'e g vocal' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'realistic vocal track' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'ethical question' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'production cycle' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'concept art storyboards' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'text prompt' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'openai 's sora' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'script analysis' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'pre - visualization' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'generative ai artificial intelligence tool' (5 words): 1.60x\n",
      "DEBUG: Length bonus for 'training ai artificial intelligence' (4 words): 1.45x\n",
      "DEBUG: Length bonus for 'artificial intelligence tool' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'visualization ai artificial intelligence enhances' (5 words): 1.60x\n",
      "DEBUG: Length bonus for 'production cycle generative' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'Everything everywhere all' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'predictive analytics platform' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'background removal' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'human movement' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'specialized suit' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'character animation' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'test audience response' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'creative field' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'melody harmony' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'exist track' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'compositional idea platform' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'customize soundtrack' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'exist one' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'both creative possibility' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'major concern' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'human writer performer' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'copyright ownership' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'complex legal challenge' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'artistic merit' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'training data' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'human creativity' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'laborious task' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'new artistic possibility' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'clear ethical guideline' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'fair compensation' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'crucial step' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'creative possibility' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'platforms amper' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'tool openai' (2 words): 1.15x\n",
      "DEBUG: Length bonus for 'stream platform voice' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'platform voice synthesis' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'artificial intelligence revolution' (3 words): 1.30x\n",
      "DEBUG: Length bonus for 'animation predictive analytics platform use ai artificial intelligence' (8 words): 1.60x\n",
      "DEBUG: Length bonus for 'ai artificial intelligence integration' (4 words): 1.45x\n",
      "DEBUG: Ensemble combined into 111 keyphrases\n",
      "DEBUG: After redundancy removal: 83 keyphrases\n",
      "DEBUG: Before post-filtering: 83 keyphrases\n",
      "DEBUG: Multi-word phrases: 44\n",
      "DEBUG: Filtered out single word 'once' as it's part of multi-word phrase(s)\n",
      "DEBUG: After post-filtering: 82 keyphrases\n",
      "DEBUG: Final top 10 keyphrases: ['algorithms', 'compensation', 'development', 'use', 'creation', 'predictive', 'voice', 'generative', 'advanced', 'however']\n",
      "\n",
      "Zero-Shot Domain Detection Results:\n",
      "----------------------------------------\n",
      "Top domain: technology (Score: 0.4038)\n",
      "  1. technology: 0.4038\n",
      "  2. artificial intelligence: 0.3885\n",
      "  3. entertainment: 0.1732\n",
      "  4. science: 0.0316\n",
      "  5. education: 0.0015\n",
      "----------------------------------------\n",
      "Threshold for 'technology': 0.45\n",
      "Using secondary domain 'artificial intelligence' (Score: 0.3885, Threshold: 0.35)\n",
      "Semantic diversity: 0.736 → 0.736\n",
      "Keyphrase count: 17 → 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing articles: 100%|██████████| 21/21 [21:39<00:00, 61.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generic term filtering: 16 -> 13 keyphrases (3 removed)\n",
      "\n",
      "Filtered out generic terms (examples):\n",
      "- predictive (0.35): exact generic match\n",
      "- development (0.24): exact generic match\n",
      "- use (0.17): exact generic match\n",
      "\n",
      "Using parameters for domain 'artificial intelligence':\n",
      "  - base_threshold: 0.090\n",
      "  - quality_threshold: 0.450\n",
      "  - percentile: 5\n",
      "  - target_count: 10\n",
      "Using domain 'artificial intelligence' with base threshold: 0.090, quality threshold: 0.450\n",
      "Using 5th percentile for domain 'artificial intelligence'\n",
      "- Using 5th percentile from top: 0.799\n",
      "\n",
      "Quality Filtering Stats:\n",
      "- Domain: artificial intelligence\n",
      "- Text Length: 394 words\n",
      "- Content Density: 0.69\n",
      "- Initial Candidates: 13\n",
      "- Base Threshold: 0.090\n",
      "- Quality Threshold: 0.350\n",
      "- After Quality Filter: 10\n",
      "Using target count 10 for domain 'artificial intelligence'\n",
      "- Final keyphrase count: 10\n",
      "- Semantic diversity score: 0.70\n",
      "Using domain 'artificial intelligence' with quality threshold: 0.60\n",
      "Title: AI Integration Sparks Innovation and Debate in Music and Film Production\n",
      "Domain: entertainment\n",
      "Extracted 9 keyphrases:\n",
      "- entertainment industry: 0.82\n",
      "- creativity: 0.79\n",
      "- music creation: 0.77\n",
      "- film production: 0.59\n",
      "- copyright: 0.59\n",
      "- voice: 0.55\n",
      "- Amper Music: 0.55\n",
      "- algorithms: 0.53\n",
      "- intelligence: 0.49\n",
      "\n",
      "Results saved to keyphrase_extraction_results.csv\n",
      "\n",
      "Summary by domain:\n",
      "Domain: artificial intelligence\n",
      "  Avg. keyphrases: 8.0\n",
      "  Avg. top score: 0.83\n",
      "  Avg. score: 0.63\n",
      "  Avg. processing time: 70.48s\n",
      "Domain: automotive\n",
      "  Avg. keyphrases: 8.0\n",
      "  Avg. top score: 0.67\n",
      "  Avg. score: 0.47\n",
      "  Avg. processing time: 63.43s\n",
      "Domain: cybersecurity\n",
      "  Avg. keyphrases: 8.0\n",
      "  Avg. top score: 0.85\n",
      "  Avg. score: 0.56\n",
      "  Avg. processing time: 55.71s\n",
      "Domain: entertainment\n",
      "  Avg. keyphrases: 8.33\n",
      "  Avg. top score: 0.85\n",
      "  Avg. score: 0.65\n",
      "  Avg. processing time: 60.91s\n",
      "Domain: environment\n",
      "  Avg. keyphrases: 8.0\n",
      "  Avg. top score: 0.84\n",
      "  Avg. score: 0.6\n",
      "  Avg. processing time: 59.47s\n",
      "Domain: food\n",
      "  Avg. keyphrases: 8.0\n",
      "  Avg. top score: 0.85\n",
      "  Avg. score: 0.48\n",
      "  Avg. processing time: 64.37s\n",
      "Domain: real estate\n",
      "  Avg. keyphrases: 8.33\n",
      "  Avg. top score: 0.85\n",
      "  Avg. score: 0.54\n",
      "  Avg. processing time: 58.87s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization saved to keyphrase_extraction_analysis.png\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1200 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def test_abstractive_keyphrase_extractor():\n",
    "    \"\"\"\n",
    "    Test the AbstractiveKeyphraseExtractor on a diverse set of news articles\n",
    "    and evaluate its performance across different domains.\n",
    "    \"\"\"\n",
    "    # Initialize the extractor\n",
    "    print(\"Initializing AbstractiveKeyphraseExtractor...\")\n",
    "    extractor =  fusion_extractor = FusionKeyphraseExtractor(\n",
    "         use_gpu=True,\n",
    "        abstractive_weight=0.45,  \n",
    "        extractive_weight=0.55,   \n",
    "        redundancy_threshold=0.68,  \n",
    "        min_score=0.09  \n",
    "    )\n",
    "    # Test articles from different domains\n",
    "    test_articles = [\n",
    "# --- AI Articles ---\n",
    "{\n",
    "    'title': 'AI Advancements Set to Transform Creative Industries in 2025',\n",
    "    'domain': 'artificial intelligence',\n",
    "    'text': \"\"\"\n",
    "    (April 12, 2025) - The creative industries, spanning film, music, design, and content creation, are experiencing a seismic shift driven by rapid advancements in Artificial Intelligence (AI). Throughout 2024 and early 2025, generative AI tools like large language models (LLMs) and sophisticated text-to-image/video generators have moved from novelty to practical application, profoundly impacting workflows and opening new creative avenues.\n",
    "\n",
    "    Industry experts predict 2025 will see further integration of AI, not necessarily replacing human creativity, but augmenting it. Applications range from AI-powered PCs enhancing rendering speeds and workflow efficiency for designers and video editors, to AI algorithms assisting musicians in composition and production. Platforms like Cinelytic are already using AI for predictive analytics in film intelligence, helping studios make more informed decisions about potential projects. Tools are also emerging to automate aspects of advertising creation and performance testing.\n",
    "\n",
    "    However, this integration isn't without challenges. The 2023 Hollywood strikes highlighted deep concerns about AI's role, resulting in agreements stipulating that AI should support, not supplant, human talent. Issues surrounding copyright, fair remuneration for creators whose work trains AI models, and the potential for AI to exacerbate biases remain critical ethical considerations. A key challenge noted by research experts is managing the intellectual property (IP) implications and ensuring creators consent and are compensated fairly.\n",
    "\n",
    "    Furthermore, while AI tools democratize creation, they also lead to content abundance, making it harder for new artists and works to gain visibility. This necessitates new strategies for discovery, potentially increasing reliance on algorithmic curation like TikTok's BookTok or personalized streaming recommendations.\n",
    "\n",
    "    Despite these hurdles, the potential benefits are significant. AI can streamline repetitive tasks in post-production, generate novel visual effects previously requiring large teams (as seen in films like \"Everything Everywhere All at Once\"), and offer personalized creative assistance. The trend is moving towards unified AI frameworks capable of handling multiple creative tasks, but human oversight remains crucial to manage inaccuracies and ethical risks. As AI evolves, the creative industries must navigate these complexities, aiming for an ecosystem where technology enhances human ingenuity and distributes benefits equitably. Striking this balance will be key to harnessing AI's full potential while safeguarding the value of human creativity.\n",
    "    \"\"\"\n",
    "},\n",
    "{\n",
    "    'title': 'Navigating the Ethical Minefield: AI Development Demands Responsible Practices',\n",
    "    'domain': 'artificial intelligence',\n",
    "    'text': \"\"\"\n",
    "    (April 12, 2025) - As Artificial Intelligence (AI) systems become increasingly integrated into critical sectors like healthcare, finance, and autonomous systems, the ethical considerations surrounding their development and deployment are intensifying. Experts and policymakers agree that establishing robust ethical frameworks is paramount to harness AI's benefits while mitigating significant risks.\n",
    "\n",
    "    Several core ethical concerns dominate the discussion in 2025. Fairness and bias remain critical challenges. AI systems, trained on historical data, can inherit and even amplify societal biases related to race, gender, or socioeconomic status. For example, biased algorithms in hiring or loan applications can perpetuate discrimination, requiring rigorous scrutiny of training data and model outputs to ensure fairness. Amazon's abandonment of a biased hiring algorithm serves as a stark reminder of this risk.\n",
    "\n",
    "    Transparency and explainability are vital for building trust. Users and regulators need clarity on how AI systems make decisions, especially in high-stakes scenarios. However, achieving full transparency can be complex, sometimes conflicting with goals like privacy or security. Striking the right balance and securing informed consent for data usage are essential.\n",
    "\n",
    "    Privacy is another major concern. AI systems often require vast amounts of data, raising questions about responsible data handling, safeguarding user information, and preventing misuse. Global standards for data storage and processing by AI are still lacking, creating regulatory gaps.\n",
    "\n",
    "    Human safety and accountability are non-negotiable. AI systems, particularly autonomous ones like self-driving cars or medical diagnostic tools, must be rigorously tested to prevent harm. Clear lines of responsibility must be established for decisions made by AI, ensuring that human oversight and accountability are not displaced.\n",
    "\n",
    "    Furthermore, the environmental impact of AI, due to high energy consumption for training large models, calls for prioritizing sustainable practices. Concerns about AI's potential to influence human behavior, disrupt labor markets, and even pose existential risks (though hotly debated) also feature in ethical discussions. UNESCO and other international bodies emphasize a human-rights-centered approach, embedding ethical considerations into the entire AI lifecycle, from design to deployment, to ensure AI aligns with human values and societal well-being. Ignoring these ethical imperatives risks undermining public trust and could lead to significant societal harm.\n",
    "    \"\"\"\n",
    "},\n",
    "{\n",
    "    'title': 'Large Language Models Reach New Milestones, Pushing Boundaries of AI Interaction',\n",
    "    'domain': 'artificial intelligence',\n",
    "    'text': \"\"\"\n",
    "    (April 12, 2025) – Large Language Models (LLMs) continue their rapid evolution, marking significant milestones in capability and application throughout late 2024 and early 2025. These sophisticated AI systems, known for their ability to understand and generate human-like text, are increasingly finding roles beyond simple chatbots, impacting research, creative fields, and business processes.\n",
    "\n",
    "    Recent advancements have focused on enhancing LLMs' reasoning abilities, contextual understanding, and multimodal capabilities. Models released in the past year demonstrate improved performance on complex tasks, better factual accuracy (though 'hallucinations' remain a challenge), and the ability to integrate and process information from text, images, and even audio inputs. This progress establishes new benchmarks in conversational AI and natural language processing.\n",
    "\n",
    "    The integration of LLMs into various software and platforms is accelerating. In the creative industries, LLMs assist with scriptwriting, generating marketing copy, and even providing narrative suggestions. Research indicates their utility in simplifying human interaction with complex systems through natural language prompts, allowing artists and designers to convey intricate instructions more intuitively. For instance, LLMs are being used to drive image and video generation tools, translating textual descriptions into visual content with increasing fidelity.\n",
    "\n",
    "    In business, LLMs are being deployed for tasks like document summarization, code generation, customer service automation, and data analysis. Their ability to process and synthesize vast amounts of text efficiently offers significant productivity gains. Startups and established tech giants are competing fiercely, releasing progressively larger and more optimized models, often focusing on specific domains like finance or healthcare to improve relevance and accuracy.\n",
    "\n",
    "    Despite the progress, challenges persist. The computational cost and energy consumption required to train state-of-the-art LLMs raise environmental concerns. Ensuring factual reliability, mitigating inherent biases learned from training data, and addressing ethical considerations related to misinformation and misuse are ongoing areas of research and debate. Furthermore, the development of robust methods for assessing LLM capabilities and limitations remains crucial. As these models become more powerful and integrated into daily life, navigating these challenges while maximizing their potential benefits will be a key focus for researchers, developers, and policymakers in the coming years.\n",
    "    \"\"\"\n",
    "},\n",
    "\n",
    "# --- Automotive Articles ---\n",
    "{\n",
    "    'title': 'Electric Vehicle Battery Technology Sees Major Breakthroughs in Range and Charging',\n",
    "    'domain': 'automotive',\n",
    "    'text': \"\"\"\n",
    "    (April 12, 2025) – The push for longer range, faster charging, and increased safety in electric vehicles (EVs) is driving significant innovation in battery technology, with several key breakthroughs announced in late 2024 and early 2025. These advancements promise to address some of the major hurdles hindering wider EV adoption.\n",
    "\n",
    "    Solid-state batteries continue to be a major focus. Several companies, including Nissan and Mercedes-Benz (leveraging Formula 1 tech), have announced prototype solid-state batteries targeting commercial availability within the next few years, possibly by 2028. These batteries replace the liquid electrolyte with a solid material, potentially offering higher energy density (leading to longer range, perhaps enabling 500-600 mile EVs), improved safety by reducing fire risk, and faster charging capabilities. Korean researchers have notably developed a fireproof lithium metal battery using a solid electrolyte. While manufacturing cost and scalability remain challenges, the progress is tangible.\n",
    "\n",
    "    Beyond solid-state, advancements are being made with current lithium-ion chemistries. Researchers are exploring novel cathode materials, like Lithium Manganese Iron Phosphate (LMFP), which tests show could significantly extend EV range compared to existing materials. Additionally, innovations like single-crystal electrodes, tested extensively at Dalhousie University, have shown remarkable durability, enduring over 20,000 charging cycles – potentially translating to an EV lifespan of millions of miles. Increasing the silicon content in anodes is another avenue being pursued by companies like GM to improve energy density and charging speeds.\n",
    "\n",
    "    Charging technology is also evolving rapidly. Chinese OEM BYD unveiled a new \"super e-platform\" claiming megawatt charging capabilities that could potentially recharge an EV in just five minutes. Furthermore, researchers have developed battery chemistries capable of charging in 10 minutes even at sub-zero temperatures, addressing a key pain point for EV owners in colder climates. Stationary storage systems, like one announced by Volvo with a 500 kWh capacity, are also being developed to support charging infrastructure, capable of recharging multiple EVs daily.\n",
    "\n",
    "    Efforts towards sustainability are also evident. Research is underway to develop EV batteries without per- and polyfluoroalkyl substances (PFAS), often called 'forever chemicals'. Recycling processes are improving, with some breakthroughs achieving nearly 100% lithium recovery rates. These combined advancements signal a future where EV batteries are more powerful, longer-lasting, faster-charging, safer, and more sustainable.\n",
    "    \"\"\"\n",
    "},\n",
    "{\n",
    "    'title': 'Automotive Industry Faces Headwinds in 2025 Despite Technological Strides',\n",
    "    'domain': 'automotive',\n",
    "    'text': \"\"\"\n",
    "    (April 12, 2025) - The global automotive industry enters the mid-decade facing a complex mix of challenges, tempering the excitement around rapid technological evolution in areas like electrification and autonomous driving. While innovation continues at pace, economic pressures, regulatory shifts, supply chain vulnerabilities, and heightened competition are creating significant headwinds for manufacturers.\n",
    "\n",
    "    Market growth forecasts for 2025 remain modest, with projections around 2.7% globally. High vehicle prices, rising consumer debt, and broader economic uncertainty are constraining demand for big-ticket items like cars. Geopolitical tensions, including potential tariff increases between major markets like the US, EU, and China, pose a substantial risk, potentially increasing component costs and dampening consumer demand further. The impact of the Russia-Ukraine war and Middle East conflicts continues to ripple through supply chains, affecting component availability and logistics.\n",
    "\n",
    "    The transition to electric vehicles (EVs) presents both opportunities and challenges. While EV sales are growing, particularly in emerging markets spurred by government incentives, the required investment in R&D, battery production, and charging infrastructure is immense. Manufacturers grapple with balancing investments across traditional internal combustion engines (ICE), hybrid technologies, and full electrification amidst regulatory uncertainty and evolving consumer preferences. Stricter emission standards, particularly the EU's 2025 CO2 targets, are forcing rapid shifts towards zero-emission technologies, adding pressure on compliance and competitiveness.\n",
    "\n",
    "    Competition is intensifying, especially from cost-advantaged Chinese manufacturers who possess significant advantages in labor costs and control over critical parts of the EV supply chain, particularly batteries. Established OEMs like Stellantis are forming joint ventures to address this, while others focus on cost-cutting measures and improving manufacturing efficiency. However, vehicles have generally become larger and heavier, partly due to regulations, adding to costs.\n",
    "\n",
    "    Supply chain resilience remains a critical concern. The industry's reliance on complex, global supply networks makes it vulnerable to disruptions, as seen with semiconductor shortages and geopolitical events. Enhancing visibility across supply tiers, diversifying suppliers, and potentially moving away from strict just-in-time models are key strategies being explored. Furthermore, the increasing connectivity and software reliance in modern vehicles significantly elevates cybersecurity risks, with remote attacks becoming more common, requiring robust security measures throughout the vehicle lifecycle. Successfully navigating these multifaceted challenges requires significant flexibility, strategic investment, and innovation from automakers in 2025.\n",
    "    \"\"\"\n",
    "},\n",
    "{\n",
    "    'title': 'Autonomous Driving Systems Progress Amidst Regulatory and Safety Hurdles',\n",
    "    'domain': 'automotive',\n",
    "    'text': \"\"\"\n",
    "    (April 12, 2025) – The development of autonomous driving (AD) systems continues its forward march, with technological capabilities advancing steadily. However, the path to widespread deployment of fully self-driving vehicles remains complex, navigating significant regulatory hurdles, ongoing safety validation requirements, and persistent public perception challenges.\n",
    "\n",
    "    Technologically, advancements in sensor fusion (combining data from cameras, lidar, radar), AI-driven perception algorithms, and high-definition mapping are enabling more sophisticated AD functionalities. Systems offering Level 3 conditional automation (where the car drives itself under specific conditions, but the driver must be ready to intervene) and Level 4 high automation (where the car drives itself within defined operational design domains, like specific geographic areas or weather conditions) are becoming more refined. Projections suggest a growing percentage of new vehicles will feature Level 3 or 4 capabilities by the end of the decade, though full Level 5 autonomy (no human intervention needed anywhere) remains a distant goal for most applications.\n",
    "\n",
    "    Companies across the automotive and tech sectors are investing heavily. Traditional automakers like GM, Ford, and Mercedes-Benz are integrating advanced driver-assistance systems (ADAS) that serve as building blocks for higher autonomy levels. Tech companies like Waymo (Google) and Cruise (GM) continue testing and expanding their robotaxi services in limited urban environments, gathering crucial real-world data but also facing intense scrutiny following incidents.\n",
    "\n",
    "    Regulatory frameworks are struggling to keep pace with the technology. Establishing clear rules for liability, safety standards, testing protocols, and data privacy across different jurisdictions (states, countries) is a major challenge. The lack of harmonized regulations creates uncertainty for manufacturers and slows down broader deployment. Ensuring the safety and reliability of AD systems remains paramount. Rigorous testing, including simulations and extensive real-world driving, is necessary to validate system performance under countless scenarios. Edge cases and unexpected events ('long tail' problems) remain difficult to fully anticipate and address.\n",
    "\n",
    "    Cybersecurity is another critical concern. As vehicles become more connected and reliant on software for driving functions, they become potential targets for malicious attacks. Protecting AD systems from hacking, ensuring secure software updates, and safeguarding collected data are essential for safety and public trust. While the promise of enhanced safety, improved mobility, and increased efficiency drives AD development, overcoming these technical, regulatory, and societal challenges is crucial for realizing the full potential of autonomous vehicles in the coming years.\n",
    "    \"\"\"\n",
    "},\n",
    "\n",
    "# --- CyberSecurity Articles ---\n",
    "{\n",
    "    'title': 'Ransomware Evolves: Attackers Shift Tactics Amidst Improved Defenses',\n",
    "    'domain': 'cybersecurity',\n",
    "    'text': \"\"\"\n",
    "    (April 12, 2025) - Ransomware remains a dominant and highly disruptive cyber threat in 2025, but attack methodologies are evolving as cybercriminals adapt to enhanced enterprise defenses and increased pressure from law enforcement. Recent reports indicate a continued rise in overall ransomware activity, despite disruptions to major gangs like LockBit in early 2024, showcasing the threat's resilience.\n",
    "\n",
    "    A key trend observed is a shift in tactics. While encryption remains a core component, attackers are increasingly focusing on data theft and extortion – known as double extortion. They exfiltrate sensitive data before or instead of encrypting systems, threatening to leak the stolen information publicly if the ransom isn't paid. This puts additional pressure on victims concerned about reputational damage and regulatory penalties, even if they have robust backups. Some groups even employ triple extortion, adding DDoS attacks or contacting the victim's customers or partners.\n",
    "\n",
    "    Attackers are refining techniques to bypass security measures. There's a marked increase in \"living off the land\" (LotL) techniques, where legitimate system administration tools already present on a network are abused for malicious purposes. This makes detection harder as the activity can blend in with normal operations. Furthermore, many ransomware incidents now involve the use of Remote Access Trojans (RATs) or the abuse of legitimate Remote Monitoring and Management (RMM) tools like ConnectWise ScreenConnect or TeamViewer for initial access and persistence.\n",
    "\n",
    "    Disabling security defenses is another common tactic. Attackers actively try to tamper with or disable antivirus (AV) and Endpoint Detection and Response (EDR) solutions before deploying the ransomware payload. A popular method for this is the \"Bring Your Own Vulnerable Driver\" (BYOVD) technique, where attackers exploit a signed, legitimate but vulnerable driver to gain kernel-level privileges and shut down security software.\n",
    "\n",
    "    Critical infrastructure, healthcare, finance, and government sectors remain prime targets due to the high potential impact of disruption and the sensitivity of the data held. However, attacks are expanding across all sectors. Ransom demands continue to be substantial, often reaching multi-million dollar figures. The rise of Ransomware-as-a-Service (RaaS) platforms continues to lower the barrier to entry, enabling less sophisticated actors to launch potent attacks. Organizations must continually adapt their defense strategies, focusing on robust endpoint security, regular patching, multi-factor authentication, network segmentation, employee training, and comprehensive backup and recovery plans to counter these evolving threats.\n",
    "    \"\"\"\n",
    "},\n",
    "{\n",
    "    'title': 'Cybersecurity Skills Gap Widens in 2025 Despite Increased Awareness',\n",
    "    'domain': 'cybersecurity',\n",
    "    'text': \"\"\"\n",
    "    (April 12, 2025) – The gap between the demand for skilled cybersecurity professionals and the available talent pool has reached a record high in 2025, posing a significant challenge to organizations worldwide seeking to defend against increasingly sophisticated cyber threats. Despite numerous initiatives and heightened awareness of cybersecurity's importance, the workforce growth is failing to keep pace with escalating needs.\n",
    "\n",
    "    Recent studies, such as those by ISC2 and reports discussed at the World Economic Forum, highlight the severity of the issue. The global cybersecurity skills gap is estimated at nearly 5 million professionals, a significant increase from previous years. While the existing workforce numbers around 5.5 million, demand necessitates over 10 million experts. This shortage is particularly acute in the public sector, which reports greater difficulty in attracting and retaining talent compared to private organizations. Two out of three organizations report experiencing moderate-to-critical skills gaps.\n",
    "\n",
    "    Several factors contribute to this widening gap. The rapid evolution of technology, including the proliferation of cloud computing, IoT devices, and AI, continuously expands the attack surface and necessitates new, specialized skills. Furthermore, adversaries are leveraging AI to create more advanced and harder-to-detect attacks, demanding greater expertise from defenders. There's a specific rising demand for specialists in areas like Active Directory security, cloud security, and AI threat defense.\n",
    "\n",
    "    The educational pipeline is struggling to produce qualified candidates at the required scale. Experts suggest a need to promote cybersecurity careers earlier, potentially in high school, and emphasize practical, hands-on skills alongside traditional degrees. Indeed, hiring trends show leaders prioritizing demonstrable skills and certifications over just academic qualifications. Upskilling existing blue teams (defensive security professionals) is also crucial, as many organizations report a deficiency in adequate defensive skills among current staff.\n",
    "\n",
    "    Organizations are turning to AI-powered security tools to augment human teams and automate tasks, helping to mitigate the personnel shortage. However, a reported knowledge gap exists even here, with many professionals not fully understanding the AI technologies within their security stack. Addressing the skills gap requires a multi-pronged approach: investing in education and training programs, promoting diversity in hiring to broaden the talent pool, fostering public-private partnerships, utilizing automation and AI effectively, and creating attractive career paths within the cybersecurity field. Failure to close this gap leaves organizations increasingly vulnerable to cyberattacks.\n",
    "    \"\"\"\n",
    "},\n",
    "{\n",
    "    'title': 'Protecting Critical Infrastructure: A Top Priority Amidst Rising Geopolitical Tensions',\n",
    "    'domain': 'cybersecurity',\n",
    "    'text': \"\"\"\n",
    "    (April 12, 2025) – Securing critical infrastructure sectors – including energy, finance, healthcare, transportation, and water systems – against cyber threats has become an paramount concern for governments and private organizations globally in 2025. Heightened geopolitical tensions and the increasing sophistication of state-sponsored and criminal hacking groups have significantly elevated the risk profile for these essential services.\n",
    "\n",
    "    Cyber incidents targeting critical infrastructure can have devastating real-world consequences, ranging from service disruptions and economic damage to potential threats to public safety and national security. Ransomware operators, in particular, are increasingly focusing on industries where operational disruption carries severe repercussions, recognizing the increased likelihood of payment or significant impact. Recent trends show a marked increase in attacks targeting these vital sectors.\n",
    "\n",
    "    The convergence of Information Technology (IT) and Operational Technology (OT) systems within critical infrastructure environments presents unique challenges. OT systems, which control physical processes (like power generation or water treatment), were often designed without modern security considerations and can be vulnerable if connected to IT networks. Protecting these interconnected systems requires specialized expertise and security measures that bridge the IT/OT divide, forming a foundation for concepts like Industry 5.0 resilience.\n",
    "\n",
    "    Governments worldwide are responding with increased regulation and collaborative initiatives. National cybersecurity strategies increasingly emphasize critical infrastructure protection, mandating stricter security standards, information sharing protocols, and incident response preparedness. Initiatives like Italy's National Cybersecurity Strategy 2022-2026 exemplify efforts to bolster defenses through training, technology deployment, and public-private partnerships. However, regulatory fragmentation across different jurisdictions remains a challenge for multinational organizations.\n",
    "\n",
    "    Key defensive strategies include robust network segmentation to isolate critical systems, continuous monitoring for anomalous activity, stringent access controls (including multi-factor authentication), regular vulnerability assessments and patching, and comprehensive incident response and recovery plans. Securing the supply chain for both hardware and software components used in critical systems is also vital, as demonstrated by attacks exploiting vulnerabilities in third-party suppliers.\n",
    "\n",
    "    The reliance on legacy systems, the shortage of skilled cybersecurity professionals with OT expertise, and the persistent threat from well-funded adversaries mean that securing critical infrastructure is an ongoing battle. Continuous investment, international cooperation, and a proactive, risk-based security posture are essential to safeguard the essential services societies depend upon.\n",
    "    \"\"\"\n",
    "},\n",
    "\n",
    "# --- Food Articles ---\n",
    "{\n",
    "    'title': 'Lab-Grown Meat Nears Commercial Reality Amidst Production Hurdles',\n",
    "    'domain': 'food',\n",
    "    'text': \"\"\"\n",
    "    (April 12, 2025) – The field of cultivated meat, often referred to as lab-grown or cell-based meat, is rapidly advancing towards broader commercial availability, driven by concerns over traditional animal agriculture's environmental impact, animal welfare, and potential for zoonotic diseases. Several companies have achieved regulatory approvals in key markets and are scaling up production, though significant hurdles remain.\n",
    "\n",
    "    Recent months have seen increased investment and pilot programs aimed at bringing cultivated chicken, beef, and seafood products to restaurants and potentially retail shelves. Startups are refining bioreactor technologies to grow animal cells efficiently and cost-effectively, using nutrient-rich media to replicate tissue growth outside of an animal. The goal is to produce meat that is biologically identical to its farm-raised counterpart, offering a similar taste and texture profile.\n",
    "\n",
    "    Key technological challenges involve scaling production to industrial levels while drastically reducing costs. The nutrient media required for cell growth remains expensive, and designing large-scale bioreactors that maintain optimal conditions for tissue development is complex. Achieving desirable textures, particularly for structured cuts of meat like steaks, requires sophisticated scaffolding techniques that are still under development. Consumer acceptance is another crucial factor, hinging on taste parity, affordability, and clear communication about the production process and safety.\n",
    "\n",
    "    Despite these challenges, progress is evident. Companies are optimizing cell lines for faster growth and higher yields, exploring plant-based or lower-cost nutrient media formulations, and improving bioreactor efficiency. Hybrid products, blending cultivated cells with plant-based ingredients, offer a potential pathway to market by leveraging existing technologies and potentially lowering costs initially.\n",
    "\n",
    "    Regulatory pathways are slowly clearing in various countries, providing frameworks for safety assessments and labeling. Early market entries are likely to focus on high-end restaurants or blended products before achieving price parity with conventional meat for widespread retail adoption. Proponents highlight the potential benefits: significantly reduced land and water use, lower greenhouse gas emissions compared to conventional livestock farming, and elimination of antibiotic use in meat production. While the timeline for mass-market availability remains uncertain, the momentum behind cultivated meat suggests it could play a significant role in the future food system, offering a more sustainable alternative protein source if scalability and cost challenges can be overcome.\n",
    "    \"\"\"\n",
    "},\n",
    "{\n",
    "    'title': 'Sustainable Agriculture Innovations Tackle Climate Change and Food Security',\n",
    "    'domain': 'food',\n",
    "    'text': \"\"\"\n",
    "    (April 12, 2025) – Faced with the dual pressures of climate change and a growing global population, innovation in sustainable agriculture is accelerating, offering promising solutions to enhance food security while minimizing environmental impact. A range of technologies and practices are being developed and adopted to make farming more resilient, efficient, and ecologically sound.\n",
    "\n",
    "    Precision agriculture remains a key driver, utilizing GPS, sensors, drones, and data analytics to optimize the use of resources like water, fertilizer, and pesticides. By applying inputs only where and when needed, farmers can reduce waste, lower costs, and lessen environmental runoff. AI and machine learning are further enhancing precision techniques, enabling better crop monitoring, yield prediction, and disease detection.\n",
    "\n",
    "    Genetic engineering and new breeding techniques like gene editing (facilitated by regulations like the UK's Precision Breeding Act) are playing a crucial role. These technologies allow for the development of crops with enhanced traits, such as drought tolerance, pest resistance, improved nutritional value, and higher yields. This reduces reliance on chemical inputs and helps crops adapt to changing climate conditions, although public acceptance and regulatory frameworks vary globally.\n",
    "\n",
    "    Agroecology and regenerative agriculture practices are gaining traction. These approaches focus on building soil health through techniques like cover cropping, no-till farming, and crop rotation. Healthy soils sequester more carbon, improve water retention, and enhance biodiversity, contributing to both climate mitigation and adaptation. Integrating livestock grazing with cropping systems can also enhance nutrient cycling and soil fertility.\n",
    "\n",
    "    Controlled Environment Agriculture (CEA), including vertical farming and hydroponics, offers solutions for producing crops in urban areas or regions with limited arable land or water. These systems use significantly less water than traditional farming and allow for year-round production independent of weather conditions, reducing transportation distances and associated emissions. Innovations in LED lighting and automation are making CEA more energy-efficient and economically viable.\n",
    "\n",
    "    Furthermore, novel solutions like methane-reducing feed additives for livestock (such as those derived from Asparagopsis seaweed by companies like CH4 Global) and bio-pesticides (like agricultural phages) offer targeted ways to reduce agriculture's environmental footprint. Investment in agtech, while experiencing some recent fluctuations, continues to fuel these innovations, supported by government initiatives and programs aimed at promoting sustainable practices. The collective goal is to transform the food system towards one that can nourish the world sustainably for generations to come.\n",
    "    \"\"\"\n",
    "},\n",
    "{\n",
    "    'title': 'Food Supply Chain Resilience Becomes Critical Focus with New Technologies',\n",
    "    'domain': 'food',\n",
    "    'text': \"\"\"\n",
    "    (April 12, 2025) – Recent global disruptions, from pandemics to geopolitical conflicts and extreme weather events, have starkly highlighted the vulnerabilities within complex food supply chains. In response, building resilience has become a top priority for the food industry, with technology playing a crucial role in enhancing transparency, efficiency, and adaptability from farm to fork.\n",
    "\n",
    "    Enhanced visibility is a cornerstone of resilience. Technologies like blockchain are being explored and implemented to create transparent, immutable records of food products as they move through the supply chain. This improves traceability, helping to quickly identify sources of contamination during recalls, verify product authenticity (combating fraud), and track sustainability metrics. IoT sensors deployed on farms, during transport, and in storage facilities provide real-time data on conditions like temperature, humidity, and location, ensuring optimal quality and safety, and allowing for proactive interventions if issues arise.\n",
    "\n",
    "    Predictive analytics and Artificial Intelligence (AI) are transforming supply chain management. By analyzing vast datasets – including weather patterns, historical demand, market trends, and logistics information – AI can forecast potential disruptions, optimize inventory levels, predict consumer demand more accurately, and suggest alternative sourcing or routing options in real-time. This helps businesses mitigate risks, reduce waste from spoilage, and ensure continuity of supply.\n",
    "\n",
    "    Automation is increasing efficiency and reducing labor dependency in various parts of the supply chain. Automated systems in warehouses, robotics in processing plants, and autonomous vehicles for transport can streamline operations, reduce handling errors, and maintain productivity even during labor shortages. Advanced planning systems help coordinate complex logistics networks more effectively.\n",
    "\n",
    "    Diversification of sourcing and decentralization of production are also key resilience strategies, often enabled by technology. Shorter supply chains, supported by local sourcing platforms and controlled environment agriculture (like vertical farms near urban centers), reduce dependence on long-distance transport and associated risks. Companies are also using technology to vet and onboard alternative suppliers more quickly when disruptions occur in primary regions.\n",
    "\n",
    "    Furthermore, platforms enhancing collaboration and information sharing between different stakeholders – farmers, processors, distributors, retailers – allow for a more coordinated response to challenges. While technology alone cannot eliminate all risks, its strategic implementation is proving essential for building more robust, agile, and transparent food supply chains capable of withstanding future shocks and ensuring consistent access to safe and nutritious food.\n",
    "    \"\"\"\n",
    "},\n",
    "\n",
    "# --- Environment Articles ---\n",
    "{\n",
    "    'title': 'Global Renewable Energy Adoption Accelerates, But Challenges Remain',\n",
    "    'domain': 'environment',\n",
    "    'text': \"\"\"\n",
    "    (April 12, 2025) – The global transition towards renewable energy sources like solar, wind, and hydropower continued to gain momentum through 2024 and into 2025, driven by falling technology costs, supportive government policies, and mounting urgency to address climate change. However, significant challenges related to grid integration, energy storage, and scaling up deployment persist.\n",
    "\n",
    "    Solar and wind power installations reached record levels in many parts of the world. Advancements in photovoltaic cell efficiency and wind turbine design have made these technologies increasingly competitive with fossil fuels on cost, even without subsidies in some regions. Corporate Power Purchase Agreements (PPAs), where large companies commit to buying renewable energy long-term, have also become a major driver of new capacity additions. Governments continue to play a key role through mechanisms like feed-in tariffs, tax credits, renewable portfolio standards, and competitive auctions for large-scale projects.\n",
    "\n",
    "    Despite this progress, integrating large amounts of variable renewable energy (VRE) like solar and wind into existing electricity grids poses technical hurdles. Grid infrastructure often needs substantial upgrades to handle fluctuating power flows and connect remote renewable generation sites to demand centers. Managing intermittency – the fact that the sun doesn't always shine and the wind doesn't always blow – requires flexible grid operations and significant investment in energy storage solutions.\n",
    "\n",
    "    Battery storage technology is improving rapidly in terms of cost and performance, becoming crucial for stabilizing grids and providing power during periods of low renewable generation. Large-scale battery projects are increasingly being co-located with solar and wind farms. Other storage solutions, including pumped hydro storage, compressed air energy storage, and green hydrogen production (using renewable electricity for electrolysis), are also being developed and deployed, though often at higher costs currently.\n",
    "\n",
    "    Permitting processes and supply chain constraints for key materials (like lithium, cobalt, and rare earth elements used in batteries and turbines) can also slow down deployment rates. Public acceptance and addressing land-use conflicts related to large renewable energy projects are further considerations. While the long-term trend is clearly towards a decarbonized energy system powered by renewables, accelerating the pace of deployment, modernizing grid infrastructure, scaling up energy storage, and streamlining regulatory processes are critical steps needed to meet international climate goals and ensure a reliable, clean energy future for all.\n",
    "    \"\"\"\n",
    "},\n",
    "{\n",
    "    'title': 'International Climate Agreements Face Crucial Test Amidst Geopolitical Landscape',\n",
    "    'domain': 'environment',\n",
    "    'text': \"\"\"\n",
    "    (April 12, 2025) - Global efforts to address climate change, primarily guided by the Paris Agreement, face a critical period in 2025 as nations work to strengthen their commitments amidst a complex geopolitical and economic backdrop. While the scientific consensus on the urgency of climate action is stronger than ever, translating international goals into concrete domestic policies and achieving sufficient global cooperation remains challenging.\n",
    "\n",
    "    The Paris Agreement framework relies on Nationally Determined Contributions (NDCs), where countries set their own targets for reducing greenhouse gas emissions. Periodic \"stocktakes\" assess collective progress, with the expectation that countries will progressively increase their ambition over time. Recent assessments indicate that current NDCs, even if fully implemented, are insufficient to limit global warming to the agreement's targets of well below 2°C, preferably 1.5°C, above pre-industrial levels. Bridging this ambition gap is a central focus of ongoing international climate negotiations.\n",
    "\n",
    "    Key issues include financing climate action, particularly support for developing nations to mitigate emissions and adapt to climate impacts. Developed countries' commitments to provide climate finance have faced scrutiny regarding delivery and adequacy. Mechanisms for carbon markets (under Article 6 of the Paris Agreement) are still being finalized, aiming to facilitate international cooperation on emissions reductions but requiring robust rules to ensure environmental integrity.\n",
    "\n",
    "    Geopolitical tensions and economic pressures, including concerns about energy security and inflation, can sometimes divert attention and resources from climate action. Trade disputes, particularly those involving tariffs on green technologies or carbon border adjustments, add complexity to international climate cooperation. Ensuring a \"just transition\" – supporting workers and communities affected by the shift away from fossil fuels – is also crucial for maintaining political support for climate policies globally.\n",
    "\n",
    "    Despite these challenges, positive signals exist. Many countries are enacting more ambitious domestic climate legislation and investing heavily in renewable energy and green technologies. The private sector is increasingly recognizing climate risks and opportunities, with growing corporate commitments to decarbonization. International platforms like the G20 and specialized UN bodies continue to provide forums for dialogue and cooperation. However, accelerating the pace of emissions reductions globally requires stronger political will, enhanced international collaboration, increased financial flows, and effective implementation of policies across all major economies. The success of the Paris Agreement ultimately depends on whether nations can collectively ramp up action sufficiently in this critical decade.\n",
    "    \"\"\"\n",
    "},\n",
    "{\n",
    "    'title': 'Biodiversity Crisis Deepens, Conservation Efforts Scale Up Response',\n",
    "    'domain': 'environment',\n",
    "    'text': \"\"\"\n",
    "    (April 12, 2025) – The global decline in biodiversity, often termed the sixth mass extinction event, continues at an alarming rate, driven primarily by habitat destruction, climate change, pollution, overexploitation of resources, and invasive species. Scientific reports consistently highlight accelerating species extinction rates and degradation of ecosystems worldwide, threatening not only nature itself but also the essential services ecosystems provide to humanity, such as clean air and water, pollination, and climate regulation.\n",
    "\n",
    "    Habitat loss, largely due to agricultural expansion, infrastructure development, and deforestation, remains the single biggest driver. Natural ecosystems are being fragmented and destroyed, shrinking the spaces available for wildlife. Climate change exacerbates the crisis by shifting species' ranges, altering habitats (e.g., coral bleaching), and increasing the frequency of extreme weather events that devastate ecosystems. Pollution, particularly from plastics, pesticides, and industrial waste, further degrades habitats and harms wildlife directly.\n",
    "\n",
    "    In response to this deepening crisis, conservation efforts are intensifying globally, guided by frameworks like the Kunming-Montreal Global Biodiversity Framework adopted in late 2022. This framework sets ambitious targets, including protecting 30% of the planet's land and sea by 2030 (the \"30x30\" goal), restoring degraded ecosystems, reducing pollution, and mobilizing financial resources for biodiversity.\n",
    "\n",
    "    Conservation strategies encompass a wide range of actions. Expanding protected areas (national parks, marine reserves) remains crucial, but effective management and ensuring connectivity between protected areas are equally important. Ecosystem restoration projects, such as reforestation, wetland rehabilitation, and coral reef restoration, aim to rebuild degraded habitats and recover lost biodiversity.\n",
    "\n",
    "    Tackling the drivers of biodiversity loss is essential. This includes promoting sustainable agriculture and forestry practices, shifting towards circular economy models to reduce waste and pollution, combating illegal wildlife trade, and managing invasive species. Integrating biodiversity considerations into economic planning and decision-making across all sectors (e.g., finance, infrastructure) is increasingly recognized as vital.\n",
    "\n",
    "    Technological advancements, such as satellite monitoring, eDNA (environmental DNA) analysis for species detection, and AI for analyzing ecological data, are providing powerful new tools for conservation monitoring and management. Community involvement and recognizing the rights and knowledge of Indigenous Peoples and local communities are also critical for successful, long-term conservation outcomes. Despite the scale of the challenge, the increased global attention, ambitious targets, and growing toolkit of conservation approaches offer hope for bending the curve of biodiversity loss, though rapid and transformative action is urgently required.\n",
    "    \"\"\"\n",
    "},\n",
    "\n",
    "# --- Real Estate Articles ---\n",
    "{\n",
    "    'title': 'Global Housing Markets Navigate Shifting Sands of Interest Rates and Affordability',\n",
    "    'domain': 'real estate',\n",
    "    'text': \"\"\"\n",
    "    (April 12, 2025) – Housing markets across the globe continue to experience volatility in early 2025, largely influenced by evolving central bank monetary policies and persistent affordability challenges. After a period of rapid interest rate hikes aimed at curbing inflation, some markets are seeing tentative signs of stabilization, while others remain under pressure.\n",
    "\n",
    "    The impact of higher interest rates, which make mortgages more expensive, significantly dampened housing demand and price growth throughout 2023 and 2024 in many developed economies. Transaction volumes slowed, and price corrections occurred in several previously overheated markets. Now, as inflation shows signs of moderating in some regions, speculation about potential interest rate cuts later in 2025 is providing a degree of cautious optimism. However, borrowing costs generally remain elevated compared to the ultra-low rates of the early 2020s.\n",
    "\n",
    "    Housing affordability remains a critical issue. Decades of price growth outpacing wage increases, coupled with the recent surge in mortgage rates, have pushed homeownership out of reach for many potential buyers, particularly first-timers. Supply constraints in desirable locations continue to underpin prices in many areas, preventing steeper declines despite weaker demand. Building activity, while necessary to address supply shortages, is often hampered by high construction costs, labor shortages, and regulatory hurdles.\n",
    "\n",
    "    Regional variations are significant. Some markets, particularly those with strong population growth or resilient economies, have proven more resistant to downturns. Conversely, markets that saw the most extreme price surges during the pandemic boom remain more vulnerable to corrections. The \"flight to affordability,\" where buyers seek out less expensive cities or suburban/rural areas, continues to influence migration patterns and relative price performance.\n",
    "\n",
    "    Rental markets also face pressure. High homeownership costs are keeping more people in the rental sector, boosting demand and pushing up rents in many cities. This exacerbates cost-of-living pressures for tenants.\n",
    "\n",
    "    Looking ahead, the trajectory of housing markets in 2025 will depend heavily on the path of interest rates set by central banks, the performance of national economies, and local supply-demand dynamics. While a major crash seems unlikely in most stable economies due to factors like tight supply and homeowner equity built up over time, affordability constraints and elevated borrowing costs are expected to continue shaping market activity, likely leading to more moderate price movements compared to the preceding boom years.\n",
    "    \"\"\"\n",
    "},\n",
    "{\n",
    "    'title': 'Commercial Real Estate Adapts to Hybrid Work Models and Tenant Demands',\n",
    "    'domain': 'real estate',\n",
    "    'text': \"\"\"\n",
    "    (April 12, 2025) – The commercial real estate (CRE) sector, particularly the office market, is undergoing a significant transformation driven by the persistence of remote and hybrid work models adopted since the COVID-19 pandemic. While initial fears of a complete office exodus have subsided, the landscape in early 2025 reflects evolving tenant needs and shifting demand patterns.\n",
    "\n",
    "    Office vacancy rates, which surged during the pandemic, have shown signs of stabilization in many major markets, partly due to return-to-office mandates implemented by numerous companies. Research indicates that hybrid models requiring employees onsite 2-3 days per week have significantly less negative impact on office demand compared to fully remote or 1-day-a-week policies. However, overall office occupancy generally remains below pre-pandemic levels, and projected office values in major hubs like New York City are expected to stay considerably lower than 2019 levels for the foreseeable future.\n",
    "\n",
    "    This shift is forcing landlords and property managers to adapt. There's a noticeable \"flight to quality,\" where tenants prioritize newer, well-located buildings with modern amenities that can entice employees back to the office. Older, less desirable office stock faces higher vacancy rates and potential pressure for conversion to other uses, such as residential or logistics.\n",
    "\n",
    "    Tenant demands have evolved. Flexibility is key, leading to increased interest in co-working spaces, shared offices, and shorter lease terms. Landlords are investing heavily in amenities to enhance the office experience and foster collaboration, including high-tech meeting rooms, virtual event capabilities, fitness centers, improved air quality systems, communal lounges, soundproof pods for focused work, and even organized networking events. These amenities are seen as crucial for attracting and retaining tenants in a competitive market, despite the significant investment costs.\n",
    "\n",
    "    The rise of remote work has also impacted geographic demand. While major city centers still hold appeal, suburban office markets have seen increased activity as some companies seek smaller, satellite offices closer to where employees live. The retail segment of CRE is also adapting, with a continued focus on experiential retail and optimizing logistics for e-commerce fulfillment. Industrial and logistics properties remain in high demand due to the ongoing growth of online shopping.\n",
    "\n",
    "    Overall, the CRE sector is navigating a period of recalibration. Success hinges on flexibility, adaptability, and investing in properties and amenities that meet the evolving needs of businesses and their employees in the hybrid work era. Innovation in property technology (\"proptech\") and a focus on sustainability are also becoming increasingly important competitive differentiators.\n",
    "    \"\"\"\n",
    "},\n",
    "{\n",
    "    'title': 'Sustainable Building Practices Gain Momentum in Real Estate Development',\n",
    "    'domain': 'real estate',\n",
    "    'text': \"\"\"\n",
    "    (April 12, 2025) – Sustainability is increasingly moving from a niche concern to a core consideration within the real estate development and investment sectors. Driven by growing environmental awareness, stricter regulations, tenant demand, and investor pressure, sustainable building practices are gaining significant momentum across residential, commercial, and industrial property types.\n",
    "\n",
    "    Energy efficiency remains a primary focus. Developers are incorporating advanced insulation, high-performance windows, energy-efficient HVAC systems, LED lighting, and smart building technologies to reduce operational energy consumption. Building designs increasingly prioritize passive strategies like optimizing orientation for natural light and ventilation. Green building certifications like LEED (Leadership in Energy and Environmental Design) and BREEAM (Building Research Establishment Environmental Assessment Method) are becoming more common as benchmarks for sustainable performance.\n",
    "\n",
    "    The sourcing and use of building materials are also under scrutiny. There's a growing emphasis on using low-carbon materials, such as mass timber (cross-laminated timber) as an alternative to steel and concrete, recycled or reclaimed materials, and locally sourced products to reduce transportation emissions. Life cycle assessments (LCAs) are being used more frequently to evaluate the environmental impact of materials from extraction to disposal. Reducing construction waste through better planning and recycling programs is another key aspect.\n",
    "\n",
    "    Water conservation measures, including rainwater harvesting, greywater recycling systems, and water-efficient fixtures, are being integrated into new developments, particularly in water-stressed regions. Sustainable site development practices focus on preserving existing vegetation, managing stormwater runoff effectively through green infrastructure (like bioswales and green roofs), and enhancing local biodiversity.\n",
    "\n",
    "    Beyond environmental factors, sustainability in real estate also encompasses social aspects, such as creating healthy indoor environments with good air quality and natural light, ensuring accessibility, and contributing positively to local communities. Tenant and buyer demand for sustainable properties is rising, as occupants recognize benefits like lower utility bills, improved well-being, and alignment with personal values. Investors are increasingly incorporating Environmental, Social, and Governance (ESG) criteria into their decision-making, viewing sustainable buildings as lower-risk, higher-value assets in the long run.\n",
    "\n",
    "    While upfront costs for sustainable construction can sometimes be higher, lifecycle cost analyses often demonstrate long-term savings through reduced operational expenses. As regulations tighten (e.g., stricter building energy codes) and awareness grows, sustainable building practices are expected to become standard procedure rather than optional extras, reshaping the future of real estate development towards more environmentally responsible and resilient structures.\n",
    "    \"\"\"\n",
    "},\n",
    "\n",
    "# --- Entertainment Articles ---\n",
    "{\n",
    "    'title': 'Streaming Wars Intensify: Services Battle for Subscribers Amidst Market Saturation',\n",
    "    'domain': 'entertainment',\n",
    "    'text': \"\"\"\n",
    "    (April 12, 2025) – The global streaming entertainment landscape remains fiercely competitive in 2025, as established giants and newer entrants battle for subscriber loyalty and sustainable profitability in an increasingly saturated market. While streaming has become the dominant mode of accessing video and music content, the strategies for success are evolving.\n",
    "\n",
    "    After years of rapid growth fueled by heavy content investment and low introductory pricing, many streaming services (Netflix, Disney+, Max, etc.) are now focusing more on profitability. This has led to widespread price increases, crackdowns on password sharing, and the introduction of lower-cost, ad-supported subscription tiers. These ad tiers represent a significant shift, creating new revenue streams but also requiring services to build sophisticated advertising technology and sales operations.\n",
    "\n",
    "    Content remains king, but the \"arms race\" for exclusive, high-budget original series and films is becoming more calculated. Services are analyzing data meticulously to understand what drives subscriber acquisition and retention, potentially leading to more focused content strategies rather than sheer volume. Licensing popular library content from traditional studios also remains a key tactic to round out offerings. Live sports broadcasting rights have emerged as a major battleground, attracting significant investment as services seek to draw in dedicated fan bases.\n",
    "\n",
    "    Market saturation in North America and Europe means international expansion, particularly in Asia and Latin America, is crucial for future growth. This requires tailoring content and pricing strategies to local tastes and economic conditions. Consolidation within the industry is also anticipated, as smaller or less profitable services may merge or be acquired by larger players seeking scale and broader content libraries.\n",
    "\n",
    "    Competition isn't just about video; music streaming platforms like Spotify and Apple Music also face pressures. They continue to innovate with features like personalized recommendations driven by AI, podcast integration, higher audio quality options, and exploring interactive experiences. However, debates around artist compensation and the economics of streaming royalties persist within the music industry.\n",
    "\n",
    "    The challenge for all streaming services in 2025 is differentiation. With numerous options available, consumers are becoming more selective, potentially subscribing to fewer services or churning more frequently. Success requires a compelling content library, a user-friendly interface, effective personalization, competitive pricing (across various tiers), and strong brand identity to stand out in the crowded digital entertainment space. The era of easy growth is over; sustainable success now demands strategic focus and operational efficiency.\n",
    "    \"\"\"\n",
    "},\n",
    "{\n",
    "    'title': 'Virtual Reality Gains Traction in Entertainment Beyond Gaming',\n",
    "    'domain': 'entertainment',\n",
    "    'text': \"\"\"\n",
    "    (April 12, 2025) – While gaming continues to be the dominant driver of the Virtual Reality (VR) market, applications in broader entertainment sectors are steadily gaining traction, offering increasingly immersive and interactive experiences for consumers. Advances in hardware affordability and capability, combined with innovative content creation, are pushing VR beyond its niche origins.\n",
    "\n",
    "    The VR hardware market, led by devices like Meta Quest 3, continues to evolve with improved resolution, wider fields of view, and more comfortable form factors. Untethered (standalone) headsets are making VR more accessible. Forecasts predict continued growth in hardware sales, reaching billions in revenue globally, with significant market presence in regions like China. Accessories like haptic feedback suits, providing physical sensations synced with the virtual experience, promise even deeper immersion, though widespread adoption is still nascent.\n",
    "\n",
    "    Beyond gaming, VR is making inroads into cinematic experiences and live events. VR films and documentaries place viewers directly inside the narrative, offering unparalleled presence. Virtual concerts allow fans to attend performances from global artists remotely, experiencing the show from unique vantage points. Platforms are emerging for social VR experiences, where users can interact as avatars in shared virtual spaces, attend virtual movie screenings, or explore digital worlds together. Museums and cultural institutions are using VR to offer virtual tours and interactive exhibits.\n",
    "\n",
    "    The integration of Artificial Intelligence (AI) is expected to further enhance VR entertainment. AI could enable more dynamic and responsive virtual characters (NPCs), procedural generation of vast virtual environments, and highly personalized narrative experiences that adapt to user choices and behaviour. The convergence of VR with Augmented Reality (AR), creating Mixed Reality (MR) experiences that blend digital elements with the real world, also opens new possibilities for entertainment, training, and social interaction.\n",
    "\n",
    "    Despite progress, challenges remain. The cost of high-end VR equipment can still be a barrier for some consumers. Content development requires specialized skills and significant investment. Issues like motion sickness ('cybersickness') persist for a subset of users. However, as the technology matures and the ecosystem of content expands, VR is poised to become a more significant part of the entertainment landscape. Its potential to offer truly immersive storytelling, social connection in virtual spaces, and novel interactive experiences suggests VR will continue its transition from a futuristic concept to a mainstream entertainment medium, complementing, rather than replacing, traditional formats.\n",
    "    \"\"\"\n",
    "},\n",
    "{\n",
    "    'title': 'AI Integration Sparks Innovation and Debate in Music and Film Production',\n",
    "    'domain': 'entertainment',\n",
    "    'text': \"\"\"\n",
    "    (April 12, 2025) – Artificial Intelligence (AI) is increasingly permeating the entertainment industry, offering powerful new tools for music creation and film production while simultaneously sparking significant debate about creativity, copyright, and the future of human roles. In 2025, AI's impact is moving beyond experimentation towards practical integration into workflows.\n",
    "\n",
    "    In music production, AI-powered tools are assisting artists and producers in various ways. Algorithms can generate melodies, harmonies, and rhythms based on specific parameters or analyze existing tracks to suggest compositional ideas. Platforms like Amper Music and AIVA allow users to create customized soundtracks, making music composition more accessible, even for those without traditional training. AI is also being used for tasks like audio mastering, separating stems (e.g., vocals from instruments) in recordings, and powering sophisticated music recommendation engines on streaming platforms. Voice synthesis technology is advancing, enabling the creation of realistic vocal tracks or modification of existing ones, raising both creative possibilities and ethical questions.\n",
    "\n",
    "    In film and television production, AI is being integrated across the production cycle. Generative AI tools can create concept art, storyboards, and even rough visual effects based on text prompts (text-to-image/video). Tools like OpenAI's Sora (still in development) aim to assist with script analysis and pre-visualization. AI enhances post-production tasks like color grading, rotoscoping (isolating objects), background removal (as seen in \"Everything Everywhere All at Once\"), and upscaling lower-resolution footage. Advanced motion capture techniques powered by AI can track human movement without specialized suits, simplifying character animation. Predictive analytics platforms use AI to forecast potential box office performance or test audience responses to concepts.\n",
    "\n",
    "    However, the use of AI in creative fields is contentious. The 2023 Hollywood writers' and actors' strikes highlighted major concerns about AI potentially replacing human writers, performers, or using their likenesses without consent or compensation. Defining copyright ownership for AI-generated or AI-assisted works remains a complex legal challenge. Questions persist about the originality and artistic merit of AI-created content and the risk of AI perpetuating biases present in its training data.\n",
    "\n",
    "    Despite the debates, the trend towards AI integration seems irreversible. The focus is shifting towards AI as a powerful co-pilot or tool that enhances human creativity, streamlines laborious tasks, and opens up new artistic possibilities. Establishing clear ethical guidelines, ensuring fair compensation, and defining the boundaries between human and machine creativity are crucial steps as the entertainment industry navigates the ongoing AI revolution.\n",
    "    \"\"\"\n",
    "}\n",
    "]\n",
    "\n",
    "    print(f\"Generated {len(test_articles)} articles.\")\n",
    "    # Example check of word count for the first article\n",
    "    # word_count = len(test_articles[0]['text'].split())\n",
    "    # print(f\"Word count for first article: {word_count}\")\n",
    "\n",
    "    \n",
    "    # Create a DataFrame to store results\n",
    "    results = []\n",
    "    \n",
    "    # Process each article\n",
    "    print(f\"\\nTesting on {len(test_articles)} diverse news articles...\\n\")\n",
    "    for i, article in enumerate(tqdm(test_articles, desc=\"Processing articles\")):\n",
    "        title = article['title']\n",
    "        domain = article['domain']\n",
    "        text = article['text'].strip()\n",
    "        \n",
    "        print(f\"\\nProcessing article {i+1}/{len(test_articles)} ({domain})...\")\n",
    "        \n",
    "        # Extract keyphrases\n",
    "        start_time = time.time()\n",
    "        keyphrases = extractor.extract_keyphrases_with_scores(text)\n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        # Store results\n",
    "        article_result = {\n",
    "            'title': title,\n",
    "            'domain': domain,\n",
    "            'text_length': len(text.split()),\n",
    "            'num_keyphrases': len(keyphrases),\n",
    "            'top_score': keyphrases[0][1] if keyphrases else 0,\n",
    "            'avg_score': sum(score for _, score in keyphrases) / len(keyphrases) if keyphrases else 0,\n",
    "            'processing_time': processing_time\n",
    "        }\n",
    "        \n",
    "        # Add keyphrases\n",
    "        for j, (keyphrase, score) in enumerate(keyphrases):\n",
    "            article_result[f'keyphrase_{j+1}'] = keyphrase\n",
    "            article_result[f'score_{j+1}'] = score\n",
    "        \n",
    "        results.append(article_result)\n",
    "        \n",
    "        # Print results for this article\n",
    "        print(f\"Title: {title}\")\n",
    "        print(f\"Domain: {domain}\")\n",
    "        print(f\"Extracted {len(keyphrases)} keyphrases:\")\n",
    "        for keyphrase, score in keyphrases:\n",
    "            print(f\"- {keyphrase}: {score:.2f}\")\n",
    "        print()\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Save results to CSV\n",
    "    results_df.to_csv(\"keyphrase_extraction_results.csv\", index=False)\n",
    "    print(f\"Results saved to keyphrase_extraction_results.csv\")\n",
    "    \n",
    "    # Generate summary statistics by domain\n",
    "    domain_stats = results_df.groupby('domain').agg({\n",
    "        'num_keyphrases': 'mean',\n",
    "        'top_score': 'mean',\n",
    "        'avg_score': 'mean',\n",
    "        'processing_time': 'mean',\n",
    "        'text_length': 'mean'\n",
    "    }).round(2)\n",
    "    \n",
    "    print(\"\\nSummary by domain:\")\n",
    "    for domain, stats in domain_stats.iterrows():\n",
    "        print(f\"Domain: {domain}\")\n",
    "        print(f\"  Avg. keyphrases: {stats['num_keyphrases']}\")\n",
    "        print(f\"  Avg. top score: {stats['top_score']}\")\n",
    "        print(f\"  Avg. score: {stats['avg_score']}\")\n",
    "        print(f\"  Avg. processing time: {stats['processing_time']:.2f}s\")\n",
    "    \n",
    "    # Visualize results\n",
    "    try:\n",
    "        # Set style\n",
    "        sns.set(style=\"whitegrid\")\n",
    "        \n",
    "        # Create figure with multiple subplots\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # Plot 1: Average number of keyphrases by domain\n",
    "        sns.barplot(x=domain_stats.index, y='num_keyphrases', data=domain_stats, ax=axes[0, 0])\n",
    "        axes[0, 0].set_title('Average Number of Keyphrases by Domain')\n",
    "        axes[0, 0].set_xlabel('Domain')\n",
    "        axes[0, 0].set_ylabel('Avg. Number of Keyphrases')\n",
    "        axes[0, 0].set_xticklabels(axes[0, 0].get_xticklabels(), rotation=45, ha='right')\n",
    "        \n",
    "        # Plot 2: Average top score by domain\n",
    "        sns.barplot(x=domain_stats.index, y='top_score', data=domain_stats, ax=axes[0, 1])\n",
    "        axes[0, 1].set_title('Average Top Keyphrase Score by Domain')\n",
    "        axes[0, 1].set_xlabel('Domain')\n",
    "        axes[0, 1].set_ylabel('Avg. Top Score')\n",
    "        axes[0, 1].set_xticklabels(axes[0, 1].get_xticklabels(), rotation=45, ha='right')\n",
    "        \n",
    "        # Plot 3: Relationship between text length and number of keyphrases\n",
    "        sns.scatterplot(x='text_length', y='num_keyphrases', hue='domain', data=results_df, ax=axes[1, 0])\n",
    "        axes[1, 0].set_title('Text Length vs. Number of Keyphrases')\n",
    "        axes[1, 0].set_xlabel('Text Length (words)')\n",
    "        axes[1, 0].set_ylabel('Number of Keyphrases')\n",
    "        \n",
    "        # Plot 4: Relationship between text length and processing time\n",
    "        sns.scatterplot(x='text_length', y='processing_time', hue='domain', data=results_df, ax=axes[1, 1])\n",
    "        axes[1, 1].set_title('Text Length vs. Processing Time')\n",
    "        axes[1, 1].set_xlabel('Text Length (words)')\n",
    "        axes[1, 1].set_ylabel('Processing Time (seconds)')\n",
    "        \n",
    "        # Adjust layout and save\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('keyphrase_extraction_analysis.png')\n",
    "        print(\"Visualization saved to keyphrase_extraction_analysis.png\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not generate visualizations: {str(e)}\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = test_abstractive_keyphrase_extractor()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
